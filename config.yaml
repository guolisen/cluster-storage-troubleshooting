# LLM Configuration
llm:
  # Provider selection: "openai", "google", or "ollama"
  provider: "openai"
  
  # OpenAI Configuration
  openai:
    #model: "gpt-4-turbo-preview"
    #model: "gpt-4o-mini-2024-07-18"
    model: "gpt-4.1-mini-2025-04-14"
    #model: "gpt-4.1-nano-2025-04-14"
    #model: "grok-3-mini"
    #api_endpoint: "https://api.openai.com/v1"
    #api_key: 'sk-proj--'
    #model: "gemini-2.5-flash-preview-05-20"
    api_key: "sk-"
    api_endpoint: "https://api.zhizengzeng.com/v1"
    temperature: 0.1
    max_tokens: 8192
  
  # Google Gemini Configuration
  google:
    model: "gemini-2.5-pro-preview-06-05"
    api_key: ""
    temperature: 0
    max_tokens: 8192
  
  # Ollama Configuration (for local LLMs)
  ollama:
    model: "qwen3:8b"
    base_url: "http://192.168.239.1:11434"
    temperature: 0
    max_tokens: 8192

# Monitoring Configuration
monitor:
  interval_seconds: 60
  api_retries: 3
  retry_backoff_seconds: 5

plan_phase:
  use_llm: true  
  timeout_seconds: 1800
  static_plan_step_path: "data/static_plan_step.json"

# Troubleshooting Configuration
troubleshoot:
  timeout_seconds: 1800
  interactive_mode: false
  auto_fix: false  
  ssh:
    enabled: true
    user: "root"
    key_path: "~/.ssh/id_ed25519"
    nodes:
      - "my-cluster-control-plane"
      - "my-cluster-worker"
    retries: 3
    retry_backoff_seconds: 5

# Command Configuration
commands:
  allowed:
    - "kubectl *"
    - "smartctl *"
    - "fio *"
    - "df *"
    - "lsblk *"
    - "mount *"
    - "dmesg *"
    - "journalctl *"
    - "ssh *"
    - "sh *"
    - "xfs_repair -n *"
  disallowed:
    - "fsck *"
    - "chmod *"
    - "chown *"
    - "dd *"
    - "mkfs *"
    - "rm *"
    - "kubectl delete *"
    - "kubectl apply *"
    - "xfs_repair *"

# Logging Configuration
logging:
  file: "troubleshoot.log"
  stdout: true

# Historical Experience Configuration
historical_experience:
  file_path: "data/historical_experience.json"

# Chat Mode Configuration
chat_mode:
  enabled: true  # Enable or disable chat mode
  entry_points:
    #- "plan_phase"  # After Plan Phase
    - "phase1"      # After Phase1

# MCP Configuration
mcp_enabled: true  # Set to true to enable MCP integration
mcp_servers:
  # Example server configurations (commented out by default)
  k8s:
    type: sse
    url: http://0.0.0.0:8000
    command: null
    args: []
    env: {}
    tools:
      plan_phase: true
      phase1: true
      phase2: false
  gaode:
    type: sse
    url: https://mcp.amap.com/sse?key=ed40afb4730ce2c8f5523c2fceca08be
    command: null
    args: []
    env: {}
    tools:
      plan_phase: true
      phase1: true
      phase2: false
  # tavily:
  #   type: stdio
  #   url: ""
  #   command: npx
  #   args: ["-y", "tavily-mcp@0.1.4"]
  #   env:
  #     TAVILY_API_KEY: ""
  #   tools:
  #     plan_phase: false
  #     phase1: true
  #     phase2: true
