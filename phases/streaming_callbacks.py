#!/usr/bin/env python3
"""
Streaming Callbacks for LLM Output

This module contains callback handlers for streaming LLM outputs
during different phases of the troubleshooting process.
"""

import sys
import logging
from typing import Dict, Any, Optional, List
from rich.console import Console
from rich.panel import Panel
from rich.live import Live
from rich.text import Text
from langchain.callbacks.base import BaseCallbackHandler

logger = logging.getLogger(__name__)

class StreamingCallbackHandler(BaseCallbackHandler):
    """
    Callback handler for streaming LLM tokens with phase-specific formatting
    
    This handler processes tokens as they are generated by the LLM and
    displays them in a formatted way based on the current phase.
    """
    
    def __init__(self, phase_name: str = None):
        """
        Initialize the streaming callback handler
        
        Args:
            phase_name: Name of the current phase (plan_phase, phase1, phase2)
        """
        self.phase_name = phase_name or "unknown"
        self.console = Console()
        self.tokens = []
        self.token_count = 0
        
        # Set up phase-specific styling
        self.phase_styles = {
            "plan_phase": {
                "title": "[bold blue]Plan Phase Streaming",
                "border_style": "blue",
                "text_style": "green"
            },
            "phase1": {
                "title": "[bold magenta]Phase 1 (Analysis) Streaming",
                "border_style": "magenta", 
                "text_style": "yellow"
            },
            "phase2": {
                "title": "[bold green]Phase 2 (Remediation) Streaming",
                "border_style": "green",
                "text_style": "cyan"
            },
            "unknown": {
                "title": "[bold white]LLM Streaming",
                "border_style": "white",
                "text_style": "white"
            }
        }
        
        # Get styling for current phase
        self.style = self.phase_styles.get(
            self.phase_name, 
            self.phase_styles["unknown"]
        )
        
        # Create initial panel
        self.panel = Panel(
            "",
            title=self.style["title"],
            border_style=self.style["border_style"],
            safe_box=True
        )
        
        # Initialize live display
        self.live = Live(
            self.panel, 
            console=self.console,
            auto_refresh=False,
            vertical_overflow="visible"
        )
        self.live.start()
        
        logger.info(f"Initialized streaming for {self.phase_name}")
    
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """
        Process a new token from the LLM
        
        Args:
            token: The new token from the LLM
            **kwargs: Additional arguments
        """
        # Add token to list
        self.tokens.append(token)
        self.token_count += 1
        
        # Update the panel with the current text
        current_text = Text("".join(self.tokens))
        current_text.stylize(self.style["text_style"])
        
        self.panel = Panel(
            current_text,
            title=f"{self.style['title']} ({self.token_count} tokens)",
            border_style=self.style["border_style"],
            safe_box=True
        )
        
        # Update the live display
        self.live.update(self.panel)
        self.live.refresh()
        
        # Also log to file
        logger.debug(f"[{self.phase_name}] New token: {token}")
    
    def on_llm_end(self, response, **kwargs) -> None:
        """
        Handle the end of LLM generation
        
        Args:
            response: The final LLM response
            **kwargs: Additional arguments
        """
        # Complete the panel
        final_text = Text("".join(self.tokens))
        final_text.stylize(self.style["text_style"])
        
        self.panel = Panel(
            final_text,
            title=f"{self.style['title']} (Complete - {self.token_count} tokens)",
            border_style=self.style["border_style"],
            safe_box=True
        )
        
        # Update the live display one last time
        self.live.update(self.panel)
        self.live.refresh()
        
        # Stop the live display
        self.live.stop()
        
        logger.info(f"Completed streaming for {self.phase_name} ({self.token_count} tokens)")
    
    def on_llm_error(self, error: Exception, **kwargs) -> None:
        """
        Handle an error during LLM generation
        
        Args:
            error: The error that occurred
            **kwargs: Additional arguments
        """
        # Show error in the panel
        error_text = Text(f"Error during LLM generation: {str(error)}")
        error_text.stylize("bold red")
        
        self.panel = Panel(
            error_text,
            title=f"{self.style['title']} (Error)",
            border_style="red",
            safe_box=True
        )
        
        # Update the live display
        self.live.update(self.panel)
        self.live.refresh()
        
        # Stop the live display
        self.live.stop()
        
        logger.error(f"Error during streaming for {self.phase_name}: {str(error)}")
