This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
docs/
  enhanced_phase1_examples.py
  mcp_agent_example.py
examples/
  standalone_react_example.py
information_collector/
  __init__.py
  base.py
  collector.py
  knowledge_builder.py
  metadata_parsers.py
  tool_executors.py
  volume_discovery.py
knowledge_graph/
  __init__.py
  demo_graph_print.py
  knowledge_graph.py
  tools.py
llm_graph/
  graphs/
    __init__.py
    phase1_llm_graph.py
    phase2_llm_graph.py
    plan_llm_graph.py
  prompt_managers/
    __init__.py
    base_prompt_manager.py
    legacy_prompt_manager.py
    phase1_prompt_manager.py
    phase2_prompt_manager.py
    plan_prompt_manager.py
  __init__.py
  graph_utility.py
  langgraph_interface.py
  prompt_manager_interface.py
monitoring/
  __init__.py
  monitor.py
phases/
  __init__.py
  chat_mode.py
  investigation_planner.py
  kg_context_builder.py
  llm_factory.py
  llm_plan_generator.py
  phase_analysis.py
  phase_information_collection.py
  phase_plan_phase.py
  phase_remediation.py
  plan_phase_react.py
  rule_based_plan_generator.py
  static_plan_step_reader.py
  streaming_callbacks.py
  tool_registry_builder.py
  utils.py
tests/
  mock_knowledge_graph.py
  mock_kubernetes_data.py
  mock_system_data.py
  test_hardware_system_entity.py
  test_langgraph_tools.py
  test_llm_providers.py
  test_plan_phase_react.py
tools/
  core/
    __init__.py
    config.py
    knowledge_graph.py
    mcp_adapter.py
  diagnostics/
    __init__.py
    disk_analysis.py
    disk_monitoring.py
    disk_performance.py
    hardware.py
    system.py
  kubernetes/
    __init__.py
    core.py
    csi_baremetal.py
  testing/
    __init__.py
    pod_creation.py
    resource_cleanup.py
    volume_testing_analysis.py
    volume_testing_basic.py
    volume_testing_filesystem.py
    volume_testing_performance.py
    volume_testing.py
  __init__.py
  registry.py
troubleshooting/
  __init__.py
  end_conditions.py
  execute_tool_node.py
  graph.py
  hook_manager.py
  prompt_manager.py
  strategies.py
  troubleshoot.py
hot.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="docs/enhanced_phase1_examples.py">
#!/usr/bin/env python3
"""
Enhanced Phase1 Examples

This module provides examples of the enhanced Phase1 functionality, demonstrating
the three possible cases:
1. No issues detected
2. Manual intervention required
3. Automatic fix possible

These examples can be used for testing and documentation purposes.
"""

import asyncio
from typing import Dict, Any, Tuple

# Sample outputs for the three cases

def get_no_issues_example() -> str:
    """
    Get an example of the 'No issues detected' case output
    
    Returns:
        str: Formatted output for the 'No issues detected' case
    """
    return """
Summary Finding: No issues detected in the system.
Evidence: Knowledge Graph query `kg_get_all_issues()` returned no error logs. Service status query `kg_get_entity_info(entity_type='service')` shows all services operational. Drive health check `kg_get_entity_info(entity_type='Drive', entity_id='drive-123')` shows Health=GOOD. Node status check `kg_get_entity_info(entity_type='Node', entity_id='node-1')` shows Ready=True, DiskPressure=False.
Advice: No action required. Continue monitoring system metrics for any future anomalies. Consider running periodic health checks using `kubectl get pods,pvc,pv` to ensure continued normal operation.
SKIP_PHASE2: YES
"""

def get_manual_intervention_example() -> str:
    """
    Get an example of the 'Manual intervention required' case output
    
    Returns:
        str: Formatted output for the 'Manual intervention required' case
    """
    return """
Summary Finding: Issue detected, but requires manual intervention.
Evidence: Knowledge Graph query `kg_get_all_issues()` indicates a hardware failure in drive 'drive-123'. Drive health check `kg_get_entity_info(entity_type='Drive', entity_id='drive-123')` shows Health=BAD. Relationship query `kg_find_path(source_entity_type='Drive', source_entity_id='drive-123', target_entity_type='Pod', target_entity_id='storage-pod')` confirms impact on 'storage-pod'.
Advice: 
1. Backup data from affected volumes using: `kubectl exec storage-pod -- tar -czf /backup/data.tar.gz /data`
2. Drain the affected node: `kubectl drain node-1 --ignore-daemonsets`
3. Replace the faulty drive 'drive-123' following hardware replacement procedures
4. Verify drive replacement with: `smartctl -a /dev/sda`
5. Uncordon the node: `kubectl uncordon node-1`
6. Verify pod rescheduling: `kubectl get pods -o wide | grep storage-pod`
SKIP_PHASE2: YES
"""

def get_automatic_fix_example() -> str:
    """
    Get an example of the 'Automatic fix possible' case output
    
    Returns:
        str: Formatted output for the 'Automatic fix possible' case
    """
    return """
# Summary of Findings
The pod "storage-pod" in namespace "default" is experiencing I/O errors on volume path "/data" due to filesystem corruption on the underlying PV.

# Detailed Analysis
Primary Issues:
- Filesystem corruption detected on PV "pv-123" bound to PVC "pvc-456" used by pod "storage-pod"
- The drive "drive-789" shows good health but the filesystem has inconsistencies

# Relationship Analysis
- Pod "storage-pod" uses PVC "pvc-456"
- PVC "pvc-456" is bound to PV "pv-123"
- PV "pv-123" is mapped to drive "drive-789"

# Investigation Process
- Executed Investigation Plan steps 1-5 successfully
- Identified filesystem corruption through log analysis
- Verified drive hardware is healthy

# Potential Root Causes
- Unexpected pod termination during write operations
- Power loss during write operations
- Filesystem aging and fragmentation

# Open Questions
- Was there a recent power outage or node restart?
- Are there other pods experiencing similar issues?

# Next Steps
- Run filesystem check and repair
- Monitor for recurrence
- Consider enabling journaling

# Root Cause
Filesystem corruption on PV "pv-123" due to improper pod termination during write operations.

# Fix Plan
1. Backup data: `kubectl exec backup-pod -- tar -czf /backup/data.tar.gz /data`
2. Unmount filesystem: `kubectl delete pod storage-pod`
3. Run filesystem check: `fsck.ext4 -y /dev/sda1`
4. Remount and verify: `kubectl apply -f storage-pod.yaml`
5. Verify data integrity: `kubectl exec storage-pod -- ls -la /data`
"""

async def simulate_phase1_execution(case: str) -> Tuple[str, bool]:
    """
    Simulate the execution of Phase1 with different cases
    
    Args:
        case: The case to simulate ('no_issues', 'manual_intervention', or 'automatic_fix')
        
    Returns:
        Tuple[str, bool]: (Analysis result, Skip Phase2 flag)
    """
    # Simulate some processing time
    await asyncio.sleep(1)
    
    if case == 'no_issues':
        result = get_no_issues_example()
        skip_phase2 = "SKIP_PHASE2: YES" in result
        if skip_phase2:
            result = result.replace("SKIP_PHASE2: YES", "").strip()
        return result, skip_phase2
    
    elif case == 'manual_intervention':
        result = get_manual_intervention_example()
        skip_phase2 = "SKIP_PHASE2: YES" in result
        if skip_phase2:
            result = result.replace("SKIP_PHASE2: YES", "").strip()
        return result, skip_phase2
    
    elif case == 'automatic_fix':
        result = get_automatic_fix_example()
        skip_phase2 = "SKIP_PHASE2: YES" in result
        if skip_phase2:
            result = result.replace("SKIP_PHASE2: YES", "").strip()
        return result, skip_phase2
    
    else:
        raise ValueError(f"Unknown case: {case}")

async def main():
    """
    Main function to demonstrate the enhanced Phase1 functionality
    """
    print("=== Enhanced Phase1 Examples ===\n")
    
    # Case 1: No issues detected
    print("Case 1: No issues detected")
    result, skip_phase2 = await simulate_phase1_execution('no_issues')
    print(f"Result:\n{result}")
    print(f"Skip Phase2: {skip_phase2}")
    print("\n" + "=" * 80 + "\n")
    
    # Case 2: Manual intervention required
    print("Case 2: Manual intervention required")
    result, skip_phase2 = await simulate_phase1_execution('manual_intervention')
    print(f"Result:\n{result}")
    print(f"Skip Phase2: {skip_phase2}")
    print("\n" + "=" * 80 + "\n")
    
    # Case 3: Automatic fix possible
    print("Case 3: Automatic fix possible")
    result, skip_phase2 = await simulate_phase1_execution('automatic_fix')
    print(f"Result:\n{result}")
    print(f"Skip Phase2: {skip_phase2}")
    print("\n" + "=" * 80 + "\n")

if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="information_collector/__init__.py">
"""
Information Collector Package for Kubernetes Volume Troubleshooting

This package implements the Phase 0 information-collection phase that executes diagnostic
LangGraph tools according to parameter's volume path and pod, to collect data and construct
a Knowledge Graph before analysis.

Components:
- ComprehensiveInformationCollector: Main collector class
- VolumeDiscovery: Volume dependency chain discovery
- ToolExecutors: LangGraph tool execution methods
- KnowledgeBuilder: Knowledge Graph construction from tool outputs
- MetadataParsers: Tool output parsing and metadata extraction
"""

import logging

# Configure logger for information collector
collector_logger = logging.getLogger('information_collector')
collector_logger.setLevel(logging.INFO)
# Don't propagate to root logger to avoid console output
collector_logger.propagate = False

from .collector import ComprehensiveInformationCollector

__all__ = ['ComprehensiveInformationCollector']
__version__ = '1.0.0'
</file>

<file path="information_collector/base.py">
"""
Base functionality for Information Collector

Contains base class with initialization, configuration, and core utilities.
"""

import os
import logging
import time
from typing import Dict, List, Any, Optional
from kubernetes import client, config
from knowledge_graph import KnowledgeGraph


class InformationCollectorBase:
    """Base class for Information Collector with core functionality"""
    
    def __init__(self, config_data: Dict[str, Any]):
        """Initialize the Information Collector Base"""
        self.config = config_data
        self.k8s_client = None
        self.knowledge_graph = KnowledgeGraph()
        self.collected_data = {
            'kubernetes': {},
            'csi_baremetal': {},
            'logs': {},
            'system': {},
            'ssh_data': {},
            'tool_outputs': {},  # Store individual tool outputs
            'log_analysis': {},  # Parsed log issues and analysis
            'errors': []
        }
        
        # Initialize Kubernetes client
        self._init_kubernetes_client()
        
        # SSH clients cache
        self.ssh_clients = {}
        
        # Interactive mode setting
        self.interactive_mode = config_data.get('troubleshoot', {}).get('interactive_mode', False)
        
        logging.info("Information Collector Base initialized")
    
    def _init_kubernetes_client(self):
        """Initialize Kubernetes client"""
        try:
            if 'KUBERNETES_SERVICE_HOST' in os.environ:
                config.load_incluster_config()
                logging.info("Using in-cluster Kubernetes configuration")
            else:
                config.load_kube_config()
                logging.info("Using kubeconfig file for Kubernetes configuration")
            
            self.k8s_client = client.CoreV1Api()
        except Exception as e:
            logging.error(f"Failed to initialize Kubernetes client: {e}")
            raise
    
    def _prompt_user_approval(self, tool_name: str, purpose: str) -> bool:
        """
        Prompt user for tool execution approval in interactive mode
        
        Args:
            tool_name: Name of the tool to execute
            purpose: Purpose of the tool execution
            
        Returns:
            bool: True if approved, False if denied
        """
        if not self.interactive_mode:
            return True
        
        try:
            response = input(f"Proposed tool: {tool_name}. Purpose: {purpose}. Approve? (y/n): ").strip().lower()
            return response in ['y', 'yes']
        except (EOFError, KeyboardInterrupt):
            logging.info("User interrupted tool approval")
            return False
    
    def _execute_tool_with_validation(self, tool_func, tool_args: List[Any], tool_name: str, purpose: str) -> str:
        """
        Execute a LangGraph tool with command validation and approval
        
        Args:
            tool_func: Tool function to execute
            tool_args: Arguments for the tool (either positional args list or dict for named args)
            tool_name: Name of the tool for logging
            purpose: Purpose of the tool execution
            
        Returns:
            str: Tool output or error message
        """
        try:
            # Check user approval in interactive mode
            if not self._prompt_user_approval(tool_name, purpose):
                return f"Tool execution denied by user: {tool_name}"
            
            # Execute the tool
            logging.info(f"Executing tool: {tool_name} - {purpose}")
            
            # Handle different argument patterns for LangChain tools
            if hasattr(tool_func, 'invoke'):
                # New LangChain pattern - use invoke with proper argument structure
                if isinstance(tool_args, dict):
                    # Named arguments
                    result = tool_func.invoke(tool_args)
                elif isinstance(tool_args, list) and len(tool_args) > 0:
                    # Convert positional args to named args based on tool function signature
                    result = self._invoke_tool_with_positional_args(tool_func, tool_args)
                else:
                    # No arguments
                    result = tool_func.invoke({})
            else:
                # Fallback to direct function call for non-LangChain tools
                if tool_args:
                    result = tool_func(*tool_args)
                else:
                    result = tool_func()
            
            # Handle result extraction if it's wrapped in a response object
            if hasattr(result, 'content'):
                result_str = str(result.content)
            elif isinstance(result, dict) and 'output' in result:
                result_str = str(result['output'])
            else:
                result_str = str(result)
            
            # Store the result
            self.collected_data['tool_outputs'][f"{tool_name}_{int(time.time())}"] = {
                'tool': tool_name,
                'purpose': purpose,
                'output': result_str,
                'timestamp': time.time()
            }
            
            logging.debug(f"Tool {tool_name} completed successfully")
            return result_str
            
        except Exception as e:
            error_msg = f"Error executing tool {tool_name}: {str(e)}"
            logging.error(error_msg)
            self.collected_data['errors'].append(error_msg)
            return f"Error: {error_msg}"
    
    def _invoke_tool_with_positional_args(self, tool_func, tool_args: List[Any]) -> Any:
        """
        Helper method to invoke LangChain tools with positional arguments
        converted to named arguments based on function signature
        
        Args:
            tool_func: LangChain tool function
            tool_args: List of positional arguments
            
        Returns:
            Tool execution result
        """
        import inspect
        
        try:
            # Get function signature to map positional args to named args
            if hasattr(tool_func, 'func'):
                sig = inspect.signature(tool_func.func)
            else:
                sig = inspect.signature(tool_func)
            
            param_names = list(sig.parameters.keys())
            
            # Create named argument dict, filtering out None values
            named_args = {}
            for i, arg_value in enumerate(tool_args):
                if i < len(param_names) and arg_value is not None:
                    named_args[param_names[i]] = arg_value
            
            return tool_func.invoke(named_args)
            
        except Exception as e:
            # Fallback: try with first argument as input
            if tool_args:
                return tool_func.invoke({'input': tool_args[0]})
            else:
                return tool_func.invoke({})
    
    def _create_enhanced_context_summary(self, analysis: Dict[str, Any], fix_plan: Dict[str, Any], volume_chain: Dict[str, List[str]]) -> Dict[str, Any]:
        """Create enhanced context summary for troubleshooting"""
        return {
            'volume_chain_summary': {
                'total_pvcs': len(volume_chain.get('pvcs', [])),
                'total_pvs': len(volume_chain.get('pvs', [])),
                'total_drives': len(volume_chain.get('drives', [])),
                'total_nodes': len(volume_chain.get('nodes', [])),
                'storage_classes': volume_chain.get('storage_classes', [])
            },
            'collection_summary': {
                'total_tools_executed': len(self.collected_data['tool_outputs']),
                'total_errors': len(self.collected_data['errors']),
                'data_categories': list(self.collected_data.keys())
            },
            'knowledge_graph_summary': self.knowledge_graph.get_summary(),
            'analysis_summary': analysis,
            'fix_plan_summary': fix_plan
        }
</file>

<file path="knowledge_graph/__init__.py">
"""
Knowledge Graph module for Kubernetes Volume Troubleshooting

This module provides NetworkX-based Knowledge Graph functionality to organize
diagnostic data, entities, and relationships for comprehensive root cause analysis
and fix plan generation in the CSI Baremetal driver troubleshooting system.
"""

from .knowledge_graph import KnowledgeGraph

__all__ = ['KnowledgeGraph']
</file>

<file path="knowledge_graph/demo_graph_print.py">
#!/usr/bin/env python3
"""
Demo script to show the nice formatted Knowledge Graph output

This script demonstrates the print_graph functionality added to the KnowledgeGraph class.
"""

from knowledge_graph.knowledge_graph import KnowledgeGraph

def demo_knowledge_graph_print():
    """Demonstrate the formatted knowledge graph print functionality"""
    
    # Create a sample knowledge graph
    kg = KnowledgeGraph()
    
    # Add some sample entities
    pod_id = kg.add_gnode_pod("test-pod", "default", volume_path="/mnt/data")
    pvc_id = kg.add_gnode_pvc("test-pvc", "default", storageClass="csi-baremetal-sc")
    pv_id = kg.add_gnode_pv("pv-12345", disk_path="/dev/sdb1")
    drive_id = kg.add_gnode_drive("drive-uuid-123", Health="GOOD", Status="ONLINE", Path="/dev/sdb")
    node_id = kg.add_gnode_node("worker-node-1", Ready=True, DiskPressure=False)
    sc_id = kg.add_gnode_storage_class("csi-baremetal-sc", provisioner="csi-baremetal.dell.com")
    
    # Add relationships
    kg.add_relationship(pod_id, pvc_id, "uses")
    kg.add_relationship(pvc_id, pv_id, "bound_to")
    kg.add_relationship(pv_id, drive_id, "maps_to")
    kg.add_relationship(pv_id, node_id, "affinity_to")
    kg.add_relationship(pvc_id, sc_id, "uses_storage_class")
    
    # Add some sample issues
    kg.add_issue(pod_id, "permission", "Pod cannot write to volume due to permission denied", "medium")
    kg.add_issue(drive_id, "disk_health", "Drive showing early signs of wear", "low")
    
    # Add another problematic drive to show patterns
    bad_drive_id = kg.add_gnode_drive("drive-uuid-456", Health="SUSPECT", Status="ONLINE", Path="/dev/sdc")
    kg.add_issue(bad_drive_id, "disk_health", "Drive has health status: SUSPECT", "high")
    
    print("=" * 80)
    print("KNOWLEDGE GRAPH DEMO - FORMATTED OUTPUT")
    print("=" * 80)
    print()
    
    # Print the formatted graph
    formatted_output = kg.print_graph()
    print(formatted_output)
    
    print("\n" + "=" * 80)
    print("This demonstrates how the Knowledge Graph will be displayed")
    print("after build_knowledge_graph is called during troubleshooting!")
    print("=" * 80)

if __name__ == "__main__":
    demo_knowledge_graph_print()
</file>

<file path="monitoring/__init__.py">
"""
Monitoring module for Kubernetes Volume Troubleshooting

This module provides monitoring functionality for Kubernetes storage components
and system health in the CSI Baremetal driver troubleshooting system.
"""

# Import will be available once dependencies are resolved

__all__ = []
</file>

<file path="phases/__init__.py">
#!/usr/bin/env python3
"""
Phase Management for Kubernetes Volume Troubleshooting

This module contains the phase management system for the troubleshooting workflow,
including all phases: Information Collection, Plan, Analysis, and Remediation.
"""

# Plan Phase components
from .phase_plan_phase import PlanPhase, run_plan_phase
from .investigation_planner import InvestigationPlanner
from .kg_context_builder import KGContextBuilder
from .tool_registry_builder import ToolRegistryBuilder
from .llm_plan_generator import LLMPlanGenerator
from .rule_based_plan_generator import RuleBasedPlanGenerator

# Other phases
from .phase_information_collection import InformationCollectionPhase, run_information_collection_phase
from .phase_analysis import AnalysisPhase, run_analysis_phase_with_plan
from .phase_remediation import RemediationPhase, run_remediation_phase

__all__ = [
    # Plan Phase
    'PlanPhase',
    'run_plan_phase',
    'InvestigationPlanner',
    'KGContextBuilder',
    'ToolRegistryBuilder',
    'LLMPlanGenerator',
    'RuleBasedPlanGenerator',
    
    # Information Collection Phase
    'InformationCollectionPhase',
    'run_information_collection_phase',
    
    # Analysis Phase
    'AnalysisPhase',
    'run_analysis_phase_with_plan',
    
    # Remediation Phase
    'RemediationPhase',
    'run_remediation_phase'
]
</file>

<file path="phases/tool_registry_builder.py">
#!/usr/bin/env python3
"""
Tool Registry Builder for Investigation Planning

This module contains utilities for preparing the tool registry
for consumption by the Investigation Planner and LLM.
"""

import logging
import inspect
from typing import Dict, List, Any, Optional

logger = logging.getLogger(__name__)

class ToolRegistryBuilder:
    """
    Prepares tool registry for Investigation Planning
    
    Extracts tool information including names, descriptions, and parameters
    to provide context for investigation planning.
    """
    
    def __init__(self):
        """
        Initialize the Tool Registry Builder
        """
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def prepare_tool_registry(self) -> Dict[str, List[Dict[str, Any]]]:
        """
        Prepare tool registry for LLM consumption
        
        Returns:
            Dict[str, List[Dict[str, Any]]]: Structured tool registry
        """
        try:
            from tools.registry import get_phase1_tools
            
            # Get tools with error handling
            try:
                tools = get_phase1_tools()
            except Exception as e:
                self.logger.error(f"Error getting Phase 1 tools: {str(e)}")
                tools = []
                
            formatted_tools = []
            
            for tool in tools:
                try:
                    # Initialize variables
                    tool_name = None
                    tool_description = None
                    tool_parameters = []
                    
                    # Handle both function tools and StructuredTool objects
                    if hasattr(tool, 'name'):
                        # StructuredTool object
                        tool_name = tool.name
                        tool_description = tool.description if hasattr(tool, 'description') else "No description available"
                        
                        # Get parameters from args_schema if available
                        if hasattr(tool, 'args_schema') and tool.args_schema:
                            schema_fields = tool.args_schema.__fields__ if hasattr(tool.args_schema, '__fields__') else {}
                            for field_name, field in schema_fields.items():
                                required = field.required if hasattr(field, 'required') else True
                                field_type = str(field.type_) if hasattr(field, 'type_') else "unknown"
                                default_value = field.default if hasattr(field, 'default') and field.default is not inspect.Parameter.empty else None
                                
                                tool_parameters.append({
                                    "name": field_name,
                                    "type": field_type,
                                    "required": required,
                                    "default": default_value
                                })
                        # If no args_schema, try to get parameters from the function
                        elif hasattr(tool, 'func') and callable(tool.func):
                            signature = inspect.signature(tool.func)
                            for param_name, param in signature.parameters.items():
                                if param_name != 'self':
                                    param_type = "unknown"
                                    if param.annotation != inspect.Parameter.empty:
                                        param_type = str(param.annotation).replace("<class '", "").replace("'>", "")
                                    
                                    required = param.default == inspect.Parameter.empty
                                    default_value = None if required else param.default
                                    
                                    tool_parameters.append({
                                        "name": param_name,
                                        "type": param_type,
                                        "required": required,
                                        "default": default_value
                                    })
                    else:
                        # Function tool
                        tool_name = tool.__name__
                        tool_description = tool.__doc__.strip() if tool.__doc__ else "No description available"
                        
                        # Extract parameters from function signature
                        signature = inspect.signature(tool)
                        for param_name, param in signature.parameters.items():
                            if param_name != 'self':
                                param_type = "unknown"
                                if param.annotation != inspect.Parameter.empty:
                                    param_type = str(param.annotation).replace("<class '", "").replace("'>", "")
                                
                                required = param.default == inspect.Parameter.empty
                                default_value = None if required else param.default
                                
                                tool_parameters.append({
                                    "name": param_name,
                                    "type": param_type,
                                    "required": required,
                                    "default": default_value
                                })
                    
                    # Add the formatted tool to the list
                    if tool_name:
                        formatted_tools.append({
                            "name": tool_name,
                            "description": tool_description,
                            "parameters": tool_parameters
                        })
                except Exception as e:
                    self.logger.error(f"Error processing tool: {str(e)}")
                    continue
            
            # Group tools by category
            tool_categories = {
                "knowledge_graph": [],
                "kubernetes": [],
                "csi_baremetal": [],
                "system_diagnostics": [],
                "hardware_diagnostics": []
            }
            
            for tool in formatted_tools:
                if tool["name"].startswith("kg_"):
                    tool_categories["knowledge_graph"].append(tool)
                elif tool["name"].startswith("kubectl_get_") and not tool["name"] == "kubectl_get":
                    tool_categories["csi_baremetal"].append(tool)
                elif tool["name"].startswith("kubectl_"):
                    tool_categories["kubernetes"].append(tool)
                elif any(tool["name"].startswith(prefix) for prefix in ["df_", "lsblk_", "mount_", "dmesg_", "journalctl_"]):
                    tool_categories["system_diagnostics"].append(tool)
                else:
                    tool_categories["hardware_diagnostics"].append(tool)
            
            return tool_categories
            
        except Exception as e:
            self.logger.error(f"Error preparing tool registry: {str(e)}")
            return {
                "knowledge_graph": [],
                "kubernetes": [],
                "csi_baremetal": [],
                "system_diagnostics": [],
                "hardware_diagnostics": []
            }
    
    def get_tool_by_name(self, tool_name: str) -> Optional[Dict[str, Any]]:
        """
        Get tool information by name
        
        Args:
            tool_name: Name of the tool
            
        Returns:
            Optional[Dict[str, Any]]: Tool information or None if not found
        """
        try:
            tool_registry = self.prepare_tool_registry()
            
            # Search for the tool in all categories
            for category, tools in tool_registry.items():
                for tool in tools:
                    if tool["name"] == tool_name:
                        return tool
            
            return None
            
        except Exception as e:
            self.logger.error(f"Error getting tool by name: {str(e)}")
            return None
    
    def get_tools_by_category(self, category: str) -> List[Dict[str, Any]]:
        """
        Get tools by category
        
        Args:
            category: Tool category
            
        Returns:
            List[Dict[str, Any]]: List of tools in the category
        """
        try:
            tool_registry = self.prepare_tool_registry()
            
            if category in tool_registry:
                return tool_registry[category]
            
            return []
            
        except Exception as e:
            self.logger.error(f"Error getting tools by category: {str(e)}")
            return []
</file>

<file path="tools/core/__init__.py">
#!/usr/bin/env python3
"""
Core utilities for the troubleshooting tools.

This module contains:
- config: Global configuration management and command utilities
- knowledge_graph: Knowledge Graph tools and management
"""

from tools.core.config import (
    INTERACTIVE_MODE,
    CONFIG_DATA,
    validate_command,
    execute_command
)

from tools.core.knowledge_graph import (
    initialize_knowledge_graph,
    get_knowledge_graph,
    kg_get_entity_info,
    kg_get_related_entities,
    kg_get_all_issues,
    kg_find_path,
    kg_get_summary,
    kg_analyze_issues,
    kg_print_graph
)

__all__ = [
    # Configuration utilities
    'INTERACTIVE_MODE',
    'CONFIG_DATA',
    'validate_command',
    'execute_command',
    
    # Knowledge Graph management
    'initialize_knowledge_graph',
    'get_knowledge_graph',
    
    # Knowledge Graph tools
    'kg_get_entity_info',
    'kg_get_related_entities',
    'kg_get_all_issues',
    'kg_find_path',
    'kg_get_summary',
    'kg_analyze_issues',
    'kg_print_graph'
]
</file>

<file path="tools/diagnostics/__init__.py">
#!/usr/bin/env python3
"""
Diagnostic tools for volume troubleshooting.

This module contains:
- hardware: Hardware-level diagnostics (disk health, performance, file system checks)
- system: System-level diagnostics (disk space, mount points, logs)
"""

from tools.diagnostics.hardware import (
    smartctl_check,
    fio_performance_test,
    fsck_check,
    xfs_repair_check, 
    ssh_execute
)

from tools.diagnostics.system import (
    df_command,
    lsblk_command,
    mount_command,
    dmesg_command,
    journalctl_command
)

__all__ = [
    # Hardware diagnostic tools
    'smartctl_check',
    'fio_performance_test',
    'fsck_check',
    'xfs_repair_check',  # Added xfs_repair_check for XFS file system checks
    'ssh_execute',
    
    # System diagnostic tools
    'df_command',
    'lsblk_command',
    'mount_command',
    'dmesg_command',
    'journalctl_command'
]
</file>

<file path="tools/kubernetes/csi_baremetal.py">
#!/usr/bin/env python3
"""
CSI Baremetal specific tools for Kubernetes volume troubleshooting.

This module contains tools for interacting with CSI Baremetal custom resources
and storage-specific operations.
"""

import subprocess
from langchain_core.tools import tool

@tool
def kubectl_get_drive(drive_uuid: str = None, output_format: str = "wide") -> str:
    """
    Get CSI Baremetal drive information
    
    Args:
        drive_uuid: Drive UUID (optional, gets all drives if not specified)
        output_format: Output format (wide, yaml, json)
        
    Returns:
        str: Command output showing drive status, health, path, etc.
    """
    cmd = ["kubectl", "get", "drive"]
    
    if drive_uuid:
        cmd.append(drive_uuid)
    
    cmd.extend(["-o", output_format])
    
    try:
        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        return f"Error: {e.stderr}"
    except Exception as e:
        return f"Error executing kubectl get drive: {str(e)}"

@tool
def kubectl_get_csibmnode(node_name: str = None, output_format: str = "wide") -> str:
    """
    Get CSI Baremetal node information
    
    Args:
        node_name: Node name (optional, gets all nodes if not specified)
        output_format: Output format (wide, yaml, json)
        
    Returns:
        str: Command output showing node mapping and drive associations
    """
    cmd = ["kubectl", "get", "csibmnode"]
    
    if node_name:
        cmd.append(node_name)
    
    cmd.extend(["-o", output_format])
    
    try:
        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        return f"Error: {e.stderr}"
    except Exception as e:
        return f"Error executing kubectl get csibmnode: {str(e)}"

@tool
def kubectl_get_availablecapacity(ac_name: str = None, output_format: str = "wide") -> str:
    """
    Get CSI Baremetal available capacity information
    
    Args:
        ac_name: Available capacity name (optional, gets all if not specified)
        output_format: Output format (wide, yaml, json)
        
    Returns:
        str: Command output showing available capacity and storage class mapping
    """
    cmd = ["kubectl", "get", "ac"]
    
    if ac_name:
        cmd.append(ac_name)
    
    cmd.extend(["-o", output_format])
    
    try:
        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        return f"Error: {e.stderr}"
    except Exception as e:
        return f"Error executing kubectl get ac: {str(e)}"

@tool
def kubectl_get_logicalvolumegroup(lvg_name: str = None, output_format: str = "wide") -> str:
    """
    Get CSI Baremetal logical volume group information
    
    Args:
        lvg_name: Logical volume group name (optional, gets all if not specified)
        output_format: Output format (wide, yaml, json)
        
    Returns:
        str: Command output showing LVG health and associated drives
    """
    cmd = ["kubectl", "get", "lvg"]
    
    if lvg_name:
        cmd.append(lvg_name)
    
    cmd.extend(["-o", output_format])
    
    try:
        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        return f"Error: {e.stderr}"
    except Exception as e:
        return f"Error executing kubectl get lvg: {str(e)}"

@tool
def kubectl_get_storageclass(sc_name: str = None, output_format: str = "yaml") -> str:
    """
    Get storage class information
    
    Args:
        sc_name: Storage class name (optional, gets all if not specified)
        output_format: Output format (yaml, json, wide)
        
    Returns:
        str: Command output showing storage class configuration
    """
    cmd = ["kubectl", "get", "storageclass"]
    
    if sc_name:
        cmd.append(sc_name)
    
    cmd.extend(["-o", output_format])
    
    try:
        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        return f"Error: {e.stderr}"
    except Exception as e:
        return f"Error executing kubectl get storageclass: {str(e)}"

@tool
def kubectl_get_csidrivers(output_format: str = "wide") -> str:
    """
    Get CSI driver registration information
    
    Args:
        output_format: Output format (wide, yaml, json)
        
    Returns:
        str: Command output showing registered CSI drivers
    """
    cmd = ["kubectl", "get", "csidrivers", "-o", output_format]
    
    try:
        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        return f"Error: {e.stderr}"
    except Exception as e:
        return f"Error executing kubectl get csidrivers: {str(e)}"
</file>

<file path="tools/testing/__init__.py">
#!/usr/bin/env python3
"""
Testing tools for Kubernetes volume troubleshooting.

This module provides tools for creating test resources, running volume tests,
and cleaning up test environments during the remediation phase.
"""

from .pod_creation import (
    create_test_pod,
    create_test_pvc,
    create_test_storage_class
)

from .volume_testing import (
    run_volume_io_test,
    validate_volume_mount,
    test_volume_permissions
)

from .resource_cleanup import (
    cleanup_test_resources,
    list_test_resources
)

__all__ = [
    'create_test_pod',
    'create_test_pvc', 
    'create_test_storage_class',
    'run_volume_io_test',
    'validate_volume_mount',
    'test_volume_permissions',
    'cleanup_test_resources',
    'list_test_resources'
]
</file>

<file path="tools/testing/resource_cleanup.py">
#!/usr/bin/env python3
"""
Resource cleanup tools for managing test resources.

This module provides tools for cleaning up test pods, PVCs, and other
resources created during troubleshooting and testing.
"""

import json
from typing import Dict, Any, List
from langchain_core.tools import tool
from tools.core.config import validate_command, execute_command

@tool
def cleanup_test_resources(namespace: str = "default", 
                          resource_types: str = "pod,pvc,storageclass",
                          label_selector: str = "test-type=troubleshooting") -> str:
    """
    Clean up test resources created during troubleshooting
    
    Args:
        namespace: Kubernetes namespace to clean up
        resource_types: Comma-separated list of resource types (pod,pvc,storageclass)
        label_selector: Label selector to identify test resources
        
    Returns:
        str: Results of cleanup operations
    """
    results = []
    resource_list = [rt.strip() for rt in resource_types.split(",")]
    
    try:
        for resource_type in resource_list:
            # List resources first
            cmd = ["kubectl", "get", resource_type, "-n", namespace, 
                   "-l", label_selector, "-o", "name"]
            list_result = execute_command(cmd)
            
            if list_result.strip():
                results.append(f"Found {resource_type} resources to delete:\n{list_result}")
                
                # Delete resources
                cmd = ["kubectl", "delete", resource_type, "-n", namespace, 
                       "-l", label_selector, "--ignore-not-found=true"]
                delete_result = execute_command(cmd)
                results.append(f"Deleted {resource_type} resources:\n{delete_result}")
            else:
                results.append(f"No {resource_type} resources found with label {label_selector}")
        
        return "\n" + "="*50 + "\n".join(results)
        
    except Exception as e:
        return f"Error cleaning up test resources: {str(e)}"

@tool
def list_test_resources(namespace: str = "default", 
                       resource_types: str = "pod,pvc,storageclass,pv",
                       label_selector: str = "test-type=troubleshooting") -> str:
    """
    List all test resources created during troubleshooting
    
    Args:
        namespace: Kubernetes namespace to search
        resource_types: Comma-separated list of resource types to list
        label_selector: Label selector to identify test resources
        
    Returns:
        str: List of test resources
    """
    results = []
    resource_list = [rt.strip() for rt in resource_types.split(",")]
    
    try:
        for resource_type in resource_list:
            # For cluster-scoped resources like PV and StorageClass, don't use namespace
            if resource_type.lower() in ['pv', 'persistentvolume', 'storageclass']:
                cmd = ["kubectl", "get", resource_type, "-l", label_selector, "-o", "wide"]
            else:
                cmd = ["kubectl", "get", resource_type, "-n", namespace, 
                       "-l", label_selector, "-o", "wide"]
            
            list_result = execute_command(cmd)
            
            if list_result.strip():
                results.append(f"{resource_type.upper()} Resources:\n{list_result}")
            else:
                results.append(f"No {resource_type} resources found with label {label_selector}")
        
        return "\n" + "="*50 + "\n".join(results)
        
    except Exception as e:
        return f"Error listing test resources: {str(e)}"

@tool
def cleanup_specific_test_pod(pod_name: str, namespace: str = "default", 
                             cleanup_pvc: bool = True) -> str:
    """
    Clean up a specific test pod and optionally its PVC
    
    Args:
        pod_name: Name of the pod to delete
        namespace: Kubernetes namespace
        cleanup_pvc: Whether to also delete associated PVC
        
    Returns:
        str: Results of cleanup operation
    """
    results = []
    
    try:
        # Get pod details first
        cmd = ["kubectl", "get", "pod", pod_name, "-n", namespace, "-o", "yaml"]
        pod_details = execute_command(cmd)
        
        # Extract PVC names from pod spec if cleanup_pvc is True
        pvc_names = []
        if cleanup_pvc:
            try:
                import yaml
                pod_data = yaml.safe_load(pod_details)
                volumes = pod_data.get('spec', {}).get('volumes', [])
                for volume in volumes:
                    if 'persistentVolumeClaim' in volume:
                        pvc_name = volume['persistentVolumeClaim']['claimName']
                        pvc_names.append(pvc_name)
            except Exception as e:
                results.append(f"Warning: Could not parse pod YAML to find PVCs: {str(e)}")
        
        # Delete the pod
        cmd = ["kubectl", "delete", "pod", pod_name, "-n", namespace, "--ignore-not-found=true"]
        pod_delete_result = execute_command(cmd)
        results.append(f"Pod Deletion:\n{pod_delete_result}")
        
        # Delete associated PVCs if requested
        if cleanup_pvc and pvc_names:
            for pvc_name in pvc_names:
                cmd = ["kubectl", "delete", "pvc", pvc_name, "-n", namespace, "--ignore-not-found=true"]
                pvc_delete_result = execute_command(cmd)
                results.append(f"PVC Deletion ({pvc_name}):\n{pvc_delete_result}")
        
        return "\n" + "="*50 + "\n".join(results)
        
    except Exception as e:
        return f"Error cleaning up test pod {pod_name}: {str(e)}"

@tool
def cleanup_orphaned_pvs(label_selector: str = "test-type=troubleshooting") -> str:
    """
    Clean up orphaned PVs that are no longer bound to PVCs
    
    Args:
        label_selector: Label selector to identify test PVs
        
    Returns:
        str: Results of PV cleanup
    """
    results = []
    
    try:
        # List PVs with the label selector
        cmd = ["kubectl", "get", "pv", "-l", label_selector, "-o", "json"]
        pv_list_result = execute_command(cmd)
        
        if not pv_list_result.strip():
            return "No test PVs found with the specified label selector"
        
        try:
            import json
            pv_data = json.loads(pv_list_result)
            orphaned_pvs = []
            
            for pv in pv_data.get('items', []):
                pv_name = pv['metadata']['name']
                pv_status = pv.get('status', {}).get('phase', 'Unknown')
                
                # Check if PV is Available (not bound) or Failed
                if pv_status in ['Available', 'Failed']:
                    orphaned_pvs.append(pv_name)
                    results.append(f"Found orphaned PV: {pv_name} (Status: {pv_status})")
            
            # Delete orphaned PVs
            for pv_name in orphaned_pvs:
                cmd = ["kubectl", "delete", "pv", pv_name, "--ignore-not-found=true"]
                delete_result = execute_command(cmd)
                results.append(f"Deleted PV {pv_name}: {delete_result}")
            
            if not orphaned_pvs:
                results.append("No orphaned PVs found to clean up")
                
        except json.JSONDecodeError as e:
            return f"Error parsing PV list JSON: {str(e)}"
        
        return "\n" + "="*50 + "\n".join(results)
        
    except Exception as e:
        return f"Error cleaning up orphaned PVs: {str(e)}"

@tool
def force_cleanup_stuck_resources(namespace: str = "default", 
                                 resource_type: str = "pod", 
                                 resource_name: str = None,
                                 label_selector: str = "test-type=troubleshooting") -> str:
    """
    Force cleanup of stuck resources using finalizer removal
    
    Args:
        namespace: Kubernetes namespace
        resource_type: Type of resource (pod, pvc, etc.)
        resource_name: Specific resource name (optional)
        label_selector: Label selector for multiple resources
        
    Returns:
        str: Results of force cleanup
    """
    results = []
    
    try:
        # Get list of resources to force delete
        if resource_name:
            resources = [resource_name]
        else:
            if resource_type.lower() in ['pv', 'persistentvolume', 'storageclass']:
                cmd = ["kubectl", "get", resource_type, "-l", label_selector, "-o", "name"]
            else:
                cmd = ["kubectl", "get", resource_type, "-n", namespace, 
                       "-l", label_selector, "-o", "name"]
            
            list_result = execute_command(cmd)
            resources = [r.split('/')[-1] for r in list_result.strip().split('\n') if r.strip()]
        
        if not resources:
            return f"No {resource_type} resources found to force cleanup"
        
        for resource in resources:
            # Try normal delete first
            if resource_type.lower() in ['pv', 'persistentvolume', 'storageclass']:
                cmd = ["kubectl", "delete", resource_type, resource, "--timeout=30s"]
            else:
                cmd = ["kubectl", "delete", resource_type, resource, "-n", namespace, "--timeout=30s"]
            
            try:
                delete_result = execute_command(cmd)
                results.append(f"Normal delete of {resource}: {delete_result}")
            except Exception:
                # If normal delete fails, try force delete
                results.append(f"Normal delete failed for {resource}, trying force delete...")
                
                # Remove finalizers
                patch_cmd = '{"metadata":{"finalizers":null}}'
                if resource_type.lower() in ['pv', 'persistentvolume', 'storageclass']:
                    cmd = ["kubectl", "patch", resource_type, resource, 
                           "-p", patch_cmd, "--type=merge"]
                else:
                    cmd = ["kubectl", "patch", resource_type, resource, "-n", namespace,
                           "-p", patch_cmd, "--type=merge"]
                
                try:
                    patch_result = execute_command(cmd)
                    results.append(f"Removed finalizers from {resource}: {patch_result}")
                except Exception as e:
                    results.append(f"Failed to remove finalizers from {resource}: {str(e)}")
        
        return "\n" + "="*50 + "\n".join(results)
        
    except Exception as e:
        return f"Error force cleaning up resources: {str(e)}"
</file>

<file path="tools/__init__.py">
#!/usr/bin/env python3
"""
Tools package for Kubernetes volume troubleshooting.

This package provides a comprehensive set of tools organized by category:
- core: Configuration, utilities, and Knowledge Graph tools
- kubernetes: Kubernetes operations (core and CSI Baremetal specific)
- diagnostics: Hardware and system diagnostic tools
- registry: Centralized tool registration and discovery
"""

# Import main registry functions for easy access
from tools.registry import (
    get_all_tools,
    get_knowledge_graph_tools,
    get_kubernetes_tools,
    get_diagnostic_tools,
    get_phase1_tools,
    get_phase2_tools,
    get_testing_tools,
    get_remediation_tools,
    define_remediation_tools
)

# Import core utilities
from tools.core.config import (
    INTERACTIVE_MODE,
    CONFIG_DATA,
    validate_command,
    execute_command
)

from tools.core.knowledge_graph import (
    initialize_knowledge_graph,
    get_knowledge_graph,
    kg_get_entity_info,
    kg_get_related_entities,
    kg_get_all_issues,
    kg_find_path,
    kg_get_summary,
    kg_analyze_issues,
    kg_print_graph
)

# Import all individual tools for backward compatibility
from tools.kubernetes.core import (
    kubectl_get,
    kubectl_describe,
    kubectl_apply,
    kubectl_delete,
    kubectl_exec,
    kubectl_logs
)

from tools.kubernetes.csi_baremetal import (
    kubectl_get_drive,
    kubectl_get_csibmnode,
    kubectl_get_availablecapacity,
    kubectl_get_logicalvolumegroup,
    kubectl_get_storageclass,
    kubectl_get_csidrivers
)

from tools.diagnostics.hardware import (
    smartctl_check,
    fio_performance_test,
    fsck_check,
    xfs_repair_check,  # Added xfs_repair_check for XFS file system checks
    ssh_execute
)

from tools.diagnostics.system import (
    df_command,
    lsblk_command,
    mount_command,
    dmesg_command,
    journalctl_command
)

__all__ = [
    # Registry functions
    'get_all_tools',
    'get_knowledge_graph_tools',
    'get_kubernetes_tools',
    'get_diagnostic_tools',
    'get_phase1_tools',
    'get_phase2_tools',
    'get_testing_tools',
    'get_remediation_tools',
    'define_remediation_tools',
    
    # Core utilities
    'INTERACTIVE_MODE',
    'CONFIG_DATA',
    'validate_command',
    'execute_command',
    'initialize_knowledge_graph',
    'get_knowledge_graph',
    
    # Knowledge Graph tools
    'kg_get_entity_info',
    'kg_get_related_entities',
    'kg_get_all_issues',
    'kg_find_path',
    'kg_get_summary',
    'kg_analyze_issues',
    'kg_print_graph',
    
    # Kubernetes core tools
    'kubectl_get',
    'kubectl_describe',
    'kubectl_apply',
    'kubectl_delete',
    'kubectl_exec',
    'kubectl_logs',
    
    # CSI Baremetal tools
    'kubectl_get_drive',
    'kubectl_get_csibmnode',
    'kubectl_get_availablecapacity',
    'kubectl_get_logicalvolumegroup',
    'kubectl_get_storageclass',
    'kubectl_get_csidrivers',
    
    # Hardware diagnostic tools
    'smartctl_check',
    'fio_performance_test',
    'fsck_check',
    'xfs_repair_check',
    'ssh_execute',
    
    # System diagnostic tools
    'df_command',
    'lsblk_command',
    'mount_command',
    'dmesg_command',
    'journalctl_command'
]
</file>

<file path="docs/mcp_agent_example.py">
"""
Smart Travel System Implementation - Based on Amap MCP + SSE + langchain_mcp_adapters
"""
from langchain.agents import create_openai_functions_agent
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain.agents import AgentExecutor
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
import os
import asyncio
from dotenv import load_dotenv
from langchain.chat_models import init_chat_model
from langgraph.prebuilt import create_react_agent

# Load environment variables
load_dotenv(override=True)

os.environ['GAODE_MAP_KEY'] = ''
#os.environ["OPENAI_API_KEY"] = '.'
#os.environ["OPENAI_API_KEY"] = 'sk-proj--Axy0c--'
os.environ["OPENAI_API_KEY"] = 'sk-'
os.environ['LANGCHAIN_TRACING_V2'] = "true"   
os.environ['LANGCHAIN_ENDPOINT'] = "https://api.smith.langchain.com"   
os.environ['LANGCHAIN_API_KEY'] = ""   
os.environ['LANGCHAIN_PROJECT'] = "pr-silver-bank-1"

# Initialize language model
llm_ds = init_chat_model(
    "gpt-4o-mini",
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url='https://api.zhizengzeng.com/v1/',
    #base_url='https://api.openai.com/v1/',
    #base_url='https://open.bigmodel.cn/api/paas/v4',
    model_provider="openai"
)


async def run_agent(query):
    """
    Implement smart travel assistant, process user queries and provide answers
    
    Parameters:
        query: User query string, for example "How to get from Beijing to Xi'an? And help me plan famous tourist attractions along the way"
        
    Returns:
        Dictionary containing query results
    """
    # Get Amap API key
    gaode_map_key = os.getenv("GAODE_MAP_KEY")
    
    # Create MCP client connecting to Amap API - using dictionary configuration
    client = MultiServerMCPClient(
        {
            "search": {
                "url": f"https://mcp.amap.com/sse?key={gaode_map_key}",
                "transport": "sse",
            }
        }
    )
    
    try:
        # Get MCP tools
        mcptools = await client.get_tools()
        
        # Create tools description string
        tools_description = "Available tools:\n"
        for tool in mcptools:
            tools_description += f"- {tool.name}: {tool.description}\n"

        prompt = ChatPromptTemplate.from_messages([
            ("system", "You are a helpful intelligent assistant. Please carefully analyze user questions and use the provided tools to answer questions."),
            ("user", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad"),
        ])

        agent = create_openai_functions_agent(
            llm=llm_ds,
            tools=mcptools,
            prompt=prompt
        )
        
        # Create agent executor
        agent_executor = AgentExecutor(
            agent=agent,
            tools=mcptools,
            verbose=True,
            max_iterations=5,
            return_intermediate_steps=True,  # Return intermediate steps for debugging
            handle_parsing_errors=True
        )
        
        # Run agent to process user query
        agent_response = await agent_executor.ainvoke({
            "input": query
        })
        
        # Return formatted response
        return {
            "status": "success",
            "result": agent_response.get("output", ""),
            "steps": len(agent_response.get("intermediate_steps", [])),
        }
    
    except Exception as e:
        # Capture and return error information
        return {
            "status": "error",
            "error": str(e)
        }
    finally:
        # Ensure resources are properly closed
        #await client.aclose()
        pass


async def call_agent_with_specific_context(query, context=None):
    """
    Call agent with specific context
    
    Parameters:
        query: User query
        context: Additional context (optional)
    
    Returns:
        Agent response
    """
    # Combine query and context
    full_query = query
    if context:
        full_query = f"Context information: {context}\n\nUser query: {query}"
    
    # Call agent
    return await run_agent(full_query)


async def main():
    """Run example query"""
    query = "How to get from Beijing to Xi'an? And help me plan famous tourist attractions along the way"
    
    print(f"Processing query: {query}")
    result = await run_agent(query)
    
    if result["status"] == "success":
        print(f"Query result: {result['result']}")
        print(f"Number of processing steps: {result['steps']}")
    else:
        print(f"Processing error: {result.get('error', 'Unknown error')}")


# Example using LangGraph StateGraph (not enabled, for reference only)
async def run_with_state_graph():
    """
    Run agent using LangGraph StateGraph (example code, not enabled)
    """
    from langgraph.graph import StateGraph, MessagesState, START
    from langgraph.prebuilt import ToolNode, tools_condition
    
    # Get Amap API key
    gaode_map_key = os.getenv("GAODE_MAP_KEY")
    
    # Create MCP client
    client = MultiServerMCPClient(
        {
            "search": {
                "url": f"https://mcp.amap.com/sse?key={gaode_map_key}",
                "transport": "sse",
            }
        }
    )
    
    # Get tools
    tools = await client.get_tools()
    
    def call_model(state: MessagesState):
        response = llm_ds.bind_tools(tools).invoke(state["messages"])
        return {"messages": response}
    
    # Build state graph
    builder = StateGraph(MessagesState)
    builder.add_node(call_model)
    builder.add_node(ToolNode(tools))
    builder.add_edge(START, "call_model")
    builder.add_conditional_edges(
        "call_model",
        tools_condition,
    )
    builder.add_edge("tools", "call_model")
    graph = builder.compile()
    
    # Run query
    response = await graph.ainvoke({"messages": "How to get from Beijing to Xi'an? And help me plan famous tourist attractions along the way"})
    
    # Clean up resources
    await client.aclose()
    
    return response


if __name__ == "__main__":
    # Run example query
    print("Smart Travel Assistant Example - Based on Amap MCP + SSE + langchain_mcp_adapters")
    asyncio.run(main())
</file>

<file path="information_collector/collector.py">
"""
Main Information Collector

Combines all components to provide the main ComprehensiveInformationCollector class.
"""

import os
import yaml
import logging
import asyncio
import time
import subprocess
import json
import paramiko
from typing import Dict, List, Any, Optional, Set, Tuple
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# Get logger configured in __init__.py
from . import collector_logger

from .base import InformationCollectorBase
from .volume_discovery import VolumeDiscovery
from .tool_executors import ToolExecutors
from .knowledge_builder import KnowledgeBuilder


class ComprehensiveInformationCollector(VolumeDiscovery, ToolExecutors, KnowledgeBuilder):
    """Enhanced Volume-Focused Information Collector for Phase 0"""
    
    def __init__(self, config_data: Dict[str, Any]):
        """Initialize the Enhanced Information Collector"""
        super().__init__(config_data)
        collector_logger.info("Enhanced Volume-Focused Information Collector initialized")
    
    async def comprehensive_collect(self, 
                                   target_pod: str = None, 
                                   target_namespace: str = None,
                                   target_volume_path: str = None) -> Dict[str, Any]:
        """
        Perform enhanced volume-focused data collection using LangGraph tools
        
        This is the main entry point for Phase 0: Information-Collection Phase
        Executes diagnostic LangGraph tools according to parameter's volume path and pod
        """
        collector_logger.info("=== PHASE 0: INFORMATION-COLLECTION - Starting volume-focused data collection ===")
        start_time = time.time()
        
        try:
            # Step 1: Discover volume dependency chain
            collector_logger.info("Step 1: Discovering volume dependency chain...")
            volume_chain = {}
            if target_pod and target_namespace:
                volume_chain = self._discover_volume_dependency_chain(target_pod, target_namespace)
            
            # Step 2: Execute volume-focused tools based on discovered chain
            collector_logger.info("Step 2: Executing volume-focused diagnostic tools...")
            
            # Pod discovery tools
            if target_pod and target_namespace:
                await self._execute_pod_discovery_tools(target_pod, target_namespace)
            
            # Volume chain discovery tools
            await self._execute_volume_chain_tools(volume_chain, target_volume_path)
            
            # CSI Baremetal discovery tools
            await self._execute_csi_baremetal_tools(volume_chain.get('drives', []))
            
            # Node and system discovery tools
            await self._execute_node_system_tools(volume_chain.get('nodes', []))
            
            # SMART data collection tools
            await self._execute_smart_data_tools(volume_chain.get('drives', []))
            
            # Enhanced log analysis tools
            await self._execute_enhanced_log_analysis_tools(volume_chain.get('nodes', []))
            
            # Step 3: Build enhanced Knowledge Graph from tool outputs
            collector_logger.info("Step 3: Building Knowledge Graph from tool outputs...")
            self.knowledge_graph = await self._build_knowledge_graph_from_tools(
                target_pod, target_namespace, target_volume_path, volume_chain
            )
            
            # Step 4: Perform analysis
            collector_logger.info("Step 4: Analyzing Knowledge Graph...")
            analysis = self.knowledge_graph.analyze_issues()
            fix_plan = self.knowledge_graph.generate_fix_plan(analysis)
            
            # Step 5: Create enhanced context summary
            context_summary = self._create_enhanced_context_summary(analysis, fix_plan, volume_chain)
            
            # Final collection summary
            collection_time = time.time() - start_time
            
            result = {
                'collected_data': self.collected_data,
                'knowledge_graph': self.knowledge_graph,
                'context_summary': context_summary,
                'volume_chain': volume_chain,
                'collection_metadata': {
                    'collection_time': collection_time,
                    'target_pod': target_pod,
                    'target_namespace': target_namespace,
                    'target_volume_path': target_volume_path,
                    'tools_executed': len(self.collected_data['tool_outputs']),
                    'total_errors': len(self.collected_data['errors']),
                    'interactive_mode': self.interactive_mode
                }
            }
            
            collector_logger.info(f"=== PHASE 0: INFORMATION-COLLECTION completed in {collection_time:.2f} seconds ===")
            collector_logger.info(f"Executed {len(self.collected_data['tool_outputs'])} tools, discovered {len(volume_chain.get('pvcs', []))} PVCs")
            return result
            
        except Exception as e:
            error_msg = f"Error during volume-focused collection: {str(e)}"
            collector_logger.error(error_msg)
            self.collected_data['errors'].append(error_msg)
            raise
</file>

<file path="knowledge_graph/tools.py">
#!/usr/bin/env python3
"""
LangGraph Tools for Kubernetes Volume I/O Error Troubleshooting

This module serves as a compatibility layer for the reorganized tools package.
All tools have been moved to the tools/ package with better organization.

For new code, import directly from the tools package:
    from tools import get_all_tools, initialize_knowledge_graph
    from tools.kubernetes import kubectl_get
    from tools.diagnostics import smartctl_check
"""

# Import everything from the new tools package for backward compatibility
from tools import *
from tools.registry import define_remediation_tools

# Re-export core utilities with original names for compatibility
from tools.core.config import (
    INTERACTIVE_MODE,
    CONFIG_DATA,
    validate_command,
    execute_command
)

from tools.core.knowledge_graph import (
    initialize_knowledge_graph,
    get_knowledge_graph
)

# Re-export all individual tools for backward compatibility
from tools.core.knowledge_graph import (
    kg_get_entity_info,
    kg_get_related_entities,
    kg_get_all_issues,
    kg_find_path,
    kg_get_summary,
    kg_analyze_issues,
    kg_print_graph,
    # Entity ID helper tools
    kg_get_entity_of_pod,
    kg_get_entity_of_pvc,
    kg_get_entity_of_pv,
    kg_get_entity_of_drive,
    kg_get_entity_of_node,
    kg_get_entity_of_storage_class,
    kg_get_entity_of_lvg,
    kg_get_entity_of_ac,
    kg_get_entity_of_volume,
    kg_get_entity_of_system,
    kg_get_entity_of_cluster_node,
    kg_get_entity_of_historical_experience
)

from tools.kubernetes.core import (
    kubectl_get,
    kubectl_describe,
    kubectl_apply,
    kubectl_delete,
    kubectl_exec,
    kubectl_logs
)

from tools.kubernetes.csi_baremetal import (
    kubectl_get_drive,
    kubectl_get_csibmnode,
    kubectl_get_availablecapacity,
    kubectl_get_logicalvolumegroup,
    kubectl_get_storageclass,
    kubectl_get_csidrivers
)

from tools.diagnostics.hardware import (
    smartctl_check,
    fio_performance_test,
    fsck_check,
    xfs_repair_check,  # Added xfs_repair_check for XFS file system checks
    ssh_execute
)

from tools.diagnostics.system import (
    df_command,
    lsblk_command,
    mount_command,
    dmesg_command,
    journalctl_command
)

# Maintain the original function for backward compatibility
def define_remediation_tools():
    """
    Define tools needed for remediation and analysis phases
    
    Returns:
        List[Any]: List of tool callables for investigation and remediation
    """
    return get_all_tools()
</file>

<file path="llm_graph/graphs/__init__.py">
"""
LangGraph implementations for different phases of the troubleshooting system.
"""

from llm_graph.graphs.plan_llm_graph import PlanLLMGraph
from llm_graph.graphs.phase1_llm_graph import Phase1LLMGraph
from llm_graph.graphs.phase2_llm_graph import Phase2LLMGraph

__all__ = ['PlanLLMGraph', 'Phase1LLMGraph', 'Phase2LLMGraph']
</file>

<file path="llm_graph/graphs/phase1_llm_graph.py">
#!/usr/bin/env python3
"""
Phase 1 (Analysis) LangGraph Implementation for Kubernetes Volume Troubleshooting

This module implements the LangGraph workflow for the Analysis phase
of the troubleshooting system using the Strategy Pattern.
"""

import logging
import asyncio
from typing import Dict, List, Any, TypedDict, Optional, Union, Tuple, Set
from enum import Enum

from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import tools_condition
from langchain_core.messages import BaseMessage, ToolMessage, HumanMessage, SystemMessage, AIMessage
from langchain_core.tools import BaseTool

from llm_graph.langgraph_interface import LangGraphInterface
from llm_graph.graph_utility import GraphUtility
from llm_graph.prompt_managers.phase1_prompt_manager import Phase1PromptManager
from phases.llm_factory import LLMFactory
from phases.utils import handle_exception
from tools.core.mcp_adapter import get_mcp_adapter

logger = logging.getLogger(__name__)

class Phase1State(TypedDict):
    """State for the Analysis Phase ReAct graph"""
    messages: List[BaseMessage]  # Conversation history
    iteration_count: int  # Track iterations
    tool_call_count: int  # Track tool calls
    goals_achieved: List[str]  # Track achieved goals
    root_cause_identified: bool  # Whether root cause was identified
    investigation_plan: str  # Investigation plan to follow

class Phase1LLMGraph(LangGraphInterface):
    """
    LangGraph implementation for the Analysis phase (Phase 1)
    
    Implements the LangGraphInterface for the Analysis phase,
    which executes the Investigation Plan to identify root causes.
    """
    
    def __init__(self, config_data: Dict[str, Any] = None):
        """
        Initialize the Phase 1 LLM Graph
        
        Args:
            config_data: Configuration data for the system
        """
        self.config_data = config_data or {}
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.graph_utility = GraphUtility(config_data)
        self.prompt_manager = Phase1PromptManager(config_data)
        
        # Initialize LLM
        self.llm = self._initialize_llm()
        
        # Get MCP tools
        self.mcp_tools = self._get_mcp_tools_for_phase1()
        
        # Load tool configuration
        self.parallel_tools, self.serial_tools = self._load_tool_config()
        
        # Maximum iterations to prevent infinite loops
        self.max_iterations = self.config_data.get("max_iterations", 30)
    
    def _initialize_llm(self):
        """
        Initialize the LLM for the ReAct graph
        
        Returns:
            BaseChatModel: Initialized LLM instance
        """
        try:
            # Create LLM using the factory
            llm_factory = LLMFactory(self.config_data)
            
            # Check if streaming is enabled in config
            streaming_enabled = self.config_data.get('llm', {}).get('streaming', False)
            
            # Create LLM with streaming if enabled
            return llm_factory.create_llm(
                streaming=streaming_enabled,
                phase_name="phase1"
            )
        except Exception as e:
            error_msg = handle_exception("_initialize_llm", e, self.logger)
            raise ValueError(f"Failed to initialize LLM: {error_msg}")
    
    def _get_mcp_tools_for_phase1(self) -> List[BaseTool]:
        """
        Get MCP tools for Phase 1
        
        Returns:
            List[BaseTool]: List of MCP tools for Phase 1
        """
        # Get MCP adapter
        mcp_adapter = get_mcp_adapter()
        
        if not mcp_adapter:
            self.logger.warning("MCP adapter not initialized, no MCP tools will be available")
            return []
        
        if not mcp_adapter.mcp_enabled:
            self.logger.warning("MCP integration is disabled, no MCP tools will be available")
            return []
        
        # Get MCP tools for phase1
        mcp_tools = mcp_adapter.get_tools_for_phase('phase1')
        
        if not mcp_tools:
            self.logger.warning("No MCP tools available for Phase 1")
            return []
        
        self.logger.info(f"Loaded {len(mcp_tools)} MCP tools for Phase 1")
        return mcp_tools
    
    def _load_tool_config(self) -> Tuple[Set[str], Set[str]]:
        """
        Load tool configuration to determine which tools
        should be executed in parallel and which should be executed serially.
        
        Returns:
            Tuple[Set[str], Set[str]]: Sets of parallel and serial tool names
        """
        try:
            import yaml
            
            with open('config.yaml', 'r') as f:
                config_data = yaml.safe_load(f)
                
            tool_config = config_data.get("tools", {})
            parallel_tools = set(tool_config.get("parallel", []))
            serial_tools = set(tool_config.get("serial", []))
            
            # Log the configuration
            self.logger.info(f"Loaded tool configuration: {len(parallel_tools)} parallel tools, {len(serial_tools)} serial tools")
            
            return parallel_tools, serial_tools
        except Exception as e:
            self.logger.error(f"Error loading tool configuration: {e}")
            # Return empty sets as fallback (all tools will be treated as serial)
            return set(), set()
    
    def initialize_graph(self) -> StateGraph:
        """
        Initialize and return the LangGraph StateGraph
        
        Returns:
            StateGraph: Compiled LangGraph StateGraph
        """
        # Build state graph
        self.logger.info("Building Phase 1 ReAct graph")
        builder = StateGraph(Phase1State)
        
        # Add nodes
        self.logger.info("Adding node: call_model")
        builder.add_node("call_model", self.call_model)
        
        # Initialize ExecuteToolNode with parallel/serial tool configuration
        execute_tools_node = self.graph_utility.create_execute_tool_node(
            self.mcp_tools, self.parallel_tools, self.serial_tools
        )
        self.logger.info("Adding node: execute_tools")
        builder.add_node("execute_tools", execute_tools_node)
        
        self.logger.info("Adding node: check_end")
        builder.add_node("check_end", self.check_end_conditions)
        
        # Add edges
        self.logger.info("Adding edge: START -> call_model")
        builder.add_edge(START, "call_model")
        
        # Add conditional edges for tools
        self.logger.info("Adding conditional edges for tools")
        builder.add_conditional_edges(
            "call_model",
            tools_condition,
            {
                "tools": "execute_tools",   # Route to execute_tools node
                "none": "check_end",        # If no tools, go to check_end
                "__end__": "check_end"
            }
        )
        
        # Add edge from execute_tools to call_model
        self.logger.info("Adding edge: execute_tools -> call_model")
        builder.add_edge("execute_tools", "call_model")
        
        # Add conditional edges from check_end node
        self.logger.info("Adding conditional edges from check_end node")
        builder.add_conditional_edges(
            "check_end",
            lambda state: self.check_end_conditions(state)["result"],
            {
                "end": END,
                "__end__": END,
                "continue": "call_model"  # Loop back if conditions not met
            }
        )
        
        # Compile graph
        self.logger.info("Compiling graph")
        return builder.compile()
    
    def call_model(self, state: Phase1State) -> Phase1State:
        """
        LLM reasoning node that analyzes current state and decides next action
        
        Args:
            state: Current state of the graph
            
        Returns:
            Phase1State: Updated state after LLM reasoning
        """
        # Increment iteration count
        state["iteration_count"] += 1
        
        # Check if we've reached max iterations
        if state["iteration_count"] > self.max_iterations:
            self.logger.info(f"Reached max iterations ({self.max_iterations}), marking analysis as complete")
            
            # Add a final message indicating max iterations reached
            final_message = AIMessage(content=f"[MAX_ITERATIONS_REACHED] Completed {self.max_iterations} iterations. Finalizing analysis with current information.")
            state["messages"].append(final_message)
            
            return state
        
        self.logger.info(f"Calling model (iteration {state['iteration_count']})")
        
        try:
            # Call the model with tools
            response = None
            if len(self.mcp_tools) != 0:
                response = self.llm.bind_tools(self.mcp_tools).invoke(state["messages"])
            else:
                response = self.llm.invoke(state["messages"])
            
            # Add response to messages
            state["messages"].append(response)
            
            # Log the response
            self.logger.info(f"Model response: {response.content[:100]}...")
            
            return state
        except Exception as e:
            error_msg = handle_exception("call_model", e, self.logger)
            
            # Add error message to state
            error_message = SystemMessage(content=f"Error calling model: {error_msg}")
            state["messages"].append(error_message)
            
            return state
    
    def check_end_conditions(self, state: Phase1State) -> Dict[str, str]:
        """
        Check if analysis is complete
        
        Args:
            state: Current state of the graph
            
        Returns:
            Dict[str, str]: Result indicating whether to end or continue
        """
        self.logger.info("Checking end conditions for Phase 1 ReAct graph")
        
        # Check if we've reached max iterations
        if state["iteration_count"] >= self.max_iterations:
            self.logger.info(f"Reached max iterations ({self.max_iterations}), ending graph")
            return {"result": "end"}
        
        # Get the last message
        messages = state["messages"]
        if not messages:
            return {"result": "continue"}
        
        last_message = messages[-1]
        
        # Skip content checks if the last message isn't from the AI
        if getattr(last_message, "type", "") != "ai":
            return {"result": "continue"}
        
        content = getattr(last_message, "content", "")
        if not content:
            return {"result": "continue"}
        
        # Check for explicit end markers in the content
        end_markers = ["[END_GRAPH]", "[END]", "End of graph", "GRAPH END", "CONCLUSION:", "SUMMARY OF FINDINGS:"]
        if any(marker in content for marker in end_markers):
            self.logger.info(f"Detected end marker in content, ending graph")
            return {"result": "end"}
        
        # Check for completion indicators
        if "Summary of Findings:" in content and "Root Cause:" in content:
            self.logger.info("Detected completion indicators in content, ending graph")
            state["root_cause_identified"] = True
            return {"result": "end"}
        
        # Check for convergence (model repeating itself)
        ai_messages = [m for m in messages if getattr(m, "type", "") == "ai"]
        if len(ai_messages) > 3:
            # Compare the last message with the third-to-last message
            last_content = content
            third_to_last_content = getattr(ai_messages[-3], "content", "")
            
            # Simple similarity check - if they start with the same paragraph
            if last_content and third_to_last_content:
                # Get first 100 chars of each message
                last_start = last_content[:100] if len(last_content) > 100 else last_content
                third_start = third_to_last_content[:100] if len(third_to_last_content) > 100 else third_to_last_content
                
                if last_start == third_start:
                    self.logger.info("Detected convergence (model repeating itself), ending graph")
                    return {"result": "end"}
        
        # Default: continue execution
        return {"result": "continue"}
    
    def extract_analysis_from_state(self, state: Phase1State) -> str:
        """
        Extract the analysis results from the final state
        
        Args:
            state: Final state of the graph
            
        Returns:
            str: Analysis results as a formatted string
        """
        # Get the last AI message
        ai_messages = [m for m in state["messages"] if getattr(m, "type", "") == "ai"]
        
        if not ai_messages:
            self.logger.warning("No AI messages found in final state")
            return "Failed to generate analysis results."
        
        # Get the content of the last AI message
        last_ai_message = ai_messages[-1]
        content = getattr(last_ai_message, "content", "")
        
        # Remove end markers
        for marker in ["[END_GRAPH]", "[END]", "End of graph", "GRAPH END"]:
            content = content.replace(marker, "")
            
        return content.strip()
    
    async def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the graph with the provided state
        
        Args:
            state: Initial state for the graph execution
            
        Returns:
            Dict[str, Any]: Final state after graph execution
        """
        try:
            # Initialize the graph
            graph = self.initialize_graph()
            
            # Prepare initial state
            initial_state = {
                "messages": state.get("messages", []),
                "iteration_count": 0,
                "tool_call_count": 0,
                "goals_achieved": [],
                "root_cause_identified": False,
                "investigation_plan": state.get("investigation_plan", "")
            }
            
            # Run the graph
            self.logger.info("Running Phase 1 ReAct graph")
            final_state = graph.invoke(initial_state)
            
            # Extract the analysis results
            analysis_results = self.extract_analysis_from_state(final_state)
            
            # Add the analysis results to the final state
            final_state["analysis_results"] = analysis_results
            
            return final_state
            
        except Exception as e:
            error_msg = handle_exception("execute", e, self.logger)
            
            return {
                "status": "error",
                "error_message": error_msg,
                "analysis_results": "Failed to complete analysis due to an error.",
                "messages": state.get("messages", [])
            }
    
    def get_prompt_manager(self):
        """
        Return the prompt manager for this graph
        
        Returns:
            Phase1PromptManager: Prompt manager for the Analysis phase
        """
        return self.prompt_manager
</file>

<file path="llm_graph/graphs/phase2_llm_graph.py">
#!/usr/bin/env python3
"""
Phase 2 (Remediation) LangGraph Implementation for Kubernetes Volume Troubleshooting

This module implements the LangGraph workflow for the Remediation phase
of the troubleshooting system using the Strategy Pattern.
"""

import logging
import asyncio
from typing import Dict, List, Any, TypedDict, Optional, Union, Tuple, Set
from enum import Enum

from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import tools_condition
from langchain_core.messages import BaseMessage, ToolMessage, HumanMessage, SystemMessage, AIMessage
from langchain_core.tools import BaseTool

from llm_graph.langgraph_interface import LangGraphInterface
from llm_graph.graph_utility import GraphUtility
from llm_graph.prompt_managers.phase2_prompt_manager import Phase2PromptManager
from phases.llm_factory import LLMFactory
from phases.utils import handle_exception
from tools.core.mcp_adapter import get_mcp_adapter

logger = logging.getLogger(__name__)

class Phase2State(TypedDict):
    """State for the Remediation Phase ReAct graph"""
    messages: List[BaseMessage]  # Conversation history
    iteration_count: int  # Track iterations
    tool_call_count: int  # Track tool calls
    goals_achieved: List[str]  # Track achieved goals
    fix_plan_executed: bool  # Whether fix plan was executed
    phase1_final_response: str  # Response from Phase 1 containing root cause and fix plan

class Phase2LLMGraph(LangGraphInterface):
    """
    LangGraph implementation for the Remediation phase (Phase 2)
    
    Implements the LangGraphInterface for the Remediation phase,
    which executes the Fix Plan to resolve identified issues.
    """
    
    def __init__(self, config_data: Dict[str, Any] = None):
        """
        Initialize the Phase 2 LLM Graph
        
        Args:
            config_data: Configuration data for the system
        """
        self.config_data = config_data or {}
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.graph_utility = GraphUtility(config_data)
        self.prompt_manager = Phase2PromptManager(config_data)
        
        # Initialize LLM
        self.llm = self._initialize_llm()
        
        # Get MCP tools
        self.mcp_tools = self._get_mcp_tools_for_phase2()
        
        # Load tool configuration
        self.parallel_tools, self.serial_tools = self._load_tool_config()
        
        # Maximum iterations to prevent infinite loops
        self.max_iterations = self.config_data.get("max_iterations", 30)
    
    def _initialize_llm(self):
        """
        Initialize the LLM for the ReAct graph
        
        Returns:
            BaseChatModel: Initialized LLM instance
        """
        try:
            # Create LLM using the factory
            llm_factory = LLMFactory(self.config_data)
            
            # Check if streaming is enabled in config
            streaming_enabled = self.config_data.get('llm', {}).get('streaming', False)
            
            # Create LLM with streaming if enabled
            return llm_factory.create_llm(
                streaming=streaming_enabled,
                phase_name="phase2"
            )
        except Exception as e:
            error_msg = handle_exception("_initialize_llm", e, self.logger)
            raise ValueError(f"Failed to initialize LLM: {error_msg}")
    
    def _get_mcp_tools_for_phase2(self) -> List[BaseTool]:
        """
        Get MCP tools for Phase 2
        
        Returns:
            List[BaseTool]: List of MCP tools for Phase 2
        """
        # Get MCP adapter
        mcp_adapter = get_mcp_adapter()
        
        if not mcp_adapter:
            self.logger.warning("MCP adapter not initialized, no MCP tools will be available")
            return []
        
        if not mcp_adapter.mcp_enabled:
            self.logger.warning("MCP integration is disabled, no MCP tools will be available")
            return []
        
        # Get MCP tools for phase2
        mcp_tools = mcp_adapter.get_tools_for_phase('phase2')
        
        if not mcp_tools:
            self.logger.warning("No MCP tools available for Phase 2")
            return []
        
        self.logger.info(f"Loaded {len(mcp_tools)} MCP tools for Phase 2")
        return mcp_tools
    
    def _load_tool_config(self) -> Tuple[Set[str], Set[str]]:
        """
        Load tool configuration to determine which tools
        should be executed in parallel and which should be executed serially.
        
        Returns:
            Tuple[Set[str], Set[str]]: Sets of parallel and serial tool names
        """
        try:
            import yaml
            
            with open('config.yaml', 'r') as f:
                config_data = yaml.safe_load(f)
                
            tool_config = config_data.get("tools", {})
            parallel_tools = set(tool_config.get("parallel", []))
            serial_tools = set(tool_config.get("serial", []))
            
            # Log the configuration
            self.logger.info(f"Loaded tool configuration: {len(parallel_tools)} parallel tools, {len(serial_tools)} serial tools")
            
            return parallel_tools, serial_tools
        except Exception as e:
            self.logger.error(f"Error loading tool configuration: {e}")
            # Return empty sets as fallback (all tools will be treated as serial)
            return set(), set()
    
    def initialize_graph(self) -> StateGraph:
        """
        Initialize and return the LangGraph StateGraph
        
        Returns:
            StateGraph: Compiled LangGraph StateGraph
        """
        # Build state graph
        self.logger.info("Building Phase 2 ReAct graph")
        builder = StateGraph(Phase2State)
        
        # Add nodes
        self.logger.info("Adding node: call_model")
        builder.add_node("call_model", self.call_model)
        
        # Initialize ExecuteToolNode with parallel/serial tool configuration
        execute_tools_node = self.graph_utility.create_execute_tool_node(
            self.mcp_tools, self.parallel_tools, self.serial_tools
        )
        self.logger.info("Adding node: execute_tools")
        builder.add_node("execute_tools", execute_tools_node)
        
        self.logger.info("Adding node: check_end")
        builder.add_node("check_end", self.check_end_conditions)
        
        # Add edges
        self.logger.info("Adding edge: START -> call_model")
        builder.add_edge(START, "call_model")
        
        # Add conditional edges for tools
        self.logger.info("Adding conditional edges for tools")
        builder.add_conditional_edges(
            "call_model",
            tools_condition,
            {
                "tools": "execute_tools",   # Route to execute_tools node
                "none": "check_end",        # If no tools, go to check_end
                "__end__": "check_end"
            }
        )
        
        # Add edge from execute_tools to call_model
        self.logger.info("Adding edge: execute_tools -> call_model")
        builder.add_edge("execute_tools", "call_model")
        
        # Add conditional edges from check_end node
        self.logger.info("Adding conditional edges from check_end node")
        builder.add_conditional_edges(
            "check_end",
            lambda state: self.check_end_conditions(state)["result"],
            {
                "end": END,
                "__end__": END,
                "continue": "call_model"  # Loop back if conditions not met
            }
        )
        
        # Compile graph
        self.logger.info("Compiling graph")
        return builder.compile()
    
    def call_model(self, state: Phase2State) -> Phase2State:
        """
        LLM reasoning node that analyzes current state and decides next action
        
        Args:
            state: Current state of the graph
            
        Returns:
            Phase2State: Updated state after LLM reasoning
        """
        # Increment iteration count
        state["iteration_count"] += 1
        
        # Check if we've reached max iterations
        if state["iteration_count"] > self.max_iterations:
            self.logger.info(f"Reached max iterations ({self.max_iterations}), marking remediation as complete")
            
            # Add a final message indicating max iterations reached
            final_message = AIMessage(content=f"[MAX_ITERATIONS_REACHED] Completed {self.max_iterations} iterations. Finalizing remediation with current information.")
            state["messages"].append(final_message)
            
            return state
        
        self.logger.info(f"Calling model (iteration {state['iteration_count']})")
        
        try:
            # Call the model with tools
            response = None
            if len(self.mcp_tools) != 0:
                response = self.llm.bind_tools(self.mcp_tools).invoke(state["messages"])
            else:
                response = self.llm.invoke(state["messages"])
            
            # Add response to messages
            state["messages"].append(response)
            
            # Log the response
            self.logger.info(f"Model response: {response.content[:100]}...")
            
            return state
        except Exception as e:
            error_msg = handle_exception("call_model", e, self.logger)
            
            # Add error message to state
            error_message = SystemMessage(content=f"Error calling model: {error_msg}")
            state["messages"].append(error_message)
            
            return state
    
    def check_end_conditions(self, state: Phase2State) -> Dict[str, str]:
        """
        Check if remediation is complete
        
        Args:
            state: Current state of the graph
            
        Returns:
            Dict[str, str]: Result indicating whether to end or continue
        """
        self.logger.info("Checking end conditions for Phase 2 ReAct graph")
        
        # Check if we've reached max iterations
        if state["iteration_count"] >= self.max_iterations:
            self.logger.info(f"Reached max iterations ({self.max_iterations}), ending graph")
            return {"result": "end"}
        
        # Get the last message
        messages = state["messages"]
        if not messages:
            return {"result": "continue"}
        
        last_message = messages[-1]
        
        # Skip content checks if the last message isn't from the AI
        if getattr(last_message, "type", "") != "ai":
            return {"result": "continue"}
        
        content = getattr(last_message, "content", "")
        if not content:
            return {"result": "continue"}
        
        # Check for explicit end markers in the content
        end_markers = ["[END_GRAPH]", "[END]", "End of graph", "GRAPH END", "CONCLUSION:", "SUMMARY OF ACTIONS:"]
        if any(marker in content for marker in end_markers):
            self.logger.info(f"Detected end marker in content, ending graph")
            return {"result": "end"}
        
        # Check for completion indicators
        if "Actions Taken:" in content and "Resolution Status:" in content:
            self.logger.info("Detected completion indicators in content, ending graph")
            state["fix_plan_executed"] = True
            return {"result": "end"}
        
        # Check for convergence (model repeating itself)
        ai_messages = [m for m in messages if getattr(m, "type", "") == "ai"]
        if len(ai_messages) > 3:
            # Compare the last message with the third-to-last message
            last_content = content
            third_to_last_content = getattr(ai_messages[-3], "content", "")
            
            # Simple similarity check - if they start with the same paragraph
            if last_content and third_to_last_content:
                # Get first 100 chars of each message
                last_start = last_content[:100] if len(last_content) > 100 else last_content
                third_start = third_to_last_content[:100] if len(third_to_last_content) > 100 else third_to_last_content
                
                if last_start == third_start:
                    self.logger.info("Detected convergence (model repeating itself), ending graph")
                    return {"result": "end"}
        
        # Default: continue execution
        return {"result": "continue"}
    
    def extract_remediation_from_state(self, state: Phase2State) -> str:
        """
        Extract the remediation results from the final state
        
        Args:
            state: Final state of the graph
            
        Returns:
            str: Remediation results as a formatted string
        """
        # Get the last AI message
        ai_messages = [m for m in state["messages"] if getattr(m, "type", "") == "ai"]
        
        if not ai_messages:
            self.logger.warning("No AI messages found in final state")
            return "Failed to generate remediation results."
        
        # Get the content of the last AI message
        last_ai_message = ai_messages[-1]
        content = getattr(last_ai_message, "content", "")
        
        # Remove end markers
        for marker in ["[END_GRAPH]", "[END]", "End of graph", "GRAPH END"]:
            content = content.replace(marker, "")
            
        return content.strip()
    
    async def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the graph with the provided state
        
        Args:
            state: Initial state for the graph execution
            
        Returns:
            Dict[str, Any]: Final state after graph execution
        """
        try:
            # Initialize the graph
            graph = self.initialize_graph()
            
            # Prepare initial state
            initial_state = {
                "messages": state.get("messages", []),
                "iteration_count": 0,
                "tool_call_count": 0,
                "goals_achieved": [],
                "fix_plan_executed": False,
                "phase1_final_response": state.get("phase1_final_response", "")
            }
            
            # Run the graph
            self.logger.info("Running Phase 2 ReAct graph")
            final_state = graph.invoke(initial_state)
            
            # Extract the remediation results
            remediation_results = self.extract_remediation_from_state(final_state)
            
            # Add the remediation results to the final state
            final_state["remediation_results"] = remediation_results
            
            return final_state
            
        except Exception as e:
            error_msg = handle_exception("execute", e, self.logger)
            
            return {
                "status": "error",
                "error_message": error_msg,
                "remediation_results": "Failed to complete remediation due to an error.",
                "messages": state.get("messages", [])
            }
    
    def get_prompt_manager(self):
        """
        Return the prompt manager for this graph
        
        Returns:
            Phase2PromptManager: Prompt manager for the Remediation phase
        """
        return self.prompt_manager
</file>

<file path="llm_graph/prompt_managers/__init__.py">
"""
Prompt Manager implementations for different phases of the troubleshooting system.
"""

from llm_graph.prompt_managers.base_prompt_manager import BasePromptManager
from llm_graph.prompt_managers.plan_prompt_manager import PlanPromptManager
from llm_graph.prompt_managers.phase1_prompt_manager import Phase1PromptManager
from llm_graph.prompt_managers.phase2_prompt_manager import Phase2PromptManager

__all__ = ['BasePromptManager', 'PlanPromptManager', 'Phase1PromptManager', 'Phase2PromptManager']
</file>

<file path="llm_graph/prompt_managers/base_prompt_manager.py">
#!/usr/bin/env python3
"""
Base Prompt Manager for Kubernetes Volume Troubleshooting

This module provides the base implementation of the PromptManagerInterface
with common functionality for all prompt managers.
"""

import logging
import json
import os
from typing import Dict, List, Any, Optional
from llm_graph.prompt_manager_interface import PromptManagerInterface
from langchain_core.messages import SystemMessage, HumanMessage

logger = logging.getLogger(__name__)

class BasePromptManager(PromptManagerInterface):
    """
    Base implementation of PromptManagerInterface
    
    Provides common functionality for all prompt managers, including
    loading historical experience data and formatting messages.
    """
    
    def __init__(self, config_data: Dict[str, Any] = None):
        """
        Initialize the Base Prompt Manager
        
        Args:
            config_data: Configuration data for the system
        """
        self.config_data = config_data or {}
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def get_system_prompt(self, **kwargs) -> str:
        """
        Return the phase-specific system prompt
        
        This is a base implementation that should be overridden by subclasses.
        
        Args:
            **kwargs: Optional arguments for customizing the prompt
            
        Returns:
            str: System prompt for the current phase
        """
        return "You are an AI assistant helping with Kubernetes volume troubleshooting."
        
    def format_user_query(self, query: str, **kwargs) -> str:
        """
        Format user query messages for the phase
        
        This is a base implementation that should be overridden by subclasses.
        
        Args:
            query: User query to format
            **kwargs: Optional arguments for customizing the formatting
            
        Returns:
            str: Formatted user query
        """
        return query
        
    def get_tool_prompt(self, **kwargs) -> str:
        """
        Return prompts for tool invocation
        
        This is a base implementation that should be overridden by subclasses.
        
        Args:
            **kwargs: Optional arguments for customizing the prompt
            
        Returns:
            str: Tool invocation prompt for the current phase
        """
        return "Use the available tools to help with troubleshooting."
    
    def prepare_messages(self, system_prompt: str, user_message: str, 
                       message_list: Optional[List[Dict[str, str]]] = None) -> List[Dict[str, str]]:
        """
        Prepare message list for LLM
        
        Args:
            system_prompt: System prompt for LLM
            user_message: User message for LLM
            message_list: Optional existing message list
            
        Returns:
            List[Dict[str, str]]: Prepared message list
        """
        if message_list is None:
            # Create new message list
            return [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_message)
            ]
        
        # Use existing message list
        # If the last message is from the user, we need to regenerate the plan
        if message_list[-1]["role"] == "user":
            # Keep the system prompt and add the new user message
            return message_list
        else:
            # This is the first call, initialize with system prompt and user message
            return [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_message)
            ]
    
    def get_context_summary(self, collected_info: Dict[str, Any]) -> str:
        """
        Get context summary from collected information
        
        Args:
            collected_info: Pre-collected diagnostic information
            
        Returns:
            str: Formatted context summary
        """
        return f"""
=== PRE-COLLECTED DIAGNOSTIC CONTEXT ===
Instructions:
    You can use the pre-collected diagnostic information to understand the current state of the Kubernetes cluster and the volume I/O issues being faced. Use this information to guide your troubleshooting process.

Knowledge Graph Summary:
{json.dumps(collected_info.get('knowledge_graph_summary', {}), indent=2)}

Pod Information:
{str(collected_info.get('pod_info', {}))}

PVC Information:
{str(collected_info.get('pvc_info', {}))}

PV Information:
{str(collected_info.get('pv_info', {}))}

Node Information Summary:
{str(collected_info.get('node_info', {}))}

CSI Driver Information:
{str(collected_info.get('csi_driver_info', {}))}

System Information:
{str(collected_info.get('system_info', {}))}

<<< Current Issues >>>
Issues Summary:
{str(collected_info.get('issues', {}))}

=== END PRE-COLLECTED CONTEXT ===
"""
    
    def _load_historical_experience(self) -> str:
        """
        Load historical experience data from JSON file
        
        Returns:
            str: Formatted historical experience examples
        """
        historical_experience_path = os.path.join(
            os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 
            'data', 
            'historical_experience.json'
        )
        historical_experience_examples = ""
        
        try:
            with open(historical_experience_path, 'r') as f:
                historical_experience = json.load(f)
                
            # Format historical experience data into CoT examples
            for i, experience in enumerate(historical_experience):
                # Create example header
                example_num = i + 1
                example_title = experience.get('observation', f"Example {example_num}")
                historical_experience_examples += f"\n## Example {example_num}: {example_title}\n\n"
                
                # Add OBSERVATION section
                historical_experience_examples += f"**OBSERVATION**: {experience.get('observation', '')}\n\n"
                
                # Add THINKING section
                historical_experience_examples += "**THINKING**:\n"
                thinking_points = experience.get('thinking', [])
                for j, point in enumerate(thinking_points):
                    historical_experience_examples += f"{j+1}. {point}\n"
                historical_experience_examples += "\n"
                
                # Add INVESTIGATION section
                historical_experience_examples += "**INVESTIGATION**:\n"
                investigation_steps = experience.get('investigation', [])
                for j, step_info in enumerate(investigation_steps):
                    if isinstance(step_info, dict):
                        step = step_info.get('step', '')
                        reasoning = step_info.get('reasoning', '')
                        historical_experience_examples += f"{j+1}. {step}\n   - {reasoning}\n"
                    else:
                        historical_experience_examples += f"{j+1}. {step_info}\n"
                historical_experience_examples += "\n"
                
                # Add DIAGNOSIS section
                historical_experience_examples += f"**DIAGNOSIS**: {experience.get('diagnosis', '')}\n\n"
                
                # Add RESOLUTION section
                historical_experience_examples += "**RESOLUTION**:\n"
                resolution_steps = experience.get('resolution', [])
                if isinstance(resolution_steps, list):
                    for j, step in enumerate(resolution_steps):
                        historical_experience_examples += f"{j+1}. {step}\n"
                else:
                    historical_experience_examples += f"{resolution_steps}\n"
                historical_experience_examples += "\n"
                
                # Limit to 2 examples to keep the prompt size manageable
                if example_num >= 2:
                    break
                    
        except Exception as e:
            logging.error(f"Error loading historical experience data: {e}")
            # Provide a fallback example in case the file can't be loaded
            historical_experience_examples = """
## Example 1: Volume Read Errors

**OBSERVATION**: Volume read errors appearing in pod logs

**THINKING**:
1. Read errors often indicate hardware issues with the underlying disk
2. Could be bad sectors, disk degradation, or controller problems
3. Need to check both logical (filesystem) and physical (hardware) health
4. Should examine error logs first, then check disk health metrics
5. Will use knowledge graph to find affected components, then check disk health

**INVESTIGATION**:
1. First, query error logs with `kg_query_nodes(type='log', time_range='24h', filters={{'message': 'I/O error'}})` to identify affected pods
   - This will show which pods are experiencing I/O errors and their frequency
2. Check disk health with `check_disk_health(node='node-1', disk_id='disk1')`
   - This will reveal SMART data and physical health indicators
3. Use 'xfs_repair -n *' to check volume health without modifying it
   - This will identify filesystem-level corruption or inconsistencies

**DIAGNOSIS**: Hardware failure in the underlying disk, specifically bad sectors causing read operations to fail

**RESOLUTION**:
1. Replace the faulty disk identified in `check_disk_health`
2. Restart the affected service with `systemctl restart db-service`
3. Verify pod status with `kubectl get pods` to ensure normal operation
"""
        
        return historical_experience_examples
        
    def format_historical_experiences_from_collected_info(self, collected_info: Dict[str, Any]) -> str:
        """
        Format historical experience data from collected information
        
        Args:
            collected_info: Pre-collected diagnostic information
            
        Returns:
            str: Formatted historical experience data
        """
        try:
            # Extract historical experiences from collected_info
            historical_experiences = collected_info.get('historical_experiences', [])
            
            if not historical_experiences:
                return "No historical experience data available."
            
            # Format historical experiences in a clear, structured way
            formatted_experiences = []
            
            for i, experience in enumerate(historical_experiences):
                # Get attributes
                attributes = experience.get('attributes', {})
                
                # Extract key fields
                observation = attributes.get('observation', attributes.get('phenomenon', 'Unknown issue'))
                diagnosis = attributes.get('diagnosis', attributes.get('root_cause', 'Unknown cause'))
                resolution = attributes.get('resolution', attributes.get('resolution_method', 'No resolution method'))
                
                # Format the experience
                formatted_exp = f"Experience #{i+1}:\n"
                formatted_exp += f"- Observation: {observation}\n"
                formatted_exp += f"- Diagnosis: {diagnosis}\n"
                formatted_exp += f"- Resolution: {resolution}\n"
                
                formatted_experiences.append(formatted_exp)
            
            return "\n".join(formatted_experiences)
            
        except Exception as e:
            logging.error(f"Error formatting historical experiences: {e}")
            return "Error formatting historical experience data."
</file>

<file path="llm_graph/prompt_managers/legacy_prompt_manager.py">
#!/usr/bin/env python3
"""
Legacy Prompt Manager for Kubernetes Volume Troubleshooting

This module provides a backward-compatible implementation of the original
PromptManager class using the new PromptManagerInterface.
"""

import logging
import json
import os
from typing import Dict, List, Any, Optional
from llm_graph.prompt_managers.base_prompt_manager import BasePromptManager

logger = logging.getLogger(__name__)

class LegacyPromptManager(BasePromptManager):
    """
    Legacy implementation of the original PromptManager
    
    Provides backward compatibility with the original PromptManager class
    while implementing the new PromptManagerInterface.
    """
    
    def __init__(self, config_data: Dict[str, Any] = None):
        """
        Initialize the Legacy Prompt Manager
        
        Args:
            config_data: Configuration data for the system
        """
        super().__init__(config_data)
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def get_phase_specific_guidance(self, phase: str, final_output_example: str = "") -> str:
        """
        Get phase-specific guidance prompt
        
        Args:
            phase: Current troubleshooting phase ("phase1" for investigation, "phase2" for action)
            final_output_example: Example of final output format
            
        Returns:
            str: Phase-specific guidance prompt
        """
        if phase == "phase1":
            return self._get_phase1_guidance(final_output_example)
        elif phase == "phase2":
            return self._get_phase2_guidance()
        else:
            return self._get_legacy_guidance()
    
    def _get_phase1_guidance(self, final_output_example: str) -> str:
        """
        Get guidance for Phase 1 (Investigation)
        
        Args:
            final_output_example: Example of final output format
            
        Returns:
            str: Phase 1 guidance prompt
        """
        return f"""
You are currently in Phase 1 (Investigation). Your primary task is to perform comprehensive root cause analysis and evidence collection using investigation tools.

PHASE 1 RESTRICTIONS:
- NO destructive operations (no kubectl_apply, kubectl_delete, fsck_check)
- NO test resource creation
- NO hardware modifications
- FOCUS on comprehensive investigation and root cause analysis

OUTPUT REQUIREMENTS:
Provide a detailed investigation report that includes:

1. Summary of Findings:
   - Brief overview of the main issues discovered
   - Severity assessment of the overall situation

2. Detailed Analysis:
   - Primary Issues:
     * Description of each major problem identified
     * Evidence supporting each issue (logs, metrics, events)
     * Impact assessment on the system and services
     * Probability or confidence level in the diagnosis
   - Secondary Issues:
     * Description of minor or related problems
     * Potential consequences if left unaddressed
   - System Metrics:
     * Key performance indicators and their current values
     * Any metrics that deviate from normal ranges
   - Environmental Factors:
     * External conditions that may be contributing to the issues

3. Relationship Analysis:
   - Connections between different issues
   - How components of the system are affecting each other

4. Investigation Process:
   - Steps taken during the troubleshooting
   - Tools and commands used
   - Reasoning behind each investigative action

5. Potential Root Causes:
   - List of possible underlying causes
   - Evidence supporting each potential root cause
   - Likelihood assessment for each cause

6. Open Questions:
   - Any unresolved aspects of the investigation
   - Areas that require further examination

7. Next Steps:
   - Recommended further diagnostic actions
   - Suggestions for additional data collection or analysis
8. Root Cause:
    - The most likely root cause based on the evidence collected
8. Fix Plan:
    - Proposed remediation steps to address the issues

INVESTIGATION RESULT EXAMPLE:
{final_output_example}
"""
    
    def _get_phase2_guidance(self) -> str:
        """
        Get guidance for Phase 2 (Action/Remediation)
        
        Returns:
            str: Phase 2 guidance prompt
        """
        return """
You are currently in Phase 2 (Action/Remediation). You have access to all Phase 1 investigation tools PLUS action tools for implementing fixes.

PHASE 2 CAPABILITIES:
- Execute remediation actions based on Phase 1 **Fix Plan**
- Create test resources to validate fixes
- Run comprehensive volume testing
- Perform hardware diagnostics and repairs
- Clean up test resources after validation

OUTPUT REQUIREMENTS:
Provide a detailed remediation report that includes:
1. Actions Taken: List of all remediation steps executed
2. Test Results: Results from validation tests
3. Resolution Status: Whether issues were resolved
4. Remaining Issues: Any unresolved problems
5. Recommendations: Suggestions for ongoing monitoring or future improvements
"""
    
    def _get_legacy_guidance(self) -> str:
        """
        Get guidance for legacy mode
        
        Returns:
            str: Legacy mode guidance prompt
        """
        return """
You are in a legacy mode. Please specify either 'phase1' for investigation or 'phase2' for action/remediation.
"""
    
    def get_system_prompt(self, phase: str = "", final_output_example: str = "", **kwargs) -> str:
        """
        Get the system prompt for a specific phase
        
        Args:
            phase: Current troubleshooting phase
            final_output_example: Example of final output format
            **kwargs: Additional arguments
            
        Returns:
            str: System prompt for the specified phase
        """
        # Get phase-specific guidance
        phase_specific_guidance = self.get_phase_specific_guidance(phase, final_output_example)
        
        # Load historical experience data
        historical_experience_examples = self._load_historical_experience()
        
        # Create system message with Chain of Thought (CoT) format and historical experience examples
        return f"""You are an AI assistant powering a Kubernetes volume troubleshooting system using LangGraph ReAct. Your role is to monitor and resolve volume I/O errors in Kubernetes pods backed by local HDD/SSD/NVMe disks managed by the CSI Baremetal driver (csi-baremetal.dell.com). Exclude remote storage (e.g., NFS, Ceph). 

<<< Note >>>: Please follow the Investigation Plan to run tools and investigate the volume i/o issue step by step, and run 5 steps at least.
<<< Note >>>: If you suspect some issue root cause according to current call tools result, you can add call tools step by yourself

{phase_specific_guidance}

# CHAIN OF THOUGHT APPROACH

When troubleshooting, use a structured Chain of Thought approach to reason through problems:

1. **OBSERVATION**: Clearly identify what issue or symptom you're seeing
   - What errors are present in logs or events?
   - What behavior is unexpected or problematic?

2. **THINKING**:
   - What are the possible causes of this issue?
   - What components could be involved?
   - What tools can I use to investigate further?
   - What patterns should I look for in the results?

3. **INVESTIGATION**:
   - Execute tools in a logical sequence
   - For each tool, explain WHY you're using it and WHAT you expect to learn
   - After each result, analyze what it tells you and what to check next

4. **DIAGNOSIS**:
   - Based on evidence, determine the most likely root cause
   - Explain your reasoning with supporting evidence
   - Consider alternative explanations and why they're less likely

5. **RESOLUTION**:
   - Propose specific steps to resolve the issue
   - Explain why each step will help address the root cause
   - Consider potential side effects or risks

# HISTORICAL EXPERIENCE EXAMPLES

Here are examples of how to apply Chain of Thought reasoning to common volume issues:
{historical_experience_examples}

Follow these strict guidelines for safe, reliable, and effective troubleshooting:

1. **Knowledge Graph Prioritization**:
   - ALWAYS check the Knowledge Graph FIRST before using command execution tools.
   - Then use detailed query tools:
     * Use kg_get_entity_info(entity_type, id) to retrieve detailed information about specific entities
     * Use kg_get_related_entities(entity_type, id) to understand relationships between components
     * Use kg_get_all_issues() to find already detected issues in the system
     * Use kg_find_path(source_entity_type, source_id, target_entity_type, target_id) to trace dependencies between entities (e.g., Pod  PVC  PV  Drive)
     * Use kg_analyze_issues() to identify patterns and root causes from the Knowledge Graph
   - Only execute commands like kubectl or SSH when Knowledge Graph lacks needed information.

2. **Troubleshooting Process**:
   - Use the LangGraph ReAct module to reason about volume I/O errors based on parameters: `PodName`, `PodNamespace`, and `VolumePath`.
   - Most of time the pod's volume file system type is xfs, ext4, or btrfs. 
   - Follow this structured diagnostic process for local HDD/SSD/NVMe disks managed by CSI Baremetal:
     a. **Check Knowledge Graph**: First use Knowledge Graph tools (kg_*) to understand the current state and existing issues.
     b. **Confirm Issue**: If Knowledge Graph lacks information, run `kubectl logs <pod-name> -n <namespace>` and `kubectl describe pod <pod-name> -n <namespace>` to identify errors (e.g., "Input/Output Error", "Permission Denied", "FailedMount").
        - Analyze the pod/pvc/volume's definition with `kubectl get pod/pvc/pv <resource_name> -o yaml`. whether the definition has keywords like: readOnlyRootFilesystem, ReadOnlyMany, readOnly, etc.
     c. **Verify Configurations**: Check Pod, PVC, and PV with `kubectl get pod/pvc/pv <resource_name> -o yaml`. Confirm PV uses local volume, valid disk path (e.g., `/dev/sda`), and correct `nodeAffinity`. Verify mount points with `kubectl exec <pod-name> -n <namespace> -- df -h` and `ls -ld <mount-path>`.
     d. **Check CSI Baremetal Driver and Resources**:
        - Identify driver: `kubectl get storageclass <storageclass-name> -o yaml` (e.g., `csi-baremetal-sc-ssd`).
        - Verify driver pod: `kubectl get pods -n kube-system -l app=csi-baremetal` and `kubectl logs <driver-pod-name> -n kube-system`. Check for errors like "failed to mount".
        - Confirm driver registration: `kubectl get csidrivers`.
        - Check drive status: `kubectl get drive -o wide` and `kubectl get drive <drive-uuid> -o yaml`. Verify `Health: GOOD`, `Status: ONLINE`, `Usage: IN_USE`, and match `Path` (e.g., `/dev/sda`) with `VolumePath`.
        - Map drive to node: `kubectl get csibmnode` to correlate `NodeId` with hostname/IP.
        - Check AvailableCapacity: `kubectl get ac -o wide` to confirm size, storage class, and location (drive UUID).
        - Check LogicalVolumeGroup: `kubectl get lvg` to verify `Health: GOOD` and associated drive UUIDs.
     e. **Test Driver**: Create a test PVC/Pod using `csi-baremetal-sc-ssd` storage class (use provided YAML template). Check logs and events for read/write errors.
     f. **Verify Node Health**: Run `kubectl describe node <node-name>` to ensure `Ready` state and no `DiskPressure`. Verify disk mounting via SSH: `mount | grep <disk-path>`.
     g. **Check Permissions**: Verify file system permissions with `kubectl exec <pod-name> -n <namespace> -- ls -ld <mount-path>` and Pod `SecurityContext` settings.
     h. **Inspect Control Plane**: Check `kube-controller-manager` and `kube-scheduler` logs for provisioning/scheduling issues.
     i. **Test Hardware Disk**:
        - Identify disk: `kubectl get pv -o yaml` and `kubectl get drive <drive-uuid> -o yaml` to confirm `Path`.
        - Check health: `kubectl get drive <drive-uuid> -o yaml` and `ssh <node-name> sudo smartctl -a /dev/<disk-device>`. Verify `Health: GOOD`, zero `Reallocated_Sector_Ct` or `Current_Pending_Sector`.
        - Test performance: `ssh <node-name> sudo fio --name=read_test --filename=/dev/<disk-device> --rw=read --bs=4k --size=100M --numjobs=1 --iodepth=1 --runtime=60 --time_based --group_reporting`.
        - Check file system (if unmounted): `ssh <node-name> sudo xfs_repair -n /dev/<disk-device>` (requires approval).
        - Test via Pod: Create a test Pod (use provided YAML) and check logs for "Write OK" and "Read OK".
     j. **Propose Remediations**:
        - Bad sectors: Recommend disk replacement if `kubectl get drive` or SMART shows `Health: BAD` or non-zero `Reallocated_Sector_Ct`.
        - Performance issues: Suggest optimizing I/O scheduler or replacing disk if `fio` results show low IOPS (HDD: 100200, SSD: thousands, NVMe: tens of thousands).
        - File system corruption: Recommend `fsck` or 'xfs_repair' (if enabled/approved) after data backup.
        - Driver issues: Suggest restarting CSI Baremetal driver pod (if enabled/approved) if logs indicate errors.
   - Only propose remediations after analyzing diagnostic data. Ensure write/change commands (e.g., `fsck`, `kubectl delete pod`) are allowed and approved.
   - Try to find all of possible root causes before proposing any remediation steps. 

3. **Error Handling**:
   - If unresolved, provide a detailed report of findings (e.g., logs, drive status, SMART data, test results) and suggest manual intervention.

4. **Knowledge Graph Usage**:
   - Follow this effective Knowledge Graph navigation strategy:
     1. Start with discovery: Use kg_list_entity_types() to understand what entity types exist
     2. Find relevant entities: Use kg_list_entities(entity_type) to find specific entities of interest
     3. Get detailed information: Use kg_get_entity_info(entity_type, id) for specific entities
     4. Explore relationships: Use kg_get_related_entities(entity_type, id) to see connections
     5. Analyze issues: Use kg_get_all_issues() to find existing issues
     6. Trace dependencies: Use kg_find_path() to find connections between entities
   - Use kg_print_graph to get a human-readable overview of the entire system state.
   - First check issues with kg_get_all_issues before running diagnostic commands. These issues are critical information to find root cause.
   - Use kg_get_summary to get high-level statistics about the cluster state.
   - For root cause analysis, use kg_analyze_issues to identify patterns across the system.

7. **Constraints**:
   - Restrict operations to the Kubernetes cluster and configured worker nodes; do not access external networks or resources.
   - Do not modify cluster state (e.g., delete pods, change configurations) unless explicitly allowed and approved.
   - Adhere to `troubleshoot.timeout_seconds` for the troubleshooting workflow.
   - Always recommend data backup before suggesting write/change operations (e.g., `fsck`).

8. **Output**:
   - << Adding the analysis and summary for each steps when call tools >>
   - Try to find all of possible root causes before proposing any remediation steps.
   - Provide clear, concise explanations of diagnostic steps, findings, and remediation proposals.
   - Include performance benchmarks in reports (e.g., HDD: 100200 IOPS, SSD: thousands, NVMe: tens of thousands).
   - Don't ask questions to user, just decide by yourself.
   - **Don't output with JSON format, use plain text for better readability.**
   - **the output should include the following sections:**
    # Summary of Findings
    # Detailed Analysis
    # Relationship Analysis
    # Investigation Process
    # Potential Root Causes
    # Open Questions
    # Next Steps
    # Root Cause
    # **Fix Plan**      # this section must exist

You must adhere to these guidelines at all times to ensure safe, reliable, and effective troubleshooting of local disk issues in Kubernetes with the CSI Baremetal driver.
Adding the analysis and summary for each call tools steps
"""
    
    def format_user_query(self, query: str, **kwargs) -> str:
        """
        Format user query messages for the phase
        
        Args:
            query: User query to format
            **kwargs: Optional arguments for customizing the formatting
            
        Returns:
            str: Formatted user query
        """
        return query
    
    def get_tool_prompt(self, **kwargs) -> str:
        """
        Return prompts for tool invocation
        
        Args:
            **kwargs: Optional arguments for customizing the prompt
            
        Returns:
            str: Tool invocation prompt for the current phase
        """
        return "Use the available tools to help with troubleshooting."
    
    def prepare_messages(self, system_prompt: str, user_message: str, 
                       message_list: Optional[List[Dict[str, str]]] = None) -> List[Dict[str, str]]:
        """
        Prepare message list for LLM
        
        Args:
            system_prompt: System prompt
            user_message: User message
            message_list: Optional existing message list
            
        Returns:
            List[Dict[str, str]]: Prepared message list
        """
        if message_list is None:
            message_list = []
            
        # Create a new message list if empty
        if not message_list:
            message_list = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ]
        else:
            # Add the user message to the existing message list
            message_list.append({"role": "user", "content": user_message})
            
        return message_list
    
    def get_context_summary(self, collected_info: Dict[str, Any]) -> str:
        """
        Get context summary from collected information
        
        Args:
            collected_info: Pre-collected diagnostic information
            
        Returns:
            str: Formatted context summary
        """
        return f"""
=== PRE-COLLECTED DIAGNOSTIC CONTEXT ===
Instructions:
    You can use the pre-collected diagnostic information to understand the current state of the Kubernetes cluster and the volume I/O issues being faced. Use this information to guide your troubleshooting process.

Knowledge Graph Summary:
{json.dumps(collected_info.get('knowledge_graph_summary', {}), indent=2)}

Pod Information:
{str(collected_info.get('pod_info', {}))}

PVC Information:
{str(collected_info.get('pvc_info', {}))}

PV Information:
{str(collected_info.get('pv_info', {}))}

Node Information Summary:
{str(collected_info.get('node_info', {}))}

CSI Driver Information:
{str(collected_info.get('csi_driver_info', {}))}

System Information:
{str(collected_info.get('system_info', {}))}

<<< Current Issues >>>
Issues Summary:
{str(collected_info.get('issues', {}))}

=== END PRE-COLLECTED CONTEXT ===
"""
    
    def _load_historical_experience(self) -> str:
        """
        Load historical experience data from JSON file
        
        Returns:
            str: Formatted historical experience examples
        """
        historical_experience_path = os.path.join(
            os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 
            'data', 
            'historical_experience.json'
        )
        historical_experience_examples = ""
        
        try:
            with open(historical_experience_path, 'r') as f:
                historical_experience = json.load(f)
                
            # Format historical experience data into CoT examples
            for i, experience in enumerate(historical_experience):
                # Create example header
                example_num = i + 1
                example_title = experience.get('observation', f"Example {example_num}")
                historical_experience_examples += f"\n## Example {example_num}: {example_title}\n\n"
                
                # Add OBSERVATION section
                historical_experience_examples += f"**OBSERVATION**: {experience.get('observation', '')}\n\n"
                
                # Add THINKING section
                historical_experience_examples += "**THINKING**:\n"
                thinking_points = experience.get('thinking', [])
                for j, point in enumerate(thinking_points):
                    historical_experience_examples += f"{j+1}. {point}\n"
                historical_experience_examples += "\n"
                
                # Add INVESTIGATION section
                historical_experience_examples += "**INVESTIGATION**:\n"
                investigation_steps = experience.get('investigation', [])
                for j, step_info in enumerate(investigation_steps):
                    if isinstance(step_info, dict):
                        step = step_info.get('step', '')
                        reasoning = step_info.get('reasoning', '')
                        historical_experience_examples += f"{j+1}. {step}\n   - {reasoning}\n"
                    else:
                        historical_experience_examples += f"{j+1}. {step_info}\n"
                historical_experience_examples += "\n"
                
                # Add DIAGNOSIS section
                historical_experience_examples += f"**DIAGNOSIS**: {experience.get('diagnosis', '')}\n\n"
                
                # Add RESOLUTION section
                historical_experience_examples += "**RESOLUTION**:\n"
                resolution_steps = experience.get('resolution', [])
                if isinstance(resolution_steps, list):
                    for j, step in enumerate(resolution_steps):
                        historical_experience_examples += f"{j+1}. {step}\n"
                else:
                    historical_experience_examples += f"{resolution_steps}\n"
                historical_experience_examples += "\n"
                
                # Limit to 2 examples to keep the prompt size manageable
                if example_num >= 2:
                    break
                    
        except Exception as e:
            logging.error(f"Error loading historical experience data: {e}")
            # Provide a fallback example in case the file can't be loaded
            historical_experience_examples = """
## Example 1: Volume Read Errors

**OBSERVATION**: Volume read errors appearing in pod logs

**THINKING**:
1. Read errors often indicate hardware issues with the underlying disk
2. Could be bad sectors, disk degradation, or controller problems
3. Need to check both logical (filesystem) and physical (hardware) health
4. Should examine error logs first, then check disk health metrics
5. Will use knowledge graph to find affected components, then check disk health

**INVESTIGATION**:
1. First, query error logs with `kg_query_nodes(type='log', time_range='24h', filters={{'message': 'I/O error'}})` to identify affected pods
   - This will show which pods are experiencing I/O errors and their frequency
2. Check disk health with `check_disk_health(node='node-1', disk_id='disk1')`
   - This will reveal SMART data and physical health indicators
3. Use 'xfs_repair -n *' to check volume health without modifying it
   - This will identify filesystem-level corruption or inconsistencies

**DIAGNOSIS**: Hardware failure in the underlying disk, specifically bad sectors causing read operations to fail

**RESOLUTION**:
1. Replace the faulty disk identified in `check_disk_health`
2. Restart the affected service with `systemctl restart db-service`
3. Verify pod status with `kubectl get pods` to ensure normal operation
"""
        
        return historical_experience_examples
</file>

<file path="llm_graph/prompt_managers/phase1_prompt_manager.py">
#!/usr/bin/env python3
"""
Phase 1 (Analysis) Prompt Manager for Kubernetes Volume Troubleshooting

This module provides the prompt manager implementation for the Analysis phase
of the troubleshooting system.
"""

import logging
from typing import Dict, List, Any, Optional
from llm_graph.prompt_managers.base_prompt_manager import BasePromptManager

logger = logging.getLogger(__name__)

class Phase1PromptManager(BasePromptManager):
    """
    Prompt manager for the Analysis phase (Phase 1)
    
    Handles prompt generation and formatting for the Analysis phase,
    which executes the Investigation Plan to identify root causes.
    """
    
    def __init__(self, config_data: Dict[str, Any] = None):
        """
        Initialize the Phase 1 Prompt Manager
        
        Args:
            config_data: Configuration data for the system
        """
        super().__init__(config_data)
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def get_system_prompt(self, final_output_example: str = "", **kwargs) -> str:
        """
        Return the system prompt for the Analysis phase
        
        Args:
            final_output_example: Example of final output format
            **kwargs: Optional arguments for customizing the prompt
            
        Returns:
            str: System prompt for the Analysis phase
        """
        # Load historical experience data
        historical_experience_examples = self._load_historical_experience()
        
        # Phase-specific guidance
        phase_specific_guidance = self._get_phase1_guidance(final_output_example)
        
        # Create system message with Chain of Thought (CoT) format and historical experience examples
        return f"""You are an AI assistant powering a Kubernetes volume troubleshooting system using LangGraph ReAct. Your role is to monitor and resolve volume I/O errors in Kubernetes pods backed by local HDD/SSD/NVMe disks managed by the CSI Baremetal driver (csi-baremetal.dell.com). Exclude remote storage (e.g., NFS, Ceph). 

<<< Note >>>: Please follow the Investigation Plan to run tools and investigate the volume i/o issue step by step, and run 5 steps at least.
<<< Note >>>: If you suspect some issue root cause according to current call tools result, you can add call tools step by yourself

{phase_specific_guidance}

# CHAIN OF THOUGHT APPROACH

When troubleshooting, use a structured Chain of Thought approach to reason through problems:

1. **OBSERVATION**: Clearly identify what issue or symptom you're seeing
   - What errors are present in logs or events?
   - What behavior is unexpected or problematic?

2. **THINKING**:
   - What are the possible causes of this issue?
   - What components could be involved?
   - What tools can I use to investigate further?
   - What patterns should I look for in the results?

3. **INVESTIGATION**:
   - Execute tools in a logical sequence
   - For each tool, explain WHY you're using it and WHAT you expect to learn
   - After each result, analyze what it tells you and what to check next

4. **DIAGNOSIS**:
   - Based on evidence, determine the most likely root cause
   - Explain your reasoning with supporting evidence
   - Consider alternative explanations and why they're less likely

5. **RESOLUTION**:
   - Propose specific steps to resolve the issue
   - Explain why each step will help address the root cause
   - Consider potential side effects or risks

# HISTORICAL EXPERIENCE EXAMPLES

Here are examples of how to apply Chain of Thought reasoning to common volume issues:
{historical_experience_examples}

Follow these strict guidelines for safe, reliable, and effective troubleshooting:

1. **Knowledge Graph Prioritization**:
   - ALWAYS check the Knowledge Graph FIRST before using command execution tools.
   - Then use detailed query tools:
     * Use kg_get_entity_info(entity_type, id) to retrieve detailed information about specific entities
     * Use kg_get_related_entities(entity_type, id) to understand relationships between components
     * Use kg_get_all_issues() to find already detected issues in the system
     * Use kg_find_path(source_entity_type, source_id, target_entity_type, target_id) to trace dependencies between entities (e.g., Pod  PVC  PV  Drive)
     * Use kg_analyze_issues() to identify patterns and root causes from the Knowledge Graph
   - Only execute commands like kubectl or SSH when Knowledge Graph lacks needed information.

2. **Troubleshooting Process**:
   - Use the LangGraph ReAct module to reason about volume I/O errors based on parameters: `PodName`, `PodNamespace`, and `VolumePath`.
   - Most of time the pod's volume file system type is xfs, ext4, or btrfs. 
   - Follow this structured diagnostic process for local HDD/SSD/NVMe disks managed by CSI Baremetal:
     a. **Check Knowledge Graph**: First use Knowledge Graph tools (kg_*) to understand the current state and existing issues.
     b. **Confirm Issue**: If Knowledge Graph lacks information, run `kubectl logs <pod-name> -n <namespace>` and `kubectl describe pod <pod-name> -n <namespace>` to identify errors (e.g., "Input/Output Error", "Permission Denied", "FailedMount").
        - Analyze the pod/pvc/volume's definition with `kubectl get pod/pvc/pv <resource_name> -o yaml`. whether the definition has keywords like: readOnlyRootFilesystem, ReadOnlyMany, readOnly, etc.
     c. **Verify Configurations**: Check Pod, PVC, and PV with `kubectl get pod/pvc/pv <resource_name> -o yaml`. Confirm PV uses local volume, valid disk path (e.g., `/dev/sda`), and correct `nodeAffinity`. Verify mount points with `kubectl exec <pod-name> -n <namespace> -- df -h` and `ls -ld <mount-path>`.
     d. **Check CSI Baremetal Driver and Resources**:
        - Identify driver: `kubectl get storageclass <storageclass-name> -o yaml` (e.g., `csi-baremetal-sc-ssd`).
        - Verify driver pod: `kubectl get pods -n kube-system -l app=csi-baremetal` and `kubectl logs <driver-pod-name> -n kube-system`. Check for errors like "failed to mount".
        - Confirm driver registration: `kubectl get csidrivers`.
        - Check drive status: `kubectl get drive -o wide` and `kubectl get drive <drive-uuid> -o yaml`. Verify `Health: GOOD`, `Status: ONLINE`, `Usage: IN_USE`, and match `Path` (e.g., `/dev/sda`) with `VolumePath`.
        - Map drive to node: `kubectl get csibmnode` to correlate `NodeId` with hostname/IP.
        - Check AvailableCapacity: `kubectl get ac -o wide` to confirm size, storage class, and location (drive UUID).
        - Check LogicalVolumeGroup: `kubectl get lvg` to verify `Health: GOOD` and associated drive UUIDs.
     e. **Test Driver**: Create a test PVC/Pod using `csi-baremetal-sc-ssd` storage class (use provided YAML template). Check logs and events for read/write errors.
     f. **Verify Node Health**: Run `kubectl describe node <node-name>` to ensure `Ready` state and no `DiskPressure`. Verify disk mounting via SSH: `mount | grep <disk-path>`.
     g. **Check Permissions**: Verify file system permissions with `kubectl exec <pod-name> -n <namespace> -- ls -ld <mount-path>` and Pod `SecurityContext` settings.
     h. **Inspect Control Plane**: Check `kube-controller-manager` and `kube-scheduler` logs for provisioning/scheduling issues.
     i. **Test Hardware Disk**:
        - Identify disk: `kubectl get pv -o yaml` and `kubectl get drive <drive-uuid> -o yaml` to confirm `Path`.
        - Check health: `kubectl get drive <drive-uuid> -o yaml` and `ssh <node-name> sudo smartctl -a /dev/<disk-device>`. Verify `Health: GOOD`, zero `Reallocated_Sector_Ct` or `Current_Pending_Sector`.
        - Test performance: `ssh <node-name> sudo fio --name=read_test --filename=/dev/<disk-device> --rw=read --bs=4k --size=100M --numjobs=1 --iodepth=1 --runtime=60 --time_based --group_reporting`.
        - Check file system (if unmounted): `ssh <node-name> sudo xfs_repair -n /dev/<disk-device>` (requires approval).
        - Test via Pod: Create a test Pod (use provided YAML) and check logs for "Write OK" and "Read OK".
     j. **Propose Remediations**:
        - Bad sectors: Recommend disk replacement if `kubectl get drive` or SMART shows `Health: BAD` or non-zero `Reallocated_Sector_Ct`.
        - Performance issues: Suggest optimizing I/O scheduler or replacing disk if `fio` results show low IOPS (HDD: 100200, SSD: thousands, NVMe: tens of thousands).
        - File system corruption: Recommend `fsck` or 'xfs_repair' (if enabled/approved) after data backup.
        - Driver issues: Suggest restarting CSI Baremetal driver pod (if enabled/approved) if logs indicate errors.
   - Only propose remediations after analyzing diagnostic data. Ensure write/change commands (e.g., `fsck`, `kubectl delete pod`) are allowed and approved.
   - Try to find all of possible root causes before proposing any remediation steps. 

3. **Error Handling**:
   - If unresolved, provide a detailed report of findings (e.g., logs, drive status, SMART data, test results) and suggest manual intervention.

4. **Knowledge Graph Usage**:
   - Follow this effective Knowledge Graph navigation strategy:
     1. Start with discovery: Use kg_list_entity_types() to understand what entity types exist
     2. Find relevant entities: Use kg_list_entities(entity_type) to find specific entities of interest
     3. Get detailed information: Use kg_get_entity_info(entity_type, id) for specific entities
     4. Explore relationships: Use kg_get_related_entities(entity_type, id) to see connections
     5. Analyze issues: Use kg_get_all_issues() to find existing issues
     6. Trace dependencies: Use kg_find_path() to find connections between entities
   - Use kg_print_graph to get a human-readable overview of the entire system state.
   - First check issues with kg_get_all_issues before running diagnostic commands. These issues are critical information to find root cause.
   - Use kg_get_summary to get high-level statistics about the cluster state.
   - For root cause analysis, use kg_analyze_issues to identify patterns across the system.

7. **Constraints**:
   - Restrict operations to the Kubernetes cluster and configured worker nodes; do not access external networks or resources.
   - Do not modify cluster state (e.g., delete pods, change configurations) unless explicitly allowed and approved.
   - Adhere to `troubleshoot.timeout_seconds` for the troubleshooting workflow.
   - Always recommend data backup before suggesting write/change operations (e.g., `fsck`).

8. **Output**:
   - << Adding the analysis and summary for each steps when call tools >>
   - Try to find all of possible root causes before proposing any remediation steps.
   - Provide clear, concise explanations of diagnostic steps, findings, and remediation proposals.
   - Include performance benchmarks in reports (e.g., HDD: 100200 IOPS, SSD: thousands, NVMe: tens of thousands).
   - Don't ask questions to user, just decide by yourself.
   - **Don't output with JSON format, use plain text for better readability.**
   - **the output should include the following sections:**
    # Summary of Findings
    # Detailed Analysis
    # Relationship Analysis
    # Investigation Process
    # Potential Root Causes
    # Open Questions
    # Next Steps
    # Root Cause
    # **Fix Plan**      # this section must exist

You must adhere to these guidelines at all times to ensure safe, reliable, and effective troubleshooting of local disk issues in Kubernetes with the CSI Baremetal driver.
Adding the analysis and summary for each call tools steps
"""
        
    def format_user_query(self, query: str, pod_name: str = "", namespace: str = "", 
                        volume_path: str = "", investigation_plan: str = "", **kwargs) -> str:
        """
        Format user query for the Analysis phase
        
        Args:
            query: User query to format
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod
            volume_path: Path of the volume with I/O error
            investigation_plan: Investigation Plan generated by the Plan Phase
            **kwargs: Optional arguments for customizing the formatting
            
        Returns:
            str: Formatted user query
        """
        # Extract and format historical experience data from collected_info
        collected_info = kwargs.get('collected_info', {})
        historical_experiences_formatted = self.format_historical_experiences_from_collected_info(collected_info)
        
        # Format the query
        return f"""Phase 1 - ReAct Investigation: Execute the Investigation Plan to actively investigate the volume I/O error in pod {pod_name} in namespace {namespace} at volume path {volume_path}.

INVESTIGATION PLAN TO FOLLOW:
{investigation_plan}

HISTORICAL EXPERIENCE:
{historical_experiences_formatted}

SPECIAL CASE DETECTION:
After executing the Investigation Plan, you must determine if one of these special cases applies:

CASE 1 - NO ISSUES DETECTED:
If the Knowledge Graph and Investigation Plan execution confirm the system has no issues:
- Output a structured summary in the following format:
  ```
  Summary Finding: No issues detected in the system.
  Evidence: [Details from Knowledge Graph queries, e.g., no error logs found, all services operational]
  Advice: [Recommendations, e.g., continue monitoring the system]
  SKIP_PHASE2: YES
  ```

CASE 2 - MANUAL INTERVENTION REQUIRED:
If the Knowledge Graph and Investigation Plan execution confirm the issue cannot be fixed automatically:
- Output a structured summary in the following format:
  ```
  Summary Finding: Issue detected, but requires manual intervention.
  Evidence: [Details from Knowledge Graph queries, e.g., specific error or configuration requiring human action]
  Advice: [Detailed step-by-step instructions for manual resolution, e.g., specific commands or actions for the user]
  SKIP_PHASE2: YES
  ```

CASE 3 - AUTOMATIC FIX POSSIBLE:
If the issue can be resolved automatically:
- Generate a fix plan based on the Investigation Plan's results
- Output a comprehensive root cause analysis and fix plan
- Do NOT include the SKIP_PHASE2 marker

<<< Note >>>: Please following the Investigation Plan to run tools step by step, and run 8 steps at least.
<<< Note >>>: Please provide the root cause and fix plan analysis within 30 tool calls.
"""
        
    def get_tool_prompt(self, **kwargs) -> str:
        """
        Return prompts for tool invocation in the Analysis phase
        
        Args:
            **kwargs: Optional arguments for customizing the prompt
            
        Returns:
            str: Tool invocation prompt for the Analysis phase
        """
        return """
PHASE 1 RESTRICTIONS:
- NO destructive operations (no kubectl_apply, kubectl_delete, fsck_check)
- NO test resource creation
- NO hardware modifications
- FOCUS on comprehensive investigation and root cause analysis

For each tool execution:
1. Explain WHY you are using this tool
2. Describe WHAT you expect to learn
3. After seeing the result, analyze what it tells you
4. Determine the next logical step based on the findings
"""
    
    def _get_phase1_guidance(self, final_output_example: str) -> str:
        """
        Get guidance for Phase 1 (Investigation)
        
        Args:
            final_output_example: Example of final output format
            
        Returns:
            str: Phase 1 guidance prompt
        """
        return f"""
You are currently in Phase 1 (Investigation). Your primary task is to perform comprehensive root cause analysis and evidence collection using investigation tools.

PHASE 1 RESTRICTIONS:
- NO destructive operations (no kubectl_apply, kubectl_delete, fsck_check)
- NO test resource creation
- NO hardware modifications
- FOCUS on comprehensive investigation and root cause analysis

OUTPUT REQUIREMENTS:
Provide a detailed investigation report that includes:

1. Summary of Findings:
   - Brief overview of the main issues discovered
   - Severity assessment of the overall situation

2. Detailed Analysis:
   - Primary Issues:
     * Description of each major problem identified
     * Evidence supporting each issue (logs, metrics, events)
     * Impact assessment on the system and services
     * Probability or confidence level in the diagnosis
   - Secondary Issues:
     * Description of minor or related problems
     * Potential consequences if left unaddressed
   - System Metrics:
     * Key performance indicators and their current values
     * Any metrics that deviate from normal ranges
   - Environmental Factors:
     * External conditions that may be contributing to the issues

3. Relationship Analysis:
   - Connections between different issues
   - How components of the system are affecting each other

4. Investigation Process:
   - Steps taken during the troubleshooting
   - Tools and commands used
   - Reasoning behind each investigative action

5. Potential Root Causes:
   - List of possible underlying causes
   - Evidence supporting each potential root cause
   - Likelihood assessment for each cause

6. Open Questions:
   - Any unresolved aspects of the investigation
   - Areas that require further examination

7. Next Steps:
   - Recommended further diagnostic actions
   - Suggestions for additional data collection or analysis
8. Root Cause:
    - The most likely root cause based on the evidence collected
8. Fix Plan:
    - Proposed remediation steps to address the issues

INVESTIGATION RESULT EXAMPLE:
{final_output_example}
"""
</file>

<file path="llm_graph/prompt_managers/phase2_prompt_manager.py">
#!/usr/bin/env python3
"""
Phase 2 (Remediation) Prompt Manager for Kubernetes Volume Troubleshooting

This module provides the prompt manager implementation for the Remediation phase
of the troubleshooting system.
"""

import logging
from typing import Dict, List, Any, Optional
from llm_graph.prompt_managers.base_prompt_manager import BasePromptManager

logger = logging.getLogger(__name__)

class Phase2PromptManager(BasePromptManager):
    """
    Prompt manager for the Remediation phase (Phase 2)
    
    Handles prompt generation and formatting for the Remediation phase,
    which executes the Fix Plan to resolve identified issues.
    """
    
    def __init__(self, config_data: Dict[str, Any] = None):
        """
        Initialize the Phase 2 Prompt Manager
        
        Args:
            config_data: Configuration data for the system
        """
        super().__init__(config_data)
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def get_system_prompt(self, **kwargs) -> str:
        """
        Return the system prompt for the Remediation phase
        
        Args:
            **kwargs: Optional arguments for customizing the prompt
            
        Returns:
            str: System prompt for the Remediation phase
        """
        # Load historical experience data
        historical_experience_examples = self._load_historical_experience()
        
        # Phase-specific guidance
        phase_specific_guidance = self._get_phase2_guidance()
        
        # Create system message with Chain of Thought (CoT) format and historical experience examples
        return f"""You are an expert Kubernetes storage troubleshooter. Your task is to execute the Fix Plan to resolve volume I/O errors in Kubernetes pods.

TASK:
1. Execute the Fix Plan to resolve the identified issues
2. Validate the fixes to ensure they resolved the problem
3. Provide a detailed report of the remediation actions taken

KNOWLEDGE GRAPH TOOLS USAGE:
- When using knowledge graph tools, use the parameters of entity_type and id format:
  * Entity ID formats:
    - Pod: "gnode:Pod:<namespace>/<name>" (example: "gnode:Pod:default/test-pod-1-0")
    - PVC: "gnode:PVC:<namespace>/<name>" (example: "gnode:PVC:default/test-pvc-1")
    - PV: "gnode:PV:<name>" (example: "gnode:PV:pv-test-123")
    - Drive: "gnode:Drive:<uuid>" (example: "gnode:Drive:a1b2c3d4-e5f6")
    - Node: "gnode:Node:<name>" (example: "gnode:Node:kind-control-plane")
    - StorageClass: "gnode:StorageClass:<name>" (example: "gnode:StorageClass:csi-baremetal-sc")
    - LVG: "gnode:LVG:<name>" (example: "gnode:LVG:lvg-1")
    - AC: "gnode:AC:<name>" (example: "gnode:AC:ac-node1-ssd")
    - Volume: "gnode:Volume:<namespace>/<name>" (example: "gnode:Volume:default/vol-1")
    - System: "gnode:System:<entity_name>" (example: "gnode:System:kernel")
    - ClusterNode: "gnode:ClusterNode:<name>" (example: "gnode:ClusterNode:worker-1")
    - HistoricalExperience: "gnode:HistoricalExperience:<experience_id>" (example: "gnode:HistoricalExperience:exp-001")

  * Helper tools for generating entity IDs:
    - Pod: kg_get_entity_of_pod(namespace, name)  returns "gnode:Pod:namespace/name"
    - PVC: kg_get_entity_of_pvc(namespace, name)  returns "gnode:PVC:namespace/name"
    - PV: kg_get_entity_of_pv(name)  returns "gnode:PV:name"
    - Drive: kg_get_entity_of_drive(uuid)  returns "gnode:Drive:uuid"
    - Node: kg_get_entity_of_node(name)  returns "gnode:Node:name"
    - StorageClass: kg_get_entity_of_storage_class(name)  returns "gnode:StorageClass:name"
    - LVG: kg_get_entity_of_lvg(name)  returns "gnode:LVG:name"
    - AC: kg_get_entity_of_ac(name)  returns "gnode:AC:name"
    - Volume: kg_get_entity_of_volume(namespace, name)  returns "gnode:Volume:namespace/name"
    - System: kg_get_entity_of_system(entity_name)  returns "gnode:System:entity_name"
    - ClusterNode: kg_get_entity_of_cluster_node(name)  returns "gnode:ClusterNode:name"
    - HistoricalExperience: kg_get_entity_of_historical_experience(experience_id)  returns "gnode:HistoricalExperience:experience_id"

- Start with discovery tools to understand what's in the Knowledge Graph:
  * Use kg_list_entity_types() to discover available entity types and their counts
  * Use kg_list_entities(entity_type) to find specific entities of a given type
  * Use kg_list_relationship_types() to understand how entities are related

- Then use detailed query tools:
  * Use kg_get_entity_info(entity_type, id) to retrieve detailed information about specific entities
  * Use kg_get_related_entities(entity_type, id) to understand relationships between components
  * Use kg_get_all_issues() to find already detected issues in the system
  * Use kg_find_path(source_entity_type, source_id, target_entity_type, target_id) to trace dependencies

{phase_specific_guidance}

# CHAIN OF THOUGHT APPROACH

When troubleshooting, use a structured Chain of Thought approach to reason through problems:

1. **OBSERVATION**: Clearly identify what issue or symptom you're seeing
   - What errors are present in logs or events?
   - What behavior is unexpected or problematic?

2. **THINKING**:
   - What are the possible causes of this issue?
   - What components could be involved?
   - What tools can I use to investigate further?
   - What patterns should I look for in the results?

3. **INVESTIGATION**:
   - Execute tools in a logical sequence
   - For each tool, explain WHY you're using it and WHAT you expect to learn
   - After each result, analyze what it tells you and what to check next

4. **DIAGNOSIS**:
   - Based on evidence, determine the most likely root cause
   - Explain your reasoning with supporting evidence
   - Consider alternative explanations and why they're less likely

5. **RESOLUTION**:
   - Propose specific steps to resolve the issue
   - Explain why each step will help address the root cause
   - Consider potential side effects or risks

# HISTORICAL EXPERIENCE EXAMPLES

Here are examples of how to apply Chain of Thought reasoning to common volume issues:
{historical_experience_examples}

CONSTRAINTS:
- Follow the Fix Plan step by step
- Use only the tools available in the Phase2 tool registry
- Validate each fix to ensure it was successful
- Provide a clear, detailed report of all actions taken

OUTPUT FORMAT:
Your response must include:
1. Actions Taken
2. Validation Results
3. Resolution Status
4. Recommendations
"""
        
    def format_user_query(self, query: str, phase1_final_response: str = "", **kwargs) -> str:
        """
        Format user query for the Remediation phase
        
        Args:
            query: User query to format
            phase1_final_response: Response from Phase 1 containing root cause and fix plan
            **kwargs: Optional arguments for customizing the formatting
            
        Returns:
            str: Formatted user query
        """
        # Extract and format historical experience data from collected_info
        collected_info = kwargs.get('collected_info', {})
        historical_experiences_formatted = self.format_historical_experiences_from_collected_info(collected_info)
        
        # Format the query
        return f"""Phase 2 - Remediation: Execute the fix plan to resolve the identified issue.

Root Cause and Fix Plan: {phase1_final_response}

HISTORICAL EXPERIENCE:
{historical_experiences_formatted}

<<< Note >>>: Please try to fix issue within 30 tool calls.
"""
        
    def get_tool_prompt(self, **kwargs) -> str:
        """
        Return prompts for tool invocation in the Remediation phase
        
        Args:
            **kwargs: Optional arguments for customizing the prompt
            
        Returns:
            str: Tool invocation prompt for the Remediation phase
        """
        return """
PHASE 2 CAPABILITIES:
- Execute remediation actions based on Phase 1 **Fix Plan**
- Create test resources to validate fixes
- Run comprehensive volume testing
- Perform hardware diagnostics and repairs
- Clean up test resources after validation

For each tool execution:
1. Explain WHY you are using this tool
2. Describe WHAT you expect to accomplish
3. After seeing the result, validate if the action was successful
4. Determine if additional steps are needed
"""
    
    def _get_phase2_guidance(self) -> str:
        """
        Get guidance for Phase 2 (Action/Remediation)
        
        Returns:
            str: Phase 2 guidance prompt
        """
        return """
You are currently in Phase 2 (Action/Remediation). You have access to all Phase 1 investigation tools PLUS action tools for implementing fixes.

PHASE 2 CAPABILITIES:
- Execute remediation actions based on Phase 1 **Fix Plan**
- Create test resources to validate fixes
- Run comprehensive volume testing
- Perform hardware diagnostics and repairs
- Clean up test resources after validation

OUTPUT REQUIREMENTS:
Provide a detailed remediation report that includes:
1. Actions Taken: List of all remediation steps executed
2. Test Results: Results from validation tests
3. Resolution Status: Whether issues were resolved
4. Remaining Issues: Any unresolved problems
5. Recommendations: Suggestions for ongoing monitoring or future improvements
"""
</file>

<file path="llm_graph/prompt_managers/plan_prompt_manager.py">
#!/usr/bin/env python3
"""
Plan Phase Prompt Manager for Kubernetes Volume Troubleshooting

This module provides the prompt manager implementation for the Plan phase
of the troubleshooting system.
"""

import logging
import json
from typing import Dict, List, Any, Optional
from llm_graph.prompt_managers.base_prompt_manager import BasePromptManager

logger = logging.getLogger(__name__)

class PlanPromptManager(BasePromptManager):
    """
    Prompt manager for the Plan phase
    
    Handles prompt generation and formatting for the Plan phase,
    which generates an Investigation Plan for Phase 1.
    """
    
    def __init__(self, config_data: Dict[str, Any] = None):
        """
        Initialize the Plan Prompt Manager
        
        Args:
            config_data: Configuration data for the system
        """
        super().__init__(config_data)
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def get_system_prompt(self, use_react: bool = False, **kwargs) -> str:
        """
        Return the system prompt for the Plan phase
        
        Args:
            use_react: Whether to use React mode (default: False)
            **kwargs: Optional arguments for customizing the prompt
            
        Returns:
            str: System prompt for the Plan phase
        """
        # Base system prompt for both modes
        base_prompt = """You are an expert Kubernetes storage troubleshooter. Your task is to refine a draft Investigation Plan for troubleshooting volume read/write errors in Kubernetes.

TASK:
1. Review the draft plan containing preliminary steps from rule-based analysis and mandatory static steps
2. Analyze the Knowledge Graph and historical experience data
3. Refine the plan by:
   - Respecting existing steps (do not remove or modify static steps as much as possible)
   - Adding necessary additional steps using only the provided Phase1 tools
   - Reordering steps if needed for logical flow
   - Adding fallback steps for error handling

CONSTRAINTS:
- When you identify a knowledge gap, use the appropriate MCP tool to gather the information you need. Don't guess or make assumptions when you can use a tool to get accurate information.
- You must only reference tools available in the Phase1 tool registry
- All tool references must match the exact name and parameter format shown in the tools registry
- Include at least one disk-related check step and one volume-related check step.
- Max Steps: 15
- IMPORTANT: Each tool should be used at most once in the entire plan. Do not include duplicate tool calls. If a tool is already used in a step, do not use it again in another step.

OUTPUT FORMAT:
Your response must be a refined Investigation Plan with steps in this format:
Step X: [Description and Reason] | Tool: [tool_name(parameters)] | Expected: [expected]

You may include fallback steps for error handling in this format:
Fallback Steps (if main steps fail):
Step FX: [Description and Reason] | Tool: [tool_name(parameters)] | Expected: [expected] | Trigger: [failure_condition]

The plan must be comprehensive, logically structured, and include all necessary steps to investigate the volume I/O errors.
"""

        # Add React-specific additions if in React mode
        if use_react:
            # Get available MCP tools information
            mcp_tools_info = kwargs.get('mcp_tools_info', "")

            react_additions = f"""
You are operating in a ReAct (Reasoning and Acting) framework where you can:
1. REASON about the problem and identify knowledge gaps
2. ACT by calling external tools to gather information
3. OBSERVE the results and update your understanding
4. Continue this loop until you have enough information to create a comprehensive plan

Available MCP tools:
{mcp_tools_info}

When you identify a knowledge gap, use the appropriate MCP tool to gather the information you need. Don't guess or make assumptions when you can use a tool to get accurate information.

When you've completed the Investigation Plan, include the marker [END_GRAPH] at the end of your message.
"""
            return base_prompt + react_additions
        
        # Return base prompt for Legacy mode
        return base_prompt
        
    def format_user_query(self, query: str, draft_plan: List[Dict[str, Any]] = None, 
                        pod_name: str = "", namespace: str = "", volume_path: str = "",
                        kg_context: Dict[str, Any] = None, phase1_tools: List[Dict[str, Any]] = None,
                        use_react: bool = True, **kwargs) -> str:
        """
        Format user query for the Plan phase
        
        Args:
            query: User query to format
            draft_plan: Draft plan from rule-based generator and static steps
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod
            volume_path: Path of the volume with I/O error
            kg_context: Knowledge Graph context with historical experience
            phase1_tools: Complete Phase1 tool registry
            use_react: Whether to use React mode (default: True)
            **kwargs: Optional arguments for customizing the formatting
            
        Returns:
            str: Formatted user query
        """
        # Format input data for LLM context
        kg_context_str = self._format_json_safely(kg_context, fallback_message="Knowledge Graph context (simplified format)")
        draft_plan_str = self._format_json_safely(draft_plan, fallback_message="Draft plan (simplified format)")
        phase1_tools_str = self._format_json_safely(phase1_tools, fallback_message="Phase1 tools (simplified format)")
        
        # Format MCP tools if available
        mcp_tools = kwargs.get('mcp_tools', [])
        mcp_tools_str = ""
        if mcp_tools:
            mcp_tools_str = self._format_json_safely(mcp_tools, fallback_message="MCP tools (simplified format)")
        
        # Extract and format historical experience data from kg_context
        historical_experiences_formatted = self._format_historical_experiences(kg_context)
        
        # Extract tools already used in draft plan
        used_tools = set()
        if draft_plan:
            for step in draft_plan:
                tool = step.get('tool', '')
                if '(' in tool:
                    tool = tool.split('(')[0]
                used_tools.add(tool)
        
        used_tools_str = ", ".join(used_tools)
        
        # Base user message content for both modes
        base_message = f"""# INVESTIGATION PLAN GENERATION
## TARGET: Volume read/write errors in pod {pod_name} (namespace: {namespace}, volume path: {volume_path})

This plan will guide troubleshooting in subsequent phases. Each step will execute specific tools according to this plan.

## BACKGROUND INFORMATION

### 1. KNOWLEDGE GRAPH CONTEXT
Current base knowledge and hardware information. Issues identified in the Knowledge Graph are critical:
{kg_context_str}

### 2. HISTORICAL EXPERIENCE
Learn from previous similar cases to improve your plan:
{historical_experiences_formatted}

### 3. DRAFT PLAN
Static plan steps and preliminary steps from rule-based generator:
{draft_plan_str}

### 4. TOOLS ALREADY USED IN DRAFT PLAN
{used_tools_str}

### 5. AVAILABLE TOOLS FOR PHASE1
These tools will be used in next phases (reference only, do not invoke):
{phase1_tools_str}

### 6. AVAILABLE MCP TOOLS
These MCP tools can be used for cloud-specific diagnostics:
{mcp_tools_str}

When you identify a knowledge gap, use the appropriate MCP tool to gather the information you need. Don't guess or make assumptions when you can use a tool to get accurate information.

## PLANNING INSTRUCTIONS

### PRIMARY OBJECTIVE
Create a comprehensive investigation plan that identifies potential problems and provides specific steps to diagnose and resolve volume read/write errors.

### SPECIFIC TASKS
1. **Task 1:** Analyze the Knowledge Graph context and historical experience to infer and list all possible problems
2. **Task 2:** For each possible problem, create detailed investigation steps using appropriate tools

### PLANNING GUIDELINES
1. Respect existing steps from the draft plan (both rule-based and static steps)
2. Infer potential volume read/write error phenomena and root causes based on Knowledge Graph and historical experience
3. Formulate detailed investigation steps, prioritizing verification steps most likely to identify the issue
4. Add additional steps as needed using available Phase1 tools
5. Reorder steps if necessary for logical flow
6. Ensure all steps reference only tools from the Phase1 tool registry

## IMPORTANT CONSTRAINTS
1. Search related information by MCP tools as referenced in the AVAILABLE TOOLS section at first
2. Include static steps from the draft plan without modification
3. Use historical experience data to inform additional steps and refinements
4. Ensure all tool references follow the format shown in the AVAILABLE TOOLS
5. IMPORTANT: Do not add steps that use tools already present in the draft plan. Each tool should be used at most once in the entire plan

## REQUIRED OUTPUT FORMAT

Investigation Plan:
PossibleProblem 1: [Problem description, e.g., PVC configuration errors, access mode is incorrect]
Step 1: [Description and Reason] | Tool: [tool_name(parameters)] | Expected: [expected]
Step 2: [Description and Reason] | Tool: [tool_name(parameters)] | Expected: [expected]
...
PossibleProblem 2: [Problem description, e.g., Drive status is OFFLINE]
Step 1: [Description and Reason] | Tool: [tool_name(parameters)] | Expected: [expected]
Step 2: [Description and Reason] | Tool: [tool_name(parameters)] | Expected: [expected]
...
"""
        
        # Add mode-specific instructions
        if use_react:
            # React mode instructions
            react_additions = """
## TASK
1. Analyze the available information to understand the context
2. Identify any knowledge gaps that need to be filled
3. Use MCP tools to gather additional information as needed
4. Create a comprehensive Investigation Plan with specific steps to diagnose and resolve the volume I/O error

Please start by analyzing the available information and identifying any knowledge gaps.
"""
            return base_message + react_additions
        else:
            return base_message
        
    def get_tool_prompt(self, **kwargs) -> str:
        """
        Return prompts for tool invocation in the Plan phase
        
        Args:
            **kwargs: Optional arguments for customizing the prompt
            
        Returns:
            str: Tool invocation prompt for the Plan phase
        """
        return """
When you identify a knowledge gap, use the appropriate MCP tool to gather the information you need.
Don't guess or make assumptions when you can use a tool to get accurate information.
"""
    
    def _format_json_safely(self, data: Any, fallback_message: str = "Data could not be formatted") -> str:
        """
        Format data as JSON with fallback for complex objects
        
        Args:
            data: Data to format as JSON
            fallback_message: Message to use if formatting fails
            
        Returns:
            str: Formatted JSON string or fallback message
        """
        try:
            if data is None:
                return fallback_message
                
            return json.dumps(data, indent=2)
        except Exception as e:
            self.logger.error(f"Error formatting JSON: {e}")
            return f"{fallback_message} (Error: {str(e)})"
    
    def _format_historical_experiences(self, kg_context: Dict[str, Any]) -> str:
        """
        Format historical experience data from Knowledge Graph context
        
        Args:
            kg_context: Knowledge Graph context containing historical experience data
            
        Returns:
            str: Formatted historical experience data for LLM consumption
        """
        try:
            # Extract historical experiences from kg_context
            historical_experiences = kg_context.get('historical_experiences', [])
            
            if not historical_experiences:
                return "No historical experience data available."
            
            # Format historical experiences in a clear, structured way
            return self._format_historical_experience_entries(historical_experiences)
            
        except Exception as e:
            self.logger.error(f"Error formatting historical experiences: {e}")
            return "Error formatting historical experience data."
    
    def _format_historical_experience_entries(self, experiences: List[Dict[str, Any]]) -> str:
        """
        Format historical experience entries using Chain of Thought (CoT) structure
        
        Args:
            experiences: List of historical experience entries
            
        Returns:
            str: Formatted entries with CoT structure
        """
        formatted_entries = []
        
        for idx, exp in enumerate(experiences, 1):
            # Get attributes from the experience
            attributes = exp.get('attributes', {})
            
            # Check for new CoT format fields first
            observation = attributes.get('observation', attributes.get('phenomenon', 'Unknown observation'))
            thinking = attributes.get('thinking', [])
            investigation = attributes.get('investigation', [])
            diagnosis = attributes.get('diagnosis', attributes.get('root_cause', 'Unknown diagnosis'))
            resolution = attributes.get('resolution', attributes.get('resolution_method', 'No resolution method provided'))
            
            # Format thinking points
            thinking_formatted = ""
            if thinking:
                thinking_formatted = "Thinking:\n"
                for i, point in enumerate(thinking, 1):
                    thinking_formatted += f"{i}. {point}\n"
            
            # Format investigation steps
            investigation_formatted = ""
            if investigation:
                investigation_formatted = "Investigation:\n"
                for i, step in enumerate(investigation, 1):
                    if isinstance(step, dict):
                        step_text = step.get('step', '')
                        reasoning = step.get('reasoning', '')
                        investigation_formatted += f"{i}. {step_text}\n   - {reasoning}\n"
                    else:
                        investigation_formatted += f"{i}. {step}\n"
            else:
                # Fall back to legacy format
                localization_method = attributes.get('localization_method', '')
                if localization_method:
                    investigation_formatted = f"Investigation:\n{localization_method}\n"
            
            # Format resolution steps
            resolution_formatted = ""
            if isinstance(resolution, list):
                resolution_formatted = "Resolution:\n"
                for i, step in enumerate(resolution, 1):
                    resolution_formatted += f"{i}. {step}\n"
            else:
                resolution_formatted = f"Resolution:\n{resolution}\n"
            
            # Format the entry using CoT structure
            entry = f"""## HISTORICAL EXPERIENCE #{idx}: {observation}

**OBSERVATION**: {observation}

**THINKING**:
{thinking_formatted}

**INVESTIGATION**:
{investigation_formatted}

**DIAGNOSIS**: {diagnosis}

**RESOLUTION**:
{resolution_formatted}
"""
            formatted_entries.append(entry)
        
        return "\n".join(formatted_entries)
</file>

<file path="llm_graph/__init__.py">
"""
LangGraph Implementation for Kubernetes Volume Troubleshooting

This package provides the LangGraph implementations for the different phases
of the Kubernetes volume troubleshooting system using the Strategy Pattern.
"""

from llm_graph.langgraph_interface import LangGraphInterface
from llm_graph.prompt_manager_interface import PromptManagerInterface
from llm_graph.graph_utility import GraphUtility

# Import prompt managers
from llm_graph.prompt_managers.base_prompt_manager import BasePromptManager
from llm_graph.prompt_managers.plan_prompt_manager import PlanPromptManager
from llm_graph.prompt_managers.phase1_prompt_manager import Phase1PromptManager
from llm_graph.prompt_managers.phase2_prompt_manager import Phase2PromptManager

# Import graph implementations
from llm_graph.graphs.plan_llm_graph import PlanLLMGraph
from llm_graph.graphs.phase1_llm_graph import Phase1LLMGraph
from llm_graph.graphs.phase2_llm_graph import Phase2LLMGraph

__all__ = [
    'LangGraphInterface',
    'PromptManagerInterface',
    'GraphUtility',
    'BasePromptManager',
    'PlanPromptManager',
    'Phase1PromptManager',
    'Phase2PromptManager',
    'PlanLLMGraph',
    'Phase1LLMGraph',
    'Phase2LLMGraph',
]
</file>

<file path="llm_graph/graph_utility.py">
#!/usr/bin/env python3
"""
Graph Utility for LangGraph Workflows

This module provides utility functions for LangGraph workflows
used in the Kubernetes volume troubleshooting system.
"""

import logging
import json
import yaml
import os
from typing import Dict, List, Any, Callable, Set, Tuple, Optional
from rich.console import Console
from rich.panel import Panel

from langgraph.graph import StateGraph
from langchain_core.messages import ToolMessage, BaseMessage
from troubleshooting.execute_tool_node import ExecuteToolNode
from troubleshooting.hook_manager import HookManager

logger = logging.getLogger(__name__)

class GraphUtility:
    """
    Utility class for common LangGraph operations
    
    Provides common functionality for LangGraph workflows, including
    node and edge management, tool execution, and state updates.
    """
    
    def __init__(self, config_data: Dict[str, Any] = None):
        """
        Initialize the GraphUtility
        
        Args:
            config_data: Configuration data for the system
        """
        self.config_data = config_data or {}
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.console = Console()
        self.file_console = Console(file=open('troubleshoot.log', 'a'))
        
    def add_node_and_edge(self, graph: StateGraph, node_name: str, 
                         node_func: Callable, source_node: Optional[str] = None):
        """
        Add a node and its edge to the graph
        
        Args:
            graph: StateGraph to modify
            node_name: Name of the node to add
            node_func: Function to execute for this node
            source_node: Source node for the edge (optional)
        """
        self.logger.info(f"Adding node: {node_name}")
        graph.add_node(node_name, node_func)
        
        if source_node:
            self.logger.info(f"Adding edge: {source_node} -> {node_name}")
            graph.add_edge(source_node, node_name)
            
    def execute_tool(self, tool_call: Dict[str, Any]) -> ToolMessage:
        """
        Execute a single tool and return a ToolMessage
        
        Args:
            tool_call: Tool call information
            
        Returns:
            ToolMessage: Result of the tool execution
        """
        try:
            tool_name = tool_call.get("name", "")
            tool_args = tool_call.get("args", {})
            
            # Log the tool execution
            self.logger.info(f"Executing tool: {tool_name}")
            self.logger.info(f"Arguments: {json.dumps(tool_args, indent=2)}")
            
            # Execute the tool (placeholder - actual implementation would call the tool)
            result = f"Tool {tool_name} executed with args {tool_args}"
            
            # Return the result as a ToolMessage
            return ToolMessage(content=result, tool_call_id=tool_call.get("id", ""))
        except Exception as e:
            self.logger.error(f"Error executing tool {tool_call.get('name', '')}: {str(e)}")
            return ToolMessage(content=f"Error: {str(e)}", tool_call_id=tool_call.get("id", ""))
            
    def update_state(self, state: Dict[str, Any], results: Any) -> Dict[str, Any]:
        """
        Update graph state with results
        
        Args:
            state: Current state
            results: Results to add to the state
            
        Returns:
            Dict[str, Any]: Updated state
        """
        # Increment iteration count if present
        if "iteration_count" in state:
            state["iteration_count"] += 1
            
        # Add results to messages if present
        if "messages" in state and hasattr(results, "content"):
            state["messages"].append(results)
            
        return state
        
    def load_tool_config(self) -> Tuple[Set[str], Set[str]]:
        """
        Load tool configuration to determine parallel and serial tools
        
        Returns:
            Tuple[Set[str], Set[str]]: Sets of parallel and serial tool names
        """
        try:
            config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'config.yaml')
            with open(config_path, 'r') as f:
                config_data = yaml.safe_load(f)
                
            tool_config = config_data.get("tools", {})
            parallel_tools = set(tool_config.get("parallel", []))
            serial_tools = set(tool_config.get("serial", []))
            
            # Log the configuration
            self.logger.info(f"Loaded tool configuration: {len(parallel_tools)} parallel tools, {len(serial_tools)} serial tools")
            
            return parallel_tools, serial_tools
        except Exception as e:
            self.logger.error(f"Error loading tool configuration: {e}")
            # Return empty sets as fallback (all tools will be treated as serial)
            return set(), set()
        
    def create_execute_tool_node(self, tools: List[Any], parallel_tools: Optional[Set[str]] = None, 
                               serial_tools: Optional[Set[str]] = None) -> ExecuteToolNode:
        """
        Create and configure the ExecuteToolNode
        
        Args:
            tools: List of tools
            parallel_tools: Set of tool names to execute in parallel (optional)
            serial_tools: Set of tool names to execute serially (optional)
            
        Returns:
            ExecuteToolNode: Configured ExecuteToolNode
        """
        # If parallel_tools and serial_tools are not provided, load from config
        if parallel_tools is None or serial_tools is None:
            parallel_tools_config, serial_tools_config = self.load_tool_config()
            parallel_tools = parallel_tools or parallel_tools_config
            serial_tools = serial_tools or serial_tools_config
        
        # If a tool is not explicitly categorized, default to serial
        all_tool_names = {tool.name for tool in tools}
        uncategorized_tools = all_tool_names - (parallel_tools or set()) - (serial_tools or set())
        if uncategorized_tools:
            self.logger.info(f"Found {len(uncategorized_tools)} uncategorized tools, defaulting to serial")
            if serial_tools is None:
                serial_tools = set()
            serial_tools.update(uncategorized_tools)
        
        # Create a hook manager for console output
        hook_manager = HookManager(console=self.console, file_console=self.file_console)
        
        # Register hook functions with the hook manager
        hook_manager.register_before_call_hook(self._before_call_tools_hook)
        hook_manager.register_after_call_hook(self._after_call_tools_hook)
        
        # Create ExecuteToolNode with the configured tools
        self.logger.info(f"Creating ExecuteToolNode for execution of {len(parallel_tools or set())} parallel and {len(serial_tools or set())} serial tools")
        execute_tool_node = ExecuteToolNode(tools, parallel_tools or set(), serial_tools or set(), name="execute_tools")
        
        # Register hook manager with the ExecuteToolNode
        execute_tool_node.register_before_call_hook(hook_manager.run_before_hook)
        execute_tool_node.register_after_call_hook(hook_manager.run_after_hook)
        
        return execute_tool_node
    
    def extract_final_message(self, response: Dict[str, Any]) -> str:
        """
        Extract the final message from a graph response
        
        Args:
            response: Response from the graph
            
        Returns:
            str: Final message content
        """
        if not response.get("messages"):
            return "Failed to generate results"
            
        if isinstance(response["messages"], list):
            return response["messages"][-1].content
        else:
            return response["messages"].content
    
    def _before_call_tools_hook(self, tool_name: str, args: Dict[str, Any], call_type: str = "Serial") -> None:
        """
        Hook function called before a tool is executed
        
        Args:
            tool_name: Name of the tool being called
            args: Arguments passed to the tool
            call_type: Type of call execution ("Parallel" or "Serial")
        """
        try:
            # Format arguments for better readability
            formatted_args = json.dumps(args, indent=2) if args else "None"
            
            # Format the tool usage in a nice way
            if formatted_args != "None":
                # Print to console
                tool_panel = Panel(
                    f"[bold yellow]Tool:[/bold yellow] [green]{tool_name}[/green] [bold cyan]({call_type})[/bold cyan]\n\n"
                    f"[bold yellow]Arguments:[/bold yellow]\n[blue]{formatted_args}[/blue]",
                    title="[bold magenta]Start to Call Tools",
                    border_style="magenta",
                    safe_box=True
                )
                self.console.print(tool_panel)
            else:
                # Simple version for tools without arguments
                tool_panel = Panel(
                    f"[bold yellow]Tool:[/bold yellow] [green]{tool_name}[/green] [bold cyan]({call_type})[/bold cyan]\n\n"
                    f"[bold yellow]Arguments:[/bold yellow] None",
                    title="[bold magenta]Start to Call Tools",
                    border_style="magenta",
                    safe_box=True
                )
                self.console.print(tool_panel)

            # Also log to file console
            self.file_console.print(f"Executing tool: {tool_name} ({call_type})")
            self.file_console.print(f"Parameters: {formatted_args}")
            
            # Log to standard logger
            self.logger.info(f"Executing tool: {tool_name} ({call_type})")
            self.logger.info(f"Parameters: {formatted_args}")
        except Exception as e:
            self.logger.error(f"Error in before_call_tools_hook: {e}")

    def _after_call_tools_hook(self, tool_name: str, args: Dict[str, Any], result: Any, call_type: str = "Serial") -> None:
        """
        Hook function called after a tool is executed
        
        Args:
            tool_name: Name of the tool that was called
            args: Arguments that were passed to the tool
            result: Result returned by the tool
            call_type: Type of call execution ("Parallel" or "Serial")
        """
        try:
            # Format result for better readability
            if isinstance(result, ToolMessage):
                result_content = result.content
                result_status = result.status if hasattr(result, 'status') else 'success'
                formatted_result = f"Status: {result_status}\nContent: {result_content[:1000]}"
            else:
                formatted_result = str(result)[:1000]
            
            # Print tool result to console
            tool_panel = Panel(
                f"[bold cyan]Tool completed:[/bold cyan] [green]{tool_name}[/green] [bold cyan]({call_type})[/bold cyan]\n"
                f"[bold cyan]Result:[/bold cyan]\n[yellow]{formatted_result}[/yellow]",
                title="[bold magenta]Call tools Result",
                border_style="magenta",
                safe_box=True
            )
            self.console.print(tool_panel)

            # Also log to file console
            self.file_console.print(f"Tool completed: {tool_name} ({call_type})")
            self.file_console.print(f"Result: {formatted_result}")
            
            # Log to standard logger
            self.logger.info(f"Tool completed: {tool_name} ({call_type})")
            self.logger.info(f"Result: {formatted_result}")
        except Exception as e:
            self.logger.error(f"Error in after_call_tools_hook: {e}")
</file>

<file path="llm_graph/langgraph_interface.py">
#!/usr/bin/env python3
"""
Abstract Interface for LangGraph Workflows

This module defines the abstract base class for all LangGraph workflows
used in the Kubernetes volume troubleshooting system.
"""

import logging
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional, Tuple
from langgraph.graph import StateGraph

logger = logging.getLogger(__name__)

class LangGraphInterface(ABC):
    """
    Abstract base class for LangGraph workflows
    
    Defines the interface that all LangGraph implementations must follow,
    ensuring consistency across different phases of the troubleshooting system.
    """
    
    @abstractmethod
    def initialize_graph(self) -> StateGraph:
        """
        Initialize and return the LangGraph StateGraph
        
        This method should set up the graph structure including nodes,
        edges, and conditional logic.
        
        Returns:
            StateGraph: Compiled LangGraph StateGraph
        """
        pass
        
    @abstractmethod
    async def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the graph with the provided state
        
        This method should run the graph with the given initial state
        and return the final state after execution.
        
        Args:
            state: Initial state for the graph execution
            
        Returns:
            Dict[str, Any]: Final state after graph execution
        """
        pass
        
    @abstractmethod
    def get_prompt_manager(self):
        """
        Return the prompt manager for this graph
        
        This method should return the appropriate PromptManagerInterface
        implementation for the current phase.
        
        Returns:
            PromptManagerInterface: Prompt manager for this graph
        """
        pass
</file>

<file path="llm_graph/prompt_manager_interface.py">
#!/usr/bin/env python3
"""
Abstract Interface for Prompt Management

This module defines the abstract base class for prompt management
used in the Kubernetes volume troubleshooting system.
"""

import logging
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional

logger = logging.getLogger(__name__)

class PromptManagerInterface(ABC):
    """
    Abstract base class for prompt management
    
    Defines the interface that all PromptManager implementations must follow,
    ensuring consistency across different phases of the troubleshooting system.
    """
    
    @abstractmethod
    def get_system_prompt(self, **kwargs) -> str:
        """
        Return the phase-specific system prompt
        
        This method should return the appropriate system prompt for the current phase,
        optionally customized based on the provided kwargs.
        
        Args:
            **kwargs: Optional arguments for customizing the prompt
            
        Returns:
            str: System prompt for the current phase
        """
        pass
        
    @abstractmethod
    def format_user_query(self, query: str, **kwargs) -> str:
        """
        Format user query messages for the phase
        
        This method should format the user query according to the requirements
        of the current phase, optionally customized based on the provided kwargs.
        
        Args:
            query: User query to format
            **kwargs: Optional arguments for customizing the formatting
            
        Returns:
            str: Formatted user query
        """
        pass
        
    @abstractmethod
    def get_tool_prompt(self, **kwargs) -> str:
        """
        Return prompts for tool invocation
        
        This method should return the appropriate prompts for tool invocation
        in the current phase, optionally customized based on the provided kwargs.
        
        Args:
            **kwargs: Optional arguments for customizing the prompt
            
        Returns:
            str: Tool invocation prompt for the current phase
        """
        pass
        
    @abstractmethod
    def prepare_messages(self, system_prompt: str, user_message: str, 
                       message_list: Optional[List[Dict[str, str]]] = None) -> List[Dict[str, str]]:
        """
        Prepare message list for LLM
        
        This method should prepare the message list for the LLM based on the
        provided system prompt, user message, and optional existing message list.
        
        Args:
            system_prompt: System prompt for LLM
            user_message: User message for LLM
            message_list: Optional existing message list
            
        Returns:
            List[Dict[str, str]]: Prepared message list
        """
        pass
</file>

<file path="phases/phase_information_collection.py">
#!/usr/bin/env python3
"""
Phase 0: Information Collection for Kubernetes Volume Troubleshooting

This module contains the implementation of Phase 0 (Information Collection)
which gathers all necessary diagnostic data upfront.
"""

import logging
import time
from typing import Dict, List, Any, Optional, Tuple
from rich.console import Console
from rich.panel import Panel

from information_collector import ComprehensiveInformationCollector
from phases.utils import handle_exception

logger = logging.getLogger(__name__)

class InformationCollectionPhase:
    """
    Implementation of Phase 0: Information Collection
    
    This class handles the collection of all necessary diagnostic information
    before starting the troubleshooting process.
    """
    
    def __init__(self, config_data: Dict[str, Any]):
        """
        Initialize the Information Collection Phase
        
        Args:
            config_data: Configuration data for the system
        """
        self.config_data = config_data
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.console = Console()
        self.file_console = Console(file=open('troubleshoot.log', 'w'))
        
    async def collect_information(self, pod_name: str, namespace: str, volume_path: str) -> Dict[str, Any]:
        """
        Collect all necessary diagnostic information
        
        Args:
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod
            volume_path: Path of the volume with I/O error
            
        Returns:
            Dict[str, Any]: Pre-collected diagnostic information
        """
        self.logger.info(f"Collecting information for pod {namespace}/{pod_name}")
        
        try:
            # Initialize information collector
            info_collector = ComprehensiveInformationCollector(self.config_data)
            
            # Run comprehensive collection
            collection_result = await info_collector.comprehensive_collect(
                target_pod=pod_name,
                target_namespace=namespace,
                target_volume_path=volume_path
            )
            
            # Get the knowledge graph from collection result
            knowledge_graph = collection_result.get('knowledge_graph')
            
            # Format collected data into expected structure
            collected_info = self._format_collected_data(collection_result, knowledge_graph)
            
            self._print_knowledge_graph_summary(knowledge_graph)
            
            return collected_info
            
        except Exception as e:
            error_msg = handle_exception("collect_information", e, self.logger)
            return self._create_empty_collected_info(error_msg)
    
    def _format_collected_data(self, collection_result: Dict[str, Any], knowledge_graph: Any) -> Dict[str, Any]:
        """
        Format collected data into expected structure
        
        Args:
            collection_result: Result from comprehensive collection
            knowledge_graph: Knowledge Graph instance
            
        Returns:
            Dict[str, Any]: Formatted collected data
        """
        return {
            "pod_info": collection_result.get('collected_data', {}).get('kubernetes', {}).get('pods', {}),
            "pvc_info": collection_result.get('collected_data', {}).get('kubernetes', {}).get('pvcs', {}),
            "pv_info": collection_result.get('collected_data', {}).get('kubernetes', {}).get('pvs', {}),
            "node_info": collection_result.get('collected_data', {}).get('kubernetes', {}).get('nodes', {}),
            "csi_driver_info": collection_result.get('collected_data', {}).get('csi_baremetal', {}),
            "storage_class_info": {},  # Will be included in kubernetes data
            "system_info": collection_result.get('collected_data', {}).get('system', {}),
            "knowledge_graph_summary": collection_result.get('context_summary', {}),
            "issues": knowledge_graph.issues if knowledge_graph else [],
            "knowledge_graph": knowledge_graph
        }
    
    def _create_empty_collected_info(self, error_msg: str) -> Dict[str, Any]:
        """
        Create empty collected info structure with error message
        
        Args:
            error_msg: Error message
            
        Returns:
            Dict[str, Any]: Empty collected info structure
        """
        return {
            "collection_error": error_msg,
            "pod_info": {},
            "pvc_info": {},
            "pv_info": {},
            "node_info": {},
            "csi_driver_info": {},
            "storage_class_info": {},
            "system_info": {},
            "knowledge_graph_summary": {},
            "issues": [],
            "knowledge_graph": None
        }
    
    def _print_knowledge_graph_summary(self, knowledge_graph: Any) -> None:
        """
        Print Knowledge Graph summary with rich formatting
        
        Args:
            knowledge_graph: Knowledge Graph instance
        """
        self.console.print("\n")
        self.console.print(Panel(
            "[bold white]Building and analyzing knowledge graph...",
            title="[bold cyan]PHASE 0: INFORMATION COLLECTION - KNOWLEDGE GRAPH",
            border_style="cyan",
            padding=(1, 2)
        ))
        
        try:
            # Try to use rich formatting with proper error handling
            formatted_output = knowledge_graph.print_graph(use_rich=True)
            
            # Handle different output types
            if formatted_output is None:
                # If there was a silent success (no return value)
                self.console.print("[green]Knowledge graph built successfully[/green]")
            elif isinstance(formatted_output, str):
                # Regular string output - print as is
                print(formatted_output)
            else:
                # For any other type of output
                self.console.print("[green]Knowledge graph analysis complete[/green]")
        except Exception as e:
            # Fall back to plain text if rich formatting fails
            error_msg = handle_exception("_print_knowledge_graph_summary", e, self.logger)
            try:
                # Try plain text formatting
                formatted_output = knowledge_graph.print_graph(use_rich=False)
                print(formatted_output)
            except Exception as e2:
                # Last resort fallback
                error_msg = handle_exception("_print_knowledge_graph_summary (plain text fallback)", e2, self.logger)
                print("=" * 80)
                print("KNOWLEDGE GRAPH SUMMARY (FALLBACK FORMAT)")
                print("=" * 80)
                print(f"Total nodes: {knowledge_graph.graph.number_of_nodes()}")
                print(f"Total edges: {knowledge_graph.graph.number_of_edges()}")
                print(f"Total issues: {len(knowledge_graph.issues)}")
        
        self.console.print("\n")


async def run_information_collection_phase(pod_name: str, namespace: str, volume_path: str, config_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Run Phase 0: Information Collection - Gather all necessary data upfront
    
    Args:
        pod_name: Name of the pod with the error
        namespace: Namespace of the pod
        volume_path: Path of the volume with I/O error
        config_data: Configuration data
        
    Returns:
        Dict[str, Any]: Pre-collected diagnostic information
    """
    logging.info("Starting Phase 0: Information Collection")
    
    try:
        # Initialize the phase
        phase = InformationCollectionPhase(config_data)
        
        # Run the collection
        collected_info = await phase.collect_information(pod_name, namespace, volume_path)
        
        return collected_info
        
    except Exception as e:
        error_msg = handle_exception("run_information_collection_phase", e, logger)
        return {
            "collection_error": error_msg,
            "pod_info": {},
            "pvc_info": {},
            "pv_info": {},
            "node_info": {},
            "csi_driver_info": {},
            "storage_class_info": {},
            "system_info": {},
            "knowledge_graph_summary": {}
        }
</file>

<file path="phases/streaming_callbacks.py">
#!/usr/bin/env python3
"""
Streaming Callbacks for LLM Output

This module contains callback handlers for streaming LLM outputs
during different phases of the troubleshooting process.
"""

import sys
import logging
from typing import Dict, Any, Optional, List
from rich.console import Console
from rich.panel import Panel
from rich.live import Live
from rich.text import Text
from langchain.callbacks.base import BaseCallbackHandler

logger = logging.getLogger(__name__)

class StreamingCallbackHandler(BaseCallbackHandler):
    """
    Callback handler for streaming LLM tokens with phase-specific formatting
    
    This handler processes tokens as they are generated by the LLM and
    displays them in a formatted way based on the current phase.
    """
    
    def __init__(self, phase_name: str = None):
        """
        Initialize the streaming callback handler
        
        Args:
            phase_name: Name of the current phase (plan_phase, phase1, phase2)
        """
        self.phase_name = phase_name or "unknown"
        self.console = Console()
        self.tokens = []
        self.token_count = 0
        
        # Set up phase-specific styling
        self.phase_styles = {
            "plan_phase": {
                "title": "[bold blue]Plan Phase Streaming",
                "border_style": "blue",
                "text_style": "green"
            },
            "phase1": {
                "title": "[bold magenta]Phase 1 (Analysis) Streaming",
                "border_style": "magenta", 
                "text_style": "yellow"
            },
            "phase2": {
                "title": "[bold green]Phase 2 (Remediation) Streaming",
                "border_style": "green",
                "text_style": "cyan"
            },
            "unknown": {
                "title": "[bold white]LLM Streaming",
                "border_style": "white",
                "text_style": "white"
            }
        }
        
        # Get styling for current phase
        self.style = self.phase_styles.get(
            self.phase_name, 
            self.phase_styles["unknown"]
        )
        
        # Create initial panel
        self.panel = Panel(
            "",
            title=self.style["title"],
            border_style=self.style["border_style"],
            safe_box=True
        )
        
        # Initialize live display
        self.live = Live(
            self.panel, 
            console=self.console,
            auto_refresh=False,
            vertical_overflow="visible"
        )
        self.live.start()
        
        logger.info(f"Initialized streaming for {self.phase_name}")
    
    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """
        Process a new token from the LLM
        
        Args:
            token: The new token from the LLM
            **kwargs: Additional arguments
        """
        # Add token to list
        self.tokens.append(token)
        self.token_count += 1
        
        # Update the panel with the current text
        current_text = Text("".join(self.tokens))
        current_text.stylize(self.style["text_style"])
        
        self.panel = Panel(
            current_text,
            title=f"{self.style['title']} ({self.token_count} tokens)",
            border_style=self.style["border_style"],
            safe_box=True
        )
        
        # Update the live display
        self.live.update(self.panel)
        self.live.refresh()
        
        # Also log to file
        logger.debug(f"[{self.phase_name}] New token: {token}")
    
    def on_llm_end(self, response, **kwargs) -> None:
        """
        Handle the end of LLM generation
        
        Args:
            response: The final LLM response
            **kwargs: Additional arguments
        """
        # Complete the panel
        final_text = Text("".join(self.tokens))
        final_text.stylize(self.style["text_style"])
        
        self.panel = Panel(
            final_text,
            title=f"{self.style['title']} (Complete - {self.token_count} tokens)",
            border_style=self.style["border_style"],
            safe_box=True
        )
        
        # Update the live display one last time
        self.live.update(self.panel)
        self.live.refresh()
        
        # Stop the live display
        self.live.stop()
        
        logger.info(f"Completed streaming for {self.phase_name} ({self.token_count} tokens)")
    
    def on_llm_error(self, error: Exception, **kwargs) -> None:
        """
        Handle an error during LLM generation
        
        Args:
            error: The error that occurred
            **kwargs: Additional arguments
        """
        # Show error in the panel
        error_text = Text(f"Error during LLM generation: {str(error)}")
        error_text.stylize("bold red")
        
        self.panel = Panel(
            error_text,
            title=f"{self.style['title']} (Error)",
            border_style="red",
            safe_box=True
        )
        
        # Update the live display
        self.live.update(self.panel)
        self.live.refresh()
        
        # Stop the live display
        self.live.stop()
        
        logger.error(f"Error during streaming for {self.phase_name}: {str(error)}")
</file>

<file path="tests/mock_knowledge_graph.py">
#!/usr/bin/env python3
"""
Mock Knowledge Graph implementation for testing and demonstration purposes
"""

from typing import Dict, Any, List, Optional, Set, Tuple
import networkx as nx
import json

class MockKnowledgeGraph:
    """
    Mock implementation of the Knowledge Graph for testing and demonstration
    """
    
    def __init__(self):
        """
        Initialize the mock Knowledge Graph
        """
        self.graph = nx.DiGraph()
        self.issues = []
        self._initialize_mock_graph()
    
    def _initialize_mock_graph(self):
        """
        Initialize the mock graph with sample data
        """
        # Add nodes
        self._add_pod_node()
        self._add_pvc_node()
        self._add_pv_node()
        self._add_node_node()
        self._add_drive_node()
        self._add_volume_node()
        self._add_storage_class_node()
        self._add_system_node()
        
        # Add edges
        self._add_relationships()
        
        # Add issues
        self._add_issues()
    
    def _add_pod_node(self):
        """
        Add a Pod node to the graph
        """
        self.graph.add_node(
            "gnode:Pod:default/test-pod",
            entity_type="Pod",
            id="gnode:Pod:default/test-pod",
            name="test-pod",
            namespace="default",
            node="worker-1",
            status="Running",
            volumes=[
                {
                    "name": "test-volume",
                    "persistentVolumeClaim": {
                        "claimName": "test-pvc"
                    }
                }
            ],
            containers=[
                {
                    "name": "test-container",
                    "volumeMounts": [
                        {
                            "name": "test-volume",
                            "mountPath": "/data"
                        }
                    ]
                }
            ],
            last_error="I/O error on volume"
        )
    
    def _add_pvc_node(self):
        """
        Add a PVC node to the graph
        """
        self.graph.add_node(
            "gnode:PVC:default/test-pvc",
            entity_type="PVC",
            id="gnode:PVC:default/test-pvc",
            name="test-pvc",
            namespace="default",
            storage_class="csi-baremetal-sc",
            volume_name="test-pv",
            status="Bound",
            capacity="10Gi",
            access_modes=["ReadWriteOnce"]
        )
    
    def _add_pv_node(self):
        """
        Add a PV node to the graph
        """
        self.graph.add_node(
            "gnode:PV:test-pv",
            entity_type="PV",
            id="gnode:PV:test-pv",
            name="test-pv",
            storage_class="csi-baremetal-sc",
            capacity="10Gi",
            access_modes=["ReadWriteOnce"],
            reclaim_policy="Delete",
            status="Bound",
            csi_driver="csi-baremetal",
            volume_handle="volume-123-456",
            fs_type="xfs",
            node="worker-1"
        )
    
    def _add_node_node(self):
        """
        Add a Node node to the graph
        """
        self.graph.add_node(
            "gnode:Node:worker-1",
            entity_type="Node",
            id="gnode:Node:worker-1",
            name="worker-1",
            status="Ready",
            ip="192.168.1.10",
            capacity={
                "cpu": "8",
                "memory": "32Gi",
                "pods": "110"
            },
            conditions=[
                {
                    "type": "Ready",
                    "status": "True"
                },
                {
                    "type": "DiskPressure",
                    "status": "False"
                }
            ]
        )
    
    def _add_drive_node(self):
        """
        Add a Drive node to the graph
        """
        self.graph.add_node(
            "gnode:Drive:drive-abc-123",
            entity_type="Drive",
            id="gnode:Drive:drive-abc-123",
            uuid="drive-abc-123",
            node="worker-1",
            path="/dev/sda",
            size="100Gi",
            type="HDD",
            status="Online",
            health="Warning",
            error_log=[
                {
                    "timestamp": "2025-06-16T04:28:00Z",
                    "error": "I/O errors detected",
                    "details": "Multiple read failures recorded"
                }
            ]
        )
    
    def _add_volume_node(self):
        """
        Add a Volume node to the graph
        """
        self.graph.add_node(
            "gnode:Volume:default/volume-123-456",
            entity_type="Volume",
            id="gnode:Volume:default/volume-123-456",
            name="volume-123-456",
            namespace="default",
            pv_name="test-pv",
            node="worker-1",
            status="Available",
            health="Warning",
            drive_uuid="drive-abc-123"
        )
    
    def _add_storage_class_node(self):
        """
        Add a StorageClass node to the graph
        """
        self.graph.add_node(
            "gnode:StorageClass:csi-baremetal-sc",
            entity_type="StorageClass",
            id="gnode:StorageClass:csi-baremetal-sc",
            name="csi-baremetal-sc",
            provisioner="csi-baremetal",
            parameters={
                "storageType": "HDD",
                "fsType": "xfs"
            },
            reclaim_policy="Delete",
            volume_binding_mode="WaitForFirstConsumer"
        )
    
    def _add_system_node(self):
        """
        Add a System node to the graph
        """
        self.graph.add_node(
            "gnode:System:filesystem",
            entity_type="System",
            id="gnode:System:filesystem",
            name="filesystem",
            type="xfs",
            mount_point="/var/lib/kubelet/pods/pod-123-456/volumes/kubernetes.io~csi/test-pv/mount",
            status="Error",
            error_log=[
                {
                    "timestamp": "2025-06-16T04:28:00Z",
                    "error": "XFS metadata corruption detected",
                    "details": "XFS_CORRUPT_INODES error found during filesystem check"
                }
            ]
        )
    
    def _add_relationships(self):
        """
        Add relationships between nodes
        """
        # Pod uses PVC
        self.graph.add_edge(
            "gnode:Pod:default/test-pod",
            "gnode:PVC:default/test-pvc",
            relationship="USES",
            mount_path="/data"
        )
        
        # PVC bound to PV
        self.graph.add_edge(
            "gnode:PVC:default/test-pvc",
            "gnode:PV:test-pv",
            relationship="BOUND_TO"
        )
        
        # PV uses Volume
        self.graph.add_edge(
            "gnode:PV:test-pv",
            "gnode:Volume:default/volume-123-456",
            relationship="USES"
        )
        
        # Volume uses Drive
        self.graph.add_edge(
            "gnode:Volume:default/volume-123-456",
            "gnode:Drive:drive-abc-123",
            relationship="USES"
        )
        
        # PV uses StorageClass
        self.graph.add_edge(
            "gnode:PV:test-pv",
            "gnode:StorageClass:csi-baremetal-sc",
            relationship="USES"
        )
        
        # PV uses System (filesystem)
        self.graph.add_edge(
            "gnode:PV:test-pv",
            "gnode:System:filesystem",
            relationship="USES"
        )
        
        # Pod runs on Node
        self.graph.add_edge(
            "gnode:Pod:default/test-pod",
            "gnode:Node:worker-1",
            relationship="RUNS_ON"
        )
        
        # Drive is on Node
        self.graph.add_edge(
            "gnode:Drive:drive-abc-123",
            "gnode:Node:worker-1",
            relationship="IS_ON"
        )
    
    def _add_issues(self):
        """
        Add issues to the graph
        """
        self.issues = [
            {
                "id": "issue-001",
                "entity_id": "gnode:System:filesystem",
                "entity_type": "System",
                "severity": "critical",
                "category": "filesystem",
                "message": "XFS filesystem corruption detected on volume test-pv",
                "details": "XFS metadata corruption found during filesystem check. This can lead to I/O errors and data loss.",
                "timestamp": "2025-06-16T04:28:00Z",
                "related_entities": ["gnode:PV:test-pv", "gnode:Pod:default/test-pod"],
                "possible_causes": [
                    "Sudden power loss",
                    "Hardware failure",
                    "Kernel bugs",
                    "Improper unmounting"
                ],
                "recommended_actions": [
                    "Run xfs_repair to attempt filesystem repair",
                    "Check disk health with SMART tools",
                    "Backup data if possible before repair"
                ]
            },
            {
                "id": "issue-002",
                "entity_id": "gnode:Drive:drive-abc-123",
                "entity_type": "Drive",
                "severity": "warning",
                "category": "hardware",
                "message": "Multiple I/O errors detected on drive /dev/sda",
                "details": "The drive has reported multiple read failures which may indicate hardware degradation.",
                "timestamp": "2025-06-16T04:28:00Z",
                "related_entities": ["gnode:Volume:default/volume-123-456", "gnode:Node:worker-1"],
                "possible_causes": [
                    "Drive hardware failure",
                    "Loose connections",
                    "Controller issues"
                ],
                "recommended_actions": [
                    "Run SMART diagnostics on the drive",
                    "Check drive connections",
                    "Consider replacing the drive if errors persist"
                ]
            }
        ]
    
    # Knowledge Graph API methods
    def print_graph(self, use_rich=False):
        """
        Print the graph structure
        
        Args:
            use_rich: Whether to use rich formatting
            
        Returns:
            str: Formatted graph output
        """
        output = []
        output.append("Knowledge Graph Summary:")
        output.append(f"Total nodes: {self.graph.number_of_nodes()}")
        output.append(f"Total edges: {self.graph.number_of_edges()}")
        output.append(f"Total issues: {len(self.issues)}")
        output.append("\nNode types:")
        
        # Count node types
        node_types = {}
        for node in self.graph.nodes:
            entity_type = self.graph.nodes[node].get('entity_type')
            if entity_type not in node_types:
                node_types[entity_type] = 0
            node_types[entity_type] += 1
        
        for node_type, count in node_types.items():
            output.append(f"  - {node_type}: {count}")
        
        output.append("\nRelationship types:")
        # Count relationship types
        rel_types = {}
        for u, v, data in self.graph.edges(data=True):
            rel_type = data.get('relationship')
            if rel_type not in rel_types:
                rel_types[rel_type] = 0
            rel_types[rel_type] += 1
        
        for rel_type, count in rel_types.items():
            output.append(f"  - {rel_type}: {count}")
        
        output.append("\nIssues by severity:")
        # Count issues by severity
        severity_counts = {}
        for issue in self.issues:
            severity = issue.get('severity')
            if severity not in severity_counts:
                severity_counts[severity] = 0
            severity_counts[severity] += 1
        
        for severity, count in severity_counts.items():
            output.append(f"  - {severity}: {count}")
        
        return "\n".join(output)
    
    def get_entity_info(self, entity_type: str, id: str) -> Dict[str, Any]:
        """
        Get entity information
        
        Args:
            entity_type: Type of entity
            id: Entity ID
            
        Returns:
            Dict[str, Any]: Entity information
        """
        if id in self.graph.nodes:
            return dict(self.graph.nodes[id])
        return {}
    
    def get_related_entities(self, entity_type: str, id: str) -> List[Dict[str, Any]]:
        """
        Get related entities
        
        Args:
            entity_type: Type of entity
            id: Entity ID
            
        Returns:
            List[Dict[str, Any]]: Related entities
        """
        related = []
        
        # Outgoing edges
        for _, v, data in self.graph.out_edges(id, data=True):
            related.append({
                "entity_id": v,
                "entity_type": self.graph.nodes[v].get('entity_type'),
                "relationship": data.get('relationship'),
                "direction": "outgoing"
            })
        
        # Incoming edges
        for u, _, data in self.graph.in_edges(id, data=True):
            related.append({
                "entity_id": u,
                "entity_type": self.graph.nodes[u].get('entity_type'),
                "relationship": data.get('relationship'),
                "direction": "incoming"
            })
        
        return related
    
    def get_all_issues(self, severity: str = None) -> List[Dict[str, Any]]:
        """
        Get all issues
        
        Args:
            severity: Optional severity filter
            
        Returns:
            List[Dict[str, Any]]: Issues
        """
        if severity:
            return [issue for issue in self.issues if issue.get('severity') == severity]
        return self.issues
    
    def find_path(self, source_entity_type: str, source_id: str, target_entity_type: str, target_id: str) -> List[Dict[str, Any]]:
        """
        Find path between entities
        
        Args:
            source_entity_type: Source entity type
            source_id: Source entity ID
            target_entity_type: Target entity type
            target_id: Target entity ID
            
        Returns:
            List[Dict[str, Any]]: Path between entities
        """
        # If target_id is wildcard, find all entities of target_entity_type
        if target_id == "*":
            target_nodes = [
                node for node in self.graph.nodes 
                if self.graph.nodes[node].get('entity_type') == target_entity_type
            ]
            
            # Find shortest path to each target
            paths = []
            for target in target_nodes:
                try:
                    path = nx.shortest_path(self.graph, source=source_id, target=target)
                    path_info = []
                    
                    # Convert path to path info
                    for i in range(len(path) - 1):
                        u = path[i]
                        v = path[i + 1]
                        edge_data = self.graph.get_edge_data(u, v)
                        
                        path_info.append({
                            "source": u,
                            "source_type": self.graph.nodes[u].get('entity_type'),
                            "target": v,
                            "target_type": self.graph.nodes[v].get('entity_type'),
                            "relationship": edge_data.get('relationship')
                        })
                    
                    paths.append(path_info)
                except nx.NetworkXNoPath:
                    # No path found
                    pass
            
            return paths
        else:
            # Find shortest path between source and target
            try:
                path = nx.shortest_path(self.graph, source=source_id, target=target_id)
                path_info = []
                
                # Convert path to path info
                for i in range(len(path) - 1):
                    u = path[i]
                    v = path[i + 1]
                    edge_data = self.graph.get_edge_data(u, v)
                    
                    path_info.append({
                        "source": u,
                        "source_type": self.graph.nodes[u].get('entity_type'),
                        "target": v,
                        "target_type": self.graph.nodes[v].get('entity_type'),
                        "relationship": edge_data.get('relationship')
                    })
                
                return path_info
            except nx.NetworkXNoPath:
                # No path found
                return []
    
    def list_entity_types(self) -> Dict[str, int]:
        """
        List entity types and counts
        
        Returns:
            Dict[str, int]: Entity types and counts
        """
        entity_types = {}
        for node in self.graph.nodes:
            entity_type = self.graph.nodes[node].get('entity_type')
            if entity_type not in entity_types:
                entity_types[entity_type] = 0
            entity_types[entity_type] += 1
        
        return entity_types
    
    def list_entities(self, entity_type: str) -> List[str]:
        """
        List entities of a specific type
        
        Args:
            entity_type: Entity type
            
        Returns:
            List[str]: Entity IDs
        """
        return [
            node for node in self.graph.nodes 
            if self.graph.nodes[node].get('entity_type') == entity_type
        ]
    
    def list_relationship_types(self) -> Dict[str, int]:
        """
        List relationship types and counts
        
        Returns:
            Dict[str, int]: Relationship types and counts
        """
        rel_types = {}
        for u, v, data in self.graph.edges(data=True):
            rel_type = data.get('relationship')
            if rel_type not in rel_types:
                rel_types[rel_type] = 0
            rel_types[rel_type] += 1
        
        return rel_types
    
    def analyze_issues(self) -> Dict[str, Any]:
        """
        Analyze issues and provide insights
        
        Returns:
            Dict[str, Any]: Issue analysis
        """
        return {
            "root_cause": {
                "entity_id": "gnode:System:filesystem",
                "entity_type": "System",
                "issue_id": "issue-001",
                "message": "XFS filesystem corruption detected on volume test-pv",
                "confidence": 0.9
            },
            "contributing_factors": [
                {
                    "entity_id": "gnode:Drive:drive-abc-123",
                    "entity_type": "Drive",
                    "issue_id": "issue-002",
                    "message": "Multiple I/O errors detected on drive /dev/sda",
                    "confidence": 0.7
                }
            ],
            "impact": {
                "affected_entities": [
                    "gnode:Pod:default/test-pod",
                    "gnode:PV:test-pv",
                    "gnode:Volume:default/volume-123-456"
                ],
                "severity": "critical",
                "description": "The filesystem corruption is causing I/O errors in the pod, preventing normal operation."
            },
            "recommendations": [
                {
                    "action": "Run xfs_repair to fix filesystem corruption",
                    "priority": "high",
                    "details": "Use xfs_repair -L on the affected volume to attempt repair of the filesystem."
                },
                {
                    "action": "Check drive health",
                    "priority": "medium",
                    "details": "Run SMART diagnostics on the drive to check for hardware issues."
                },
                {
                    "action": "Consider replacing the drive",
                    "priority": "low",
                    "details": "If errors persist after repair, consider replacing the drive."
                }
            ]
        }

def create_mock_knowledge_graph():
    """
    Create a mock Knowledge Graph instance
    
    Returns:
        MockKnowledgeGraph: Mock Knowledge Graph instance
    """
    return MockKnowledgeGraph()
</file>

<file path="tests/mock_kubernetes_data.py">
#!/usr/bin/env python3
"""
Mock Kubernetes data for testing and demonstration purposes
"""

from typing import Dict, Any

# Mock Kubernetes data
MOCK_KUBERNETES_DATA = {
    "pods": {
        "default/test-pod": {
            "apiVersion": "v1",
            "kind": "Pod",
            "metadata": {
                "name": "test-pod",
                "namespace": "default",
                "uid": "pod-123-456",
                "labels": {
                    "app": "test-app"
                },
                "annotations": {
                    "kubernetes.io/created-by": "test-user"
                }
            },
            "spec": {
                "containers": [
                    {
                        "name": "test-container",
                        "image": "nginx:latest",
                        "ports": [
                            {
                                "containerPort": 80,
                                "protocol": "TCP"
                            }
                        ],
                        "volumeMounts": [
                            {
                                "name": "test-volume",
                                "mountPath": "/data"
                            }
                        ],
                        "resources": {
                            "limits": {
                                "cpu": "500m",
                                "memory": "512Mi"
                            },
                            "requests": {
                                "cpu": "250m",
                                "memory": "256Mi"
                            }
                        }
                    }
                ],
                "volumes": [
                    {
                        "name": "test-volume",
                        "persistentVolumeClaim": {
                            "claimName": "test-pvc"
                        }
                    }
                ],
                "nodeName": "worker-1"
            },
            "status": {
                "phase": "Running",
                "conditions": [
                    {
                        "type": "Ready",
                        "status": "True",
                        "lastProbeTime": None,
                        "lastTransitionTime": "2025-06-16T04:00:00Z",
                        "reason": "ContainersReady",
                        "message": "All containers are ready"
                    }
                ],
                "containerStatuses": [
                    {
                        "name": "test-container",
                        "state": {
                            "running": {
                                "startedAt": "2025-06-16T04:00:00Z"
                            }
                        },
                        "lastState": {
                            "terminated": {
                                "exitCode": 1,
                                "reason": "Error",
                                "message": "I/O error on volume",
                                "startedAt": "2025-06-16T03:50:00Z",
                                "finishedAt": "2025-06-16T03:55:00Z"
                            }
                        },
                        "ready": True,
                        "restartCount": 1,
                        "image": "nginx:latest",
                        "imageID": "docker-pullable://nginx@sha256:abcdef123456",
                        "containerID": "containerd://container-123-456"
                    }
                ],
                "qosClass": "Burstable"
            }
        }
    },
    "pvcs": {
        "default/test-pvc": {
            "apiVersion": "v1",
            "kind": "PersistentVolumeClaim",
            "metadata": {
                "name": "test-pvc",
                "namespace": "default",
                "uid": "pvc-123-456",
                "annotations": {
                    "pv.kubernetes.io/bind-completed": "yes",
                    "pv.kubernetes.io/bound-by-controller": "yes",
                    "volume.beta.kubernetes.io/storage-provisioner": "csi-baremetal"
                }
            },
            "spec": {
                "accessModes": [
                    "ReadWriteOnce"
                ],
                "resources": {
                    "requests": {
                        "storage": "10Gi"
                    }
                },
                "volumeName": "test-pv",
                "storageClassName": "csi-baremetal-sc",
                "volumeMode": "Filesystem"
            },
            "status": {
                "phase": "Bound",
                "accessModes": [
                    "ReadWriteOnce"
                ],
                "capacity": {
                    "storage": "10Gi"
                }
            }
        }
    },
    "pvs": {
        "test-pv": {
            "apiVersion": "v1",
            "kind": "PersistentVolume",
            "metadata": {
                "name": "test-pv",
                "uid": "pv-123-456",
                "annotations": {
                    "pv.kubernetes.io/provisioned-by": "csi-baremetal"
                }
            },
            "spec": {
                "capacity": {
                    "storage": "10Gi"
                },
                "accessModes": [
                    "ReadWriteOnce"
                ],
                "persistentVolumeReclaimPolicy": "Delete",
                "storageClassName": "csi-baremetal-sc",
                "csi": {
                    "driver": "csi-baremetal",
                    "volumeHandle": "volume-123-456",
                    "fsType": "xfs",
                    "volumeAttributes": {
                        "storage": "HDD",
                        "node": "worker-1"
                    }
                },
                "nodeAffinity": {
                    "required": {
                        "nodeSelectorTerms": [
                            {
                                "matchExpressions": [
                                    {
                                        "key": "kubernetes.io/hostname",
                                        "operator": "In",
                                        "values": [
                                            "worker-1"
                                        ]
                                    }
                                ]
                            }
                        ]
                    }
                }
            },
            "status": {
                "phase": "Bound"
            }
        }
    },
    "nodes": {
        "worker-1": {
            "apiVersion": "v1",
            "kind": "Node",
            "metadata": {
                "name": "worker-1",
                "uid": "node-123-456",
                "labels": {
                    "beta.kubernetes.io/arch": "amd64",
                    "beta.kubernetes.io/os": "linux",
                    "kubernetes.io/arch": "amd64",
                    "kubernetes.io/hostname": "worker-1",
                    "kubernetes.io/os": "linux",
                    "node-role.kubernetes.io/worker": ""
                }
            },
            "spec": {
                "podCIDR": "10.244.1.0/24",
                "taints": []
            },
            "status": {
                "capacity": {
                    "cpu": "8",
                    "ephemeral-storage": "102834Mi",
                    "hugepages-1Gi": "0",
                    "hugepages-2Mi": "0",
                    "memory": "32Gi",
                    "pods": "110"
                },
                "allocatable": {
                    "cpu": "7800m",
                    "ephemeral-storage": "94822323329",
                    "hugepages-1Gi": "0",
                    "hugepages-2Mi": "0",
                    "memory": "31Gi",
                    "pods": "110"
                },
                "conditions": [
                    {
                        "type": "Ready",
                        "status": "True",
                        "lastHeartbeatTime": "2025-06-16T04:30:00Z",
                        "lastTransitionTime": "2025-06-16T00:00:00Z",
                        "reason": "KubeletReady",
                        "message": "kubelet is posting ready status"
                    },
                    {
                        "type": "DiskPressure",
                        "status": "False",
                        "lastHeartbeatTime": "2025-06-16T04:30:00Z",
                        "lastTransitionTime": "2025-06-16T00:00:00Z",
                        "reason": "KubeletHasSufficientDisk",
                        "message": "kubelet has sufficient disk space available"
                    },
                    {
                        "type": "MemoryPressure",
                        "status": "False",
                        "lastHeartbeatTime": "2025-06-16T04:30:00Z",
                        "lastTransitionTime": "2025-06-16T00:00:00Z",
                        "reason": "KubeletHasSufficientMemory",
                        "message": "kubelet has sufficient memory available"
                    },
                    {
                        "type": "PIDPressure",
                        "status": "False",
                        "lastHeartbeatTime": "2025-06-16T04:30:00Z",
                        "lastTransitionTime": "2025-06-16T00:00:00Z",
                        "reason": "KubeletHasSufficientPID",
                        "message": "kubelet has sufficient PID available"
                    },
                    {
                        "type": "NetworkUnavailable",
                        "status": "False",
                        "lastHeartbeatTime": "2025-06-16T04:30:00Z",
                        "lastTransitionTime": "2025-06-16T00:00:00Z",
                        "reason": "RouteCreated",
                        "message": "RouteController created a route"
                    }
                ],
                "addresses": [
                    {
                        "type": "InternalIP",
                        "address": "192.168.1.10"
                    },
                    {
                        "type": "Hostname",
                        "address": "worker-1"
                    }
                ],
                "nodeInfo": {
                    "machineID": "abc123def456",
                    "systemUUID": "abc123def456",
                    "bootID": "abc123def456",
                    "kernelVersion": "5.15.0-1.el9",
                    "osImage": "CentOS Stream 9",
                    "containerRuntimeVersion": "containerd://1.6.0",
                    "kubeletVersion": "v1.26.0",
                    "kubeProxyVersion": "v1.26.0",
                    "operatingSystem": "linux",
                    "architecture": "amd64"
                }
            }
        }
    },
    "storage_classes": {
        "csi-baremetal-sc": {
            "apiVersion": "storage.k8s.io/v1",
            "kind": "StorageClass",
            "metadata": {
                "name": "csi-baremetal-sc",
                "annotations": {
                    "storageclass.kubernetes.io/is-default-class": "true"
                }
            },
            "provisioner": "csi-baremetal",
            "parameters": {
                "storageType": "HDD",
                "fsType": "xfs"
            },
            "reclaimPolicy": "Delete",
            "volumeBindingMode": "WaitForFirstConsumer",
            "allowVolumeExpansion": True
        }
    },
    "csi_driver": {
        "name": "csi-baremetal",
        "version": "1.0.0",
        "nodeID": "worker-1",
        "topology": {
            "segments": {
                "topology.csi-baremetal/node": "worker-1"
            }
        },
        "volumes": {
            "volume-123-456": {
                "id": "volume-123-456",
                "name": "volume-123-456",
                "capacity": "10Gi",
                "status": "published",
                "published_node": "worker-1",
                "storage_type": "HDD",
                "fs_type": "xfs",
                "health": "warning",
                "drive_uuid": "drive-abc-123",
                "error_log": [
                    {
                        "timestamp": "2025-06-16T04:28:00Z",
                        "error": "I/O error detected",
                        "details": "Error accessing filesystem metadata"
                    }
                ]
            }
        },
        "drives": {
            "drive-abc-123": {
                "uuid": "drive-abc-123",
                "serial": "ABC123DEF456",
                "size": "100Gi",
                "node": "worker-1",
                "type": "HDD",
                "path": "/dev/sda",
                "status": "online",
                "health": "warning",
                "error_log": [
                    {
                        "timestamp": "2025-06-16T04:28:00Z",
                        "error": "Multiple read failures detected",
                        "details": "5 uncorrectable errors reported by SMART"
                    }
                ]
            }
        }
    }
}

# Function to get mock Kubernetes data
def get_mock_kubernetes_data() -> Dict[str, Any]:
    """
    Get mock Kubernetes data for testing and demonstration
    
    Returns:
        Dict[str, Any]: Mock Kubernetes data
    """
    return MOCK_KUBERNETES_DATA
</file>

<file path="tests/mock_system_data.py">
#!/usr/bin/env python3
"""
Mock system data for testing and demonstration purposes
"""

from typing import Dict, Any, List

# Mock system information
MOCK_SYSTEM_INFO = {
    "kernel": {
        "version": "5.15.0-1.el9",
        "arch": "x86_64",
        "modules": [
            {"name": "xfs", "version": "5.15.0", "status": "loaded"},
            {"name": "dm_thin_pool", "version": "5.15.0", "status": "loaded"},
            {"name": "dm_persistent_data", "version": "5.15.0", "status": "loaded"},
            {"name": "dm_bio_prison", "version": "5.15.0", "status": "loaded"},
            {"name": "dm_mod", "version": "5.15.0", "status": "loaded"}
        ],
        "parameters": {
            "fs.file-max": "9223372036854775807",
            "fs.inotify.max_user_watches": "1048576",
            "vm.max_map_count": "262144"
        }
    },
    "os": {
        "name": "CentOS Stream",
        "version": "9",
        "id": "centos",
        "id_like": "rhel fedora",
        "pretty_name": "CentOS Stream 9"
    },
    "hardware": {
        "cpu": {
            "model": "Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz",
            "cores": 8,
            "threads": 16
        },
        "memory": {
            "total": "32GB",
            "free": "12GB",
            "used": "20GB",
            "swap_total": "8GB",
            "swap_free": "8GB"
        },
        "disks": [
            {
                "device": "/dev/sda",
                "size": "100GB",
                "type": "HDD",
                "model": "HGST HUS726020ALA610",
                "serial": "ABC123DEF456",
                "smart_status": "WARN",
                "smart_attributes": [
                    {
                        "id": 5,
                        "name": "Reallocated_Sector_Ct",
                        "value": 100,
                        "worst": 100,
                        "threshold": 10,
                        "status": "OK"
                    },
                    {
                        "id": 187,
                        "name": "Reported_Uncorrect",
                        "value": 95,
                        "worst": 95,
                        "threshold": 0,
                        "status": "WARN"
                    },
                    {
                        "id": 197,
                        "name": "Current_Pending_Sector",
                        "value": 98,
                        "worst": 98,
                        "threshold": 0,
                        "status": "WARN"
                    },
                    {
                        "id": 198,
                        "name": "Offline_Uncorrectable",
                        "value": 100,
                        "worst": 100,
                        "threshold": 0,
                        "status": "OK"
                    }
                ],
                "partitions": [
                    {
                        "name": "/dev/sda1",
                        "size": "1GB",
                        "type": "EFI System",
                        "mountpoint": "/boot/efi"
                    },
                    {
                        "name": "/dev/sda2",
                        "size": "99GB",
                        "type": "Linux LVM",
                        "mountpoint": None
                    }
                ]
            }
        ],
        "filesystems": [
            {
                "device": "/dev/mapper/vg0-root",
                "mountpoint": "/",
                "type": "xfs",
                "size": "50GB",
                "used": "30GB",
                "available": "20GB",
                "use_percent": 60
            },
            {
                "device": "/dev/mapper/vg0-var",
                "mountpoint": "/var",
                "type": "xfs",
                "size": "20GB",
                "used": "10GB",
                "available": "10GB",
                "use_percent": 50
            },
            {
                "device": "/dev/mapper/vg0-home",
                "mountpoint": "/home",
                "type": "xfs",
                "size": "10GB",
                "used": "2GB",
                "available": "8GB",
                "use_percent": 20
            },
            {
                "device": "/dev/sda1",
                "mountpoint": "/boot/efi",
                "type": "vfat",
                "size": "1GB",
                "used": "0.2GB",
                "available": "0.8GB",
                "use_percent": 20
            }
        ]
    },
    "network": {
        "interfaces": [
            {
                "name": "eth0",
                "mac": "00:11:22:33:44:55",
                "ip": "192.168.1.10",
                "netmask": "255.255.255.0",
                "gateway": "192.168.1.1",
                "status": "up",
                "speed": "1000Mb/s",
                "mtu": 1500
            }
        ],
        "hostname": "worker-1",
        "dns_servers": ["8.8.8.8", "8.8.4.4"],
        "routes": [
            {
                "destination": "0.0.0.0/0",
                "gateway": "192.168.1.1",
                "interface": "eth0"
            },
            {
                "destination": "192.168.1.0/24",
                "gateway": "0.0.0.0",
                "interface": "eth0"
            }
        ]
    },
    "processes": {
        "total": 234,
        "running": 2,
        "sleeping": 232,
        "stopped": 0,
        "zombie": 0,
        "top": [
            {
                "pid": 1,
                "user": "root",
                "command": "/usr/lib/systemd/systemd --system --deserialize 35",
                "cpu_percent": 0.0,
                "memory_percent": 0.1
            },
            {
                "pid": 1234,
                "user": "root",
                "command": "/usr/bin/kubelet --config=/var/lib/kubelet/config.yaml",
                "cpu_percent": 2.5,
                "memory_percent": 3.2
            },
            {
                "pid": 2345,
                "user": "root",
                "command": "/usr/bin/containerd",
                "cpu_percent": 1.8,
                "memory_percent": 2.5
            }
        ]
    },
    "logs": {
        "kernel": [
            {
                "timestamp": "2025-06-16T04:28:00Z",
                "level": "err",
                "message": "XFS (dm-3): Metadata corruption detected at xfs_inode_buf_verify+0x89/0x1c0 [xfs], xfs_inode block 0x1234"
            },
            {
                "timestamp": "2025-06-16T04:28:01Z",
                "level": "err",
                "message": "XFS (dm-3): Unmount and run xfs_repair"
            },
            {
                "timestamp": "2025-06-16T04:28:02Z",
                "level": "err",
                "message": "XFS (dm-3): First 64 bytes of corrupted metadata buffer:"
            },
            {
                "timestamp": "2025-06-16T04:28:10Z",
                "level": "err",
                "message": "Buffer I/O error on dev dm-3, logical block 1234, async page read"
            }
        ],
        "system": [
            {
                "timestamp": "2025-06-16T04:29:00Z",
                "service": "kubelet",
                "level": "error",
                "message": "Error syncing pod default/test-pod: I/O error on volume mount"
            },
            {
                "timestamp": "2025-06-16T04:29:05Z",
                "service": "containerd",
                "level": "error",
                "message": "Container test-container exited with error: I/O error on volume"
            }
        ]
    },
    "volume_diagnostics": {
        "mount_info": {
            "device": "/dev/mapper/volume-123-456",
            "mountpoint": "/var/lib/kubelet/pods/pod-123-456/volumes/kubernetes.io~csi/test-pv/mount",
            "type": "xfs",
            "options": "rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,noquota"
        },
        "xfs_info": {
            "block_size": 4096,
            "data_blocks": 2621440,
            "imaxpct": 25,
            "log_blocks": 2560,
            "naming": "version 2",
            "uuid": "12345678-abcd-1234-efgh-123456789abc"
        },
        "xfs_repair_check": {
            "status": "error",
            "errors_found": [
                "Inode 1234 has corrupt core.mode",
                "Inode 5678 has corrupt core.size",
                "Filesystem has corrupt metadata"
            ],
            "repair_recommended": True
        },
        "io_stats": {
            "read_ops": 12345,
            "write_ops": 23456,
            "read_bytes": 123456789,
            "write_bytes": 234567890,
            "read_time_ms": 45678,
            "write_time_ms": 56789,
            "io_time_ms": 78901,
            "io_in_progress": 0,
            "errors": 123
        }
    }
}

# Function to get mock system data
def get_mock_system_data() -> Dict[str, Any]:
    """
    Get mock system data for testing and demonstration
    
    Returns:
        Dict[str, Any]: Mock system data
    """
    return MOCK_SYSTEM_INFO
</file>

<file path="tests/test_hardware_system_entity.py">
#!/usr/bin/env python3
"""
Test script to demonstrate adding a system hardware entity to Knowledge Graph in phase0.

This script shows how to collect system hardware information and add it to the
Knowledge Graph, including calling tools like get_system_hardware_info() and other
system diagnostic tools.
"""

import asyncio
import logging
from knowledge_graph.knowledge_graph import KnowledgeGraph
from information_collector.knowledge_builder import KnowledgeBuilder
from information_collector.base import InformationCollectorBase
import json

# Configure logging
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TestCollector(InformationCollectorBase, KnowledgeBuilder):
    """Test implementation of the Information Collector with Knowledge Builder capabilities"""
    
    def __init__(self, config=None):
        """Initialize the test collector with minimal configuration"""
        self.knowledge_graph = KnowledgeGraph()
        self.collected_data = {
            'node_names': ['localhost'],  # Use localhost for testing
            'errors': []
        }
        self.config = config or {}

async def main():
    """Main test function to demonstrate adding a system hardware entity to the Knowledge Graph"""
    logger.info("Starting test for adding system hardware entity to Knowledge Graph...")
    
    # Create test collector
    collector = TestCollector()
    
    # Build the knowledge graph
    # This will call _add_system_entities() which now includes _add_hardware_system_entity()
    kg = await collector._build_knowledge_graph_from_tools()
    
    # Print summary of the knowledge graph
    summary = kg.get_summary()
    logger.info(f"Knowledge Graph built: {summary['total_nodes']} nodes, "
               f"{summary['total_edges']} edges, {summary['total_issues']} issues")
    
    # Check if hardware system entity exists
    system_nodes = kg.find_nodes_by_type('System')
    hardware_nodes = [node for node in system_nodes 
                     if kg.graph.nodes[node].get('name') == 'hardware']
    
    if hardware_nodes:
        logger.info("Hardware system entity successfully added to Knowledge Graph!")
        
        # Print hardware entity details
        hardware_id = hardware_nodes[0]
        hardware_data = kg.graph.nodes[hardware_id]
        
        logger.info(f"Hardware system entity ID: {hardware_id}")
        logger.info(f"Hardware system entity description: {hardware_data.get('description')}")
        
        # Get issues related to hardware
        issues = [issue for issue in kg.issues if issue['node_id'] == hardware_id]
        logger.info(f"Found {len(issues)} hardware-related issues")
        
        # Print issues
        for i, issue in enumerate(issues):
            logger.info(f"Issue {i+1}: {issue['severity']} - {issue['description']}")
        
        # Print connections to nodes
        connected_nodes = kg.find_connected_nodes(hardware_id)
        logger.info(f"Hardware entity is connected to {len(connected_nodes)} other nodes")
    else:
        logger.error("Hardware system entity was not added to Knowledge Graph!")
    
    # Print the formatted knowledge graph
    print(kg.print_graph())
    
    return kg

if __name__ == "__main__":
    # Run the test
    kg = asyncio.run(main())
    
    # Save knowledge graph
    kg_summary = {
        "entities": {entity_type: len(kg.find_nodes_by_type(entity_type))
                   for entity_type in ['System', 'Node', 'Drive', 'Pod', 'PVC', 'PV']},
        "relationships": kg.graph.number_of_edges(),
        "issues": len(kg.issues)
    }
    
    # Save summary as JSON
    with open('hardware_entity_test_results.json', 'w') as f:
        json.dump(kg_summary, f, indent=2)
    
    print("Test completed. Results saved to hardware_entity_test_results.json")
</file>

<file path="tests/test_llm_providers.py">
#!/usr/bin/env python3
"""
Test script for verifying multiple LLM providers

This script tests the LLMFactory with different providers (OpenAI, Google, Ollama)
to ensure that configuration switching works properly.
"""

import os
import sys
import logging
import argparse
from typing import Dict, Any

# Add parent directory to path to import project modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from phases.llm_factory import LLMFactory
from langchain_core.messages import SystemMessage, HumanMessage

# Setup logging
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_test_config(provider: str) -> Dict[str, Any]:
    """
    Create a test configuration for the specified provider
    
    Args:
        provider: Provider name ('openai', 'google', or 'ollama')
        
    Returns:
        Dict[str, Any]: Test configuration
    """
    # Base configuration
    config = {
        'llm': {
            'provider': provider,
            # Legacy config for backward compatibility testing
            'model': 'gpt-4',
            'api_key': 'test-key',
            'api_endpoint': 'https://api.example.com/v1',
            'temperature': 0.1,
            'max_tokens': 1000,
            
            # Provider-specific configurations
            'openai': {
                'model': 'gpt-4o',
                'api_key': os.environ.get('OPENAI_API_KEY', 'sk-openai-test-key'),
                'api_endpoint': os.environ.get('OPENAI_API_BASE', 'https://api.openai.com/v1'),
                'temperature': 0.1,
                'max_tokens': 1000
            },
            'google': {
                'model': 'gemini-2.5-pro',
                'api_key': os.environ.get('GOOGLE_API_KEY', 'google-test-key'),
                'temperature': 0.1,
                'max_tokens': 1000
            },
            'ollama': {
                'model': 'llama3',
                'base_url': os.environ.get('OLLAMA_BASE_URL', 'http://localhost:11434'),
                'temperature': 0.1,
                'max_tokens': 1000
            }
        }
    }
    
    return config

def test_llm_provider(provider: str, perform_call: bool = False) -> None:
    """
    Test a specific LLM provider
    
    Args:
        provider: Provider name ('openai', 'google', or 'ollama')
        perform_call: Whether to actually perform an API call
    """
    logger.info(f"Testing {provider.upper()} LLM provider...")
    
    # Get test configuration
    config = get_test_config(provider)
    
    try:
        # Create LLM using factory
        factory = LLMFactory(config)
        llm = factory.create_llm()
        
        # Check if LLM was created
        if llm is None:
            logger.error(f"Failed to create {provider} LLM")
            return
            
        logger.info(f"Successfully created {provider} LLM instance: {llm.__class__.__name__}")
        
        # Perform a test call if requested
        if perform_call:
            logger.info(f"Performing test call to {provider} API...")
            
            # Prepare test messages
            messages = [
                SystemMessage(content="You are a helpful assistant."),
                HumanMessage(content="Say 'Hello from LLM Provider test!'")
            ]
            
            # Perform the call
            response = llm.invoke(messages)
            
            # Log the response
            logger.info(f"Response: {response.content}")
            
            logger.info(f"Successfully tested {provider} API call")
    
    except Exception as e:
        logger.error(f"Error testing {provider} LLM: {str(e)}")

def main():
    """Main function to run the test script"""
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Test LLM providers')
    parser.add_argument('--provider', type=str, choices=['all', 'openai', 'google', 'ollama'],
                      default='all', help='LLM provider to test')
    parser.add_argument('--call', action='store_true', 
                      help='Perform actual API calls (requires valid API keys)')
    args = parser.parse_args()
    
    # Test the specified provider(s)
    if args.provider == 'all':
        test_llm_provider('openai', args.call)
        test_llm_provider('google', args.call)
        test_llm_provider('ollama', args.call)
    else:
        test_llm_provider(args.provider, args.call)

if __name__ == "__main__":
    main()
</file>

<file path="tools/core/config.py">
#!/usr/bin/env python3
"""
Core configuration and utility functions for the troubleshooting tools.

This module contains global configuration management, command validation,
and execution utilities used across all tool categories.
"""

import logging
import subprocess
from typing import Dict, List, Any, Optional, Tuple

# Global variables
INTERACTIVE_MODE = False  # To be set by the caller
CONFIG_DATA = None  # To be set by the caller with configuration

def validate_command(command_list: List[str], config_data: Dict[str, Any] = None) -> Tuple[bool, str]:
    """
    Validate command against allowed/disallowed patterns in configuration
    
    Args:
        command_list: Command to validate as list of strings
        config_data: Configuration data containing command restrictions
        
    Returns:
        Tuple[bool, str]: (is_allowed, reason)
    """
    if not command_list:
        return False, "Empty command list"
    
    if config_data is None:
        config_data = CONFIG_DATA
    
    if config_data is None:
        return True, "No configuration available - allowing command"
    
    command_str = ' '.join(command_list)
    commands_config = config_data.get('commands', {})
    
    # Check disallowed commands first (higher priority)
    disallowed = commands_config.get('disallowed', [])
    for pattern in disallowed:
        if _matches_pattern(command_str, pattern):
            return False, f"Command matches disallowed pattern: {pattern}"
    
    # Check allowed commands
    allowed = commands_config.get('allowed', [])
    if allowed:  # If allowed list exists, command must match one of them
        for pattern in allowed:
            if _matches_pattern(command_str, pattern):
                return True, f"Command matches allowed pattern: {pattern}"
        return False, "Command does not match any allowed pattern"
    
    # If no allowed list, allow by default (only disallowed list matters)
    return True, "No allowed list specified - command permitted"

def _matches_pattern(command: str, pattern: str) -> bool:
    """
    Check if command matches a pattern (supports wildcards)
    
    Args:
        command: Full command string
        pattern: Pattern to match against (supports * wildcard)
        
    Returns:
        bool: True if command matches pattern
    """
    import fnmatch
    return fnmatch.fnmatch(command, pattern)

def execute_command(command_list: List[str], purpose: str = "none", requires_approval: bool = True) -> str:
    """
    Execute a command and return its output
    
    Args:
        command_list: Command to execute as a list of strings
        purpose: Purpose of the command
        requires_approval: Whether this command requires user approval in interactive mode
        
    Returns:
        str: Command output
    """
    global INTERACTIVE_MODE
    
    if not command_list:
        logging.error("execute_command received an empty command_list")
        return "Error: Empty command list provided"

    executable = command_list[0]
    command_display_str = ' '.join(command_list)
    
    # Execute command
    try:
        logging.info(f"Executing command: {command_display_str}")
        result = subprocess.run(command_list, shell=False, check=True, 
                               stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                               universal_newlines=True)
        output = result.stdout
        logging.debug(f"Command output: {output}")
        return output
    except subprocess.CalledProcessError as e:
        error_msg = f"Command failed with exit code {e.returncode}: {e.stderr}"
        logging.error(error_msg)
        return f"Error: {error_msg}"
    except FileNotFoundError:
        error_msg = f"Command not found: {executable}"
        logging.error(error_msg)
        return f"Error: {error_msg}"
    except Exception as e:
        error_msg = f"Failed to execute command {command_display_str}: {str(e)}"
        logging.error(error_msg)
        return f"Error: {error_msg}"
</file>

<file path="tools/diagnostics/hardware.py">
#!/usr/bin/env python3
"""
Hardware diagnostic tools for volume troubleshooting.

This module contains tools for hardware-level diagnostics including
disk health checks, performance testing, and file system validation.
"""

import time
import json
import subprocess
import re
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from langchain_core.tools import tool

@tool
def smartctl_check(node_name: str, device_path: str) -> str:
    """
    Check disk health using smartctl via SSH
    
    Args:
        node_name: Node hostname or IP
        device_path: Device path (e.g., /dev/sda)
        
    Returns:
        str: SMART data showing disk health, reallocated sectors, etc.
    """
    cmd = f"sudo smartctl -a {device_path}"
    return ssh_execute.invoke({"node_name": node_name, "command": cmd})

@tool
def fio_performance_test(node_name: str, device_path: str, test_type: str = "read") -> str:
    """
    Test disk performance using fio via SSH
    
    Args:
        node_name: Node hostname or IP
        device_path: Device path (e.g., /dev/sda)
        test_type: Test type (read, write, randread, randwrite)
        
    Returns:
        str: Performance test results showing IOPS and throughput
    """
    cmd = f"sudo fio --name={test_type}_test --filename={device_path} --rw={test_type} --bs=4k --size=100M --numjobs=1 --iodepth=1 --runtime=60 --time_based --group_reporting"
    return ssh_execute.invoke({"node_name": node_name, "command": cmd})

@tool
def fsck_check(node_name: str, device_path: str, check_only: bool = True) -> str:
    """
    Check file system integrity using fsck via SSH
    
    Args:
        node_name: Node hostname or IP
        device_path: Device path (e.g., /dev/sda1)
        check_only: If True, only check without fixing (safer)
        
    Returns:
        str: File system check results
    """
    if check_only:
        cmd = f"sudo fsck -n {device_path}"  # -n flag means no changes, check only
    else:
        cmd = f"sudo fsck -y {device_path}"  # -y flag means auto-fix (requires approval)
    
    return ssh_execute.invoke({"node_name": node_name, "command": cmd})

@tool
def xfs_repair_check(node_name: str, device_path: str) -> str:
    """
    Check XFS file system integrity using xfs_repair via SSH
    
    Args:
        node_name: Node hostname or IP
        device_path: Device path (e.g., /dev/sda1)
        
    Returns:
        str: XFS file system check results
    """
    cmd = f"sudo xfs_repair -n {device_path}"  # -n flag means no changes, check only
    return ssh_execute.invoke({"node_name": node_name, "command": cmd})

@tool
def ssh_execute(node_name: str, command: str) -> str:
    """
    Execute command on remote node via SSH
    
    Args:
        node_name: Node hostname or IP
        command: Command to execute
        
    Returns:
        str: Command output
    """
    try:
        import paramiko
        import os
        
        # Get SSH configuration from global config (would be passed in real implementation)
        ssh_user = "root"  # Default, should come from config
        ssh_key_path = os.path.expanduser("~/.ssh/id_ed25519")  # Default, should come from config
        
        # Create SSH client
        ssh_client = paramiko.SSHClient()
        ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        
        try:
            # Connect using SSH key
            ssh_client.connect(
                hostname=node_name,
                username=ssh_user,
                password='abc123',
                #key_filename=ssh_key_path,
                timeout=30
            )
            
            # Execute command
            stdin, stdout, stderr = ssh_client.exec_command(command, timeout=60)
            
            # Get output
            output = stdout.read().decode('utf-8')
            error = stderr.read().decode('utf-8')
            
            # Return combined output
            if error:
                return f"Output:\n{output}\nError:\n{error}"
            return output
            
        except Exception as e:
            return f"SSH execution failed: {str(e)}"
        finally:
            ssh_client.close()
            
    except ImportError:
        return f"Error: paramiko not available. Install with: pip install paramiko"
    except Exception as e:
        return f"SSH setup error: {str(e)}"
</file>

<file path="tools/kubernetes/__init__.py">
#!/usr/bin/env python3
"""
Kubernetes tools for volume troubleshooting.

This module contains:
- core: Basic kubectl operations and general Kubernetes resource management
- csi_baremetal: CSI Baremetal specific tools for custom resources
"""

from tools.kubernetes.core import (
    kubectl_get,
    kubectl_describe,
    kubectl_apply,
    kubectl_delete,
    kubectl_exec,
    kubectl_logs,
    kubectl_ls_pod_volume
)

from tools.kubernetes.csi_baremetal import (
    kubectl_get_drive,
    kubectl_get_csibmnode,
    kubectl_get_availablecapacity,
    kubectl_get_logicalvolumegroup,
    kubectl_get_storageclass,
    kubectl_get_csidrivers
)

__all__ = [
    # Core Kubernetes tools
    'kubectl_get',
    'kubectl_describe',
    'kubectl_apply',
    'kubectl_delete',
    'kubectl_exec',
    'kubectl_logs',
    'kubectl_ls_pod_volume',
    
    # CSI Baremetal specific tools
    'kubectl_get_drive',
    'kubectl_get_csibmnode',
    'kubectl_get_availablecapacity',
    'kubectl_get_logicalvolumegroup',
    'kubectl_get_storageclass',
    'kubectl_get_csidrivers'
]
</file>

<file path="tools/testing/volume_testing.py">
#!/usr/bin/env python3
"""
Volume testing tools for LangGraph.

This module imports and re-exports all volume testing tools for
validating volume functionality, checking filesystem health,
and monitoring performance.
"""

# Import tools from volume_testing_basic.py
from tools.testing.volume_testing_basic import (
    run_volume_io_test,
    validate_volume_mount,
    test_volume_permissions,
    verify_volume_mount
)

# Import tools from volume_testing_performance.py
from tools.testing.volume_testing_performance import (
    run_volume_stress_test,
    test_volume_io_performance,
    monitor_volume_latency
)

# Import tools from volume_testing_filesystem.py
from tools.testing.volume_testing_filesystem import (
    check_pod_volume_filesystem
)

# Import tools from volume_testing_analysis.py
from tools.testing.volume_testing_analysis import (
    analyze_volume_space_usage,
    check_volume_data_integrity
)

# Make all tools available when importing from this module
__all__ = [
    # Basic tools
    'run_volume_io_test',
    'validate_volume_mount',
    'test_volume_permissions',
    'verify_volume_mount',
    
    # Performance tools
    'run_volume_stress_test',
    'test_volume_io_performance',
    'monitor_volume_latency',
    
    # Filesystem tools
    'check_pod_volume_filesystem',
    
    # Analysis tools
    'analyze_volume_space_usage',
    'check_volume_data_integrity'
]
</file>

<file path="troubleshooting/end_conditions.py">
"""
End Condition Checker for Kubernetes Volume I/O Error Troubleshooting

This module defines classes for checking graph termination conditions.
It implements the Strategy Pattern for different end condition checks.
"""

import logging
import re
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional
from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage

# Configure logging
logger = logging.getLogger('end_conditions')
logger.setLevel(logging.INFO)

class EndConditionChecker(ABC):
    """Abstract base class for end condition checkers."""
    
    @abstractmethod
    def check_conditions(self, state: Dict[str, Any]) -> Dict[str, str]:
        """Check if specific end conditions are met.
        
        Args:
            state: The current state of the graph
            
        Returns:
            Dict with "result" key set to "end" or "continue"
        """
        pass

class LLMBasedEndConditionChecker(EndConditionChecker):
    """End condition checker that uses LLM to determine when to end graph execution."""
    
    def __init__(self, model, phase: str, max_iterations: int = 30):
        """Initialize the LLM-based end condition checker.
        
        Args:
            model: The LLM model to use for checking
            phase: Current phase (phase1 or phase2)
            max_iterations: Maximum number of iterations before forcing end
        """
        self.model = model
        self.phase = phase
        self.max_iterations = max_iterations
    
    def check_conditions(self, state: Dict[str, Any]) -> Dict[str, str]:
        """Check if specific end conditions are met using LLM assistance when available.
        
        Args:
            state: The current state of the graph
            
        Returns:
            Dict with "result" key set to "end" or "continue"
        """
        messages = state.get("messages", [])
        if not messages:
            return {"result": "continue"}
            
        last_message = messages[-1]
        
        # Situation 1: Check if the last message is a tool response
        # Check if we've reached max iterations
        ai_messages = [m for m in messages if getattr(m, "type", "") == "ai"]
        if len(ai_messages) > self.max_iterations:
            logger.info(f"Ending graph: reached max iterations ({self.max_iterations})")
            return {"result": "end"}
            
        # Skip content checks if the last message isn't from the AI
        if getattr(last_message, "type", "") != "ai":
            return {"result": "continue"}
            
        content = getattr(last_message, "content", "")
        if not content:
            return {"result": "continue"}

        # Situation 2: Check if has explicit end markers in the content using LLM
        if self._check_explicit_end_markers(content):
            logger.info("Ending graph: LLM detected explicit end markers")
            return {"result": "end"}
        
        # Situation 3: Check for specific phrases indicating completion using LLM
        if self._check_completion_indicators(content):
            logger.info("Ending graph: LLM detected completion indicators")
            return {"result": "end"}
        
        # Situation 4: Check for convergence (model repeating itself)
        if len(ai_messages) > 3:
            # Compare the last message with the third-to-last message (skipping the tool response in between)
            last_content = content
            third_to_last_content = getattr(ai_messages[-3], "content", "")
            
            # Simple similarity check - if they start with the same paragraph
            if last_content and third_to_last_content:
                # Get first 100 chars of each message
                last_start = last_content[:100] if len(last_content) > 100 else last_content
                third_start = third_to_last_content[:100] if len(third_to_last_content) > 100 else third_to_last_content
                
                if last_start == third_start:
                    logger.info("Ending graph: detected convergence (model repeating itself)")
                    return {"result": "end"}
        
        # Default: continue execution
        return {"result": "continue"}
    
    def _check_explicit_end_markers(self, content: str) -> bool:
        """Use LLM to check if content contains explicit or implicit end markers.
        
        Args:
            content: The content to check for end markers
            
        Returns:
            bool: True if end markers detected, False otherwise
        """
        # Create a focused prompt for the LLM
        system_prompt = """
        You are an AI assistant tasked with determining if a text contains explicit or implicit markers 
        indicating the end of a process or conversation. Your task is to analyze the given text and 
        determine if it contains phrases or markers that suggest completion or termination.
        
        Examples of explicit end markers include:
        - "[END_GRAPH]", "[END]", "End of graph", "GRAPH END"
        - "This concludes the analysis"
        - "Final report"
        - "Investigation complete"
        - "FIX PLAN", "Fix Plan"
        - " Would you like to"
        - A question from AI that indicates the end of the process, such as " Would you like to proceed with planning the disk replacement or further investigate filesystem integrity?"
        - If just a call tools result, then return 'NO'

        Examples of implicit end markers include:
        - A summary followed by recommendations with no further questions
        - A conclusion paragraph that wraps up all findings
        - A complete analysis with all required sections present
        - A question from AI that indicates the end of the process, such as "Is there anything else I can help you with?" or "Do you have any further questions?"
        
        Respond with "YES" if you detect end markers, or "NO" if you don't.
        """
        
        user_prompt = f"""
        Analyze the following text and determine if it contains explicit or implicit end markers:
        
        {content}  # Limit content length to avoid token limits
        
        Does this text contain markers indicating it's the end of the process? Respond with only YES or NO.
        """
        
        try:
            # Create messages for the LLM
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            
            # Call the LLM
            response = self.model.invoke(messages)
            
            # Check if the response indicates end markers
            response_text = response.content.strip().upper()
            
            # Log the LLM's response
            logger.info(f"LLM end marker detection response: {response_text}")
            
            # Return True if the LLM detected end markers
            return "YES" in response_text
        except Exception as e:
            # Log any errors and fall back to the original behavior
            logger.error(f"Error in LLM end marker detection: {e}")
            
            # Fall back to simple string matching
            return any(marker in content for marker in ["[END_GRAPH]", "[END]", "End of graph", "GRAPH END", "Fix Plan", "FIX PLAN"])
    
    def _check_completion_indicators(self, content: str) -> bool:
        """Use LLM to check if content indicates task completion based on phase requirements.
        
        Args:
            content: The content to check for completion indicators
            
        Returns:
            bool: True if completion indicators detected, False otherwise
        """
        # Define phase-specific required sections
        phase1_sections = [
            "Summary of Findings:",
            "Special Case Detected",
            "Detailed Analysis:",
            "Relationship Analysis:",
            "Investigation Process:",
            "Potential Root Causes:",
            "Root Cause:",
            "Fix Plan:",
            "Summary",
            "Recommendations"
        ]
        
        phase2_sections = [
            "Actions Taken:",
            "Test Results:",
            "Resolution Status:",
            "Remaining Issues:",
            "Recommendations:",
            "Summary of Findings:",
            "Special Case Detected",
            "Detailed Analysis:",
            "Relationship Analysis:",
            "Investigation Process:",
            "Potential Root Causes:",
            "Root Cause:",
            "Fix Plan:",
            "Summary",
            "Recommendations"
        ]
        
        # Select the appropriate sections based on the phase
        required_sections = phase1_sections if self.phase == "phase1" else phase2_sections
        
        # Create a focused prompt for the LLM
        system_prompt = f"""
        You are an AI assistant tasked with determining if a text contains sufficient information 
        to indicate that a troubleshooting process is complete. Your task is to analyze the given text 
        and determine if it contains the required sections and information for a {self.phase} report.
        
        For {self.phase}, the following sections are expected in a complete report:
        {', '.join(required_sections)}
        
        A complete report should have some of these sections and provide comprehensive information 
        in each section. The report should feel complete and not leave major questions unanswered.
        If just a call tools result, then return 'NO'.
        
        Respond with "YES" if you believe the text represents a complete report, or "NO" if it seems incomplete.
        """
        
        user_prompt = f"""
        Analyze the following text and determine if it represents a complete {self.phase} report:
        
        {content}  # Limit content length to avoid token limits
        
        Does this text contain sufficient information to be considered a complete report? Respond with only YES or NO.
        """
        
        try:
            # Create messages for the LLM
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            
            # Call the LLM
            response = self.model.invoke(messages)
            
            # Check if the response indicates completion
            response_text = response.content.strip().upper()
            
            # Log the LLM's response
            logger.info(f"LLM completion detection response for {self.phase}: {response_text}")
            
            # Return True if the LLM detected completion
            return "YES" in response_text
        except Exception as e:
            # Log any errors and fall back to the original behavior
            logger.error(f"Error in LLM completion detection: {e}")
            
            # Fall back to counting sections
            sections_found = sum(1 for section in required_sections if section in content)
            threshold = 3 if self.phase == "phase1" else 2
            return sections_found >= threshold

class SimpleEndConditionChecker(EndConditionChecker):
    """Simple end condition checker that uses regex patterns to check for end conditions."""
    
    def __init__(self, max_iterations: int = 30):
        """Initialize the simple end condition checker.
        
        Args:
            max_iterations: Maximum number of iterations before forcing end
        """
        self.max_iterations = max_iterations
        # Define patterns that indicate completion
        self.end_markers = [
            r"\[END_GRAPH\]", 
            r"\[END\]", 
            r"End of graph", 
            r"GRAPH END",
            r"This concludes the (analysis|investigation|troubleshooting)",
            r"Fix Plan:",
            r"Root Cause:",
            r"Summary of Findings:"
        ]
    
    def check_conditions(self, state: Dict[str, Any]) -> Dict[str, str]:
        """Check if specific end conditions are met.
        
        Args:
            state: The current state of the graph
            
        Returns:
            Dict with "result" key set to "end" or "continue"
        """
        messages = state.get("messages", [])
        if not messages:
            return {"result": "continue"}
            
        last_message = messages[-1]
        
        # Check if we've reached max iterations
        ai_messages = [m for m in messages if getattr(m, "type", "") == "ai"]
        if len(ai_messages) > self.max_iterations:
            logger.info(f"Ending graph: reached max iterations ({self.max_iterations})")
            return {"result": "end"}
            
        # Skip content checks if the last message isn't from the AI
        if getattr(last_message, "type", "") != "ai":
            return {"result": "continue"}
            
        content = getattr(last_message, "content", "")
        if not content:
            return {"result": "continue"}
            
        # Check for end markers in the content
        for pattern in self.end_markers:
            if re.search(pattern, content, re.IGNORECASE):
                logger.info(f"Ending graph: detected end marker matching pattern {pattern}")
                return {"result": "end"}
                
        # Check for convergence (model repeating itself)
        if len(ai_messages) > 3:
            last_content = content
            third_to_last_content = getattr(ai_messages[-3], "content", "")
            
            if last_content and third_to_last_content:
                # Get first 100 chars of each message
                last_start = last_content[:100] if len(last_content) > 100 else last_content
                third_start = third_to_last_content[:100] if len(third_to_last_content) > 100 else third_to_last_content
                
                if last_start == third_start:
                    logger.info("Ending graph: detected convergence (model repeating itself)")
                    return {"result": "end"}
                    
        # Default: continue execution
        return {"result": "continue"}

class EndConditionFactory:
    """Factory class for creating end condition checkers."""
    
    @staticmethod
    def create_checker(checker_type: str, **kwargs) -> EndConditionChecker:
        """Create an end condition checker of the specified type.
        
        Args:
            checker_type: Type of checker to create ('llm' or 'simple')
            **kwargs: Additional arguments to pass to the checker constructor
            
        Returns:
            An EndConditionChecker instance
        """
        if checker_type.lower() == "llm":
            return LLMBasedEndConditionChecker(**kwargs)
        return SimpleEndConditionChecker(**kwargs)
</file>

<file path="troubleshooting/hook_manager.py">
"""
Hook Manager for Tool Execution in Kubernetes Volume I/O Error Troubleshooting

This module defines classes for managing hooks that run before and after tool execution.
It separates hook management from the main execution logic for better modularity.
"""

import logging
import json
from typing import Any, Callable, Dict, Optional
from rich.console import Console
from rich.panel import Panel

# Configure logging
logger = logging.getLogger('hook_manager')
logger.setLevel(logging.INFO)

# Hook type definitions
BeforeCallToolsHook = Callable[[str, Dict[str, Any], str], None]
AfterCallToolsHook = Callable[[str, Dict[str, Any], Any, str], None]

class HookManager:
    """Manager for before and after tool execution hooks."""
    
    def __init__(self, console: Optional[Console] = None, file_console: Optional[Console] = None):
        """Initialize the hook manager.
        
        Args:
            console: Rich console for output. If None, a new console will be created.
            file_console: Rich console for file output. If None, no file output will be generated.
        """
        self.before_call_hook: Optional[BeforeCallToolsHook] = None
        self.after_call_hook: Optional[AfterCallToolsHook] = None
        self.console = console or Console()
        self.file_console = file_console
    
    def register_before_call_hook(self, hook: BeforeCallToolsHook) -> None:
        """Register a hook function to be called before tool execution.
        
        Args:
            hook: A callable that takes tool name and arguments as parameters
        """
        self.before_call_hook = hook
    
    def register_after_call_hook(self, hook: AfterCallToolsHook) -> None:
        """Register a hook function to be called after tool execution.
        
        Args:
            hook: A callable that takes tool name, arguments, and result as parameters
        """
        self.after_call_hook = hook
    
    def run_before_hook(self, tool_name: str, args: Dict[str, Any], call_type: str) -> None:
        """Run the before call hook if registered.
        
        Args:
            tool_name: Name of the tool being called
            args: Arguments passed to the tool
            call_type: Type of call execution ("Parallel" or "Serial")
        """
        if self.before_call_hook:
            try:
                self.before_call_hook(tool_name, args, call_type)
            except Exception as e:
                logger.error(f"Error in before_call_hook: {e}")
        else:
            # Default implementation if no hook is registered
            self._default_before_hook(tool_name, args, call_type)
    
    def run_after_hook(self, tool_name: str, args: Dict[str, Any], result: Any, call_type: str) -> None:
        """Run the after call hook if registered.
        
        Args:
            tool_name: Name of the tool that was called
            args: Arguments that were passed to the tool
            result: Result returned by the tool
            call_type: Type of call execution ("Parallel" or "Serial")
        """
        if self.after_call_hook:
            try:
                self.after_call_hook(tool_name, args, result, call_type)
            except Exception as e:
                logger.error(f"Error in after_call_hook: {e}")
        else:
            # Default implementation if no hook is registered
            self._default_after_hook(tool_name, args, result, call_type)
    
    def _default_before_hook(self, tool_name: str, args: Dict[str, Any], call_type: str) -> None:
        """Default implementation for the before call hook.
        
        Args:
            tool_name: Name of the tool being called
            args: Arguments passed to the tool
            call_type: Type of call execution ("Parallel" or "Serial")
        """
        try:
            # Format arguments for better readability
            formatted_args = json.dumps(args, indent=2) if args else "None"
            
            # Format the tool usage in a nice way
            if formatted_args != "None":
                # Print to console and log file
                tool_panel = Panel(
                    f"[bold yellow]Tool:[/bold yellow] [green]{tool_name}[/green] [bold cyan]({call_type})[/bold cyan]\n\n"
                    f"[bold yellow]Arguments:[/bold yellow]\n[blue]{formatted_args}[/blue]",
                    title="[bold magenta]Thinking Step",
                    border_style="magenta",
                    safe_box=True
                )
                self.console.print(tool_panel)
            else:
                # Simple version for tools without arguments
                tool_panel = Panel(
                    f"[bold yellow]Tool:[/bold yellow] [green]{tool_name}[/green] [bold cyan]({call_type})[/bold cyan]\n\n"
                    f"[bold yellow]Arguments:[/bold yellow] None",
                    title="[bold magenta]Thinking Step",
                    border_style="magenta",
                    safe_box=True
                )
                self.console.print(tool_panel)

            # Also log to file console if available
            if self.file_console:
                self.file_console.print(f"Executing tool: {tool_name} ({call_type})")
                self.file_console.print(f"Parameters: {formatted_args}")
            
            # Log to standard logger
            logger.info(f"Executing tool: {tool_name} ({call_type})")
            logger.info(f"Parameters: {formatted_args}")
        except Exception as e:
            logger.error(f"Error in default_before_hook: {e}")
    
    def _default_after_hook(self, tool_name: str, args: Dict[str, Any], result: Any, call_type: str) -> None:
        """Default implementation for the after call hook.
        
        Args:
            tool_name: Name of the tool that was called
            args: Arguments that were passed to the tool
            result: Result returned by the tool
            call_type: Type of call execution ("Parallel" or "Serial")
        """
        try:
            try:
                # Format result for better readability
                if hasattr(result, 'content'):
                    result_content = result.content
                    result_status = result.status if hasattr(result, 'status') else 'success'
                    formatted_result = f"Status: {result_status}\nContent: {result_content[:1000]}"
                else:
                    formatted_result = str(result)[:1000]
                
                # Print tool result to console
                tool_panel = Panel(
                    f"[bold cyan]Tool completed:[/bold cyan] [green]{tool_name}[/green] [bold cyan]({call_type})[/bold cyan]\n"
                    f"[bold cyan]Result:[/bold cyan]\n[yellow]{formatted_result}[/yellow]",
                    title="[bold magenta]Call tools",
                    border_style="magenta",
                    safe_box=True
                )
                self.console.print(tool_panel)
            except Exception as e:
                logger.error(f"Error formatting result: {e}")
                formatted_result = "Error formatting result"

            # Also log to file console if available
            if self.file_console:
                self.file_console.print(f"Tool completed: {tool_name} ({call_type})")
                self.file_console.print(f"Result: {formatted_result}")
            
            # Log to standard logger
            logger.info(f"Tool completed: {tool_name} ({call_type})")
            logger.info(f"Result: {formatted_result}")
        except Exception as e:
            logger.error(f"Error in default_after_hook: {e}")
</file>

<file path="troubleshooting/strategies.py">
"""
Tool Execution Strategies for Kubernetes Volume I/O Error Troubleshooting

This module defines strategy classes for tool execution in the troubleshooting system.
It implements the Strategy Pattern to separate different execution approaches.
"""

import logging
import concurrent.futures
import asyncio
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Literal, Optional, Union
from enum import Enum

from langchain_core.messages import ToolMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.runnables.config import get_config_list

# Configure logging
logger = logging.getLogger('strategies')
logger.setLevel(logging.INFO)

class ExecutionType(Enum):
    """Enumeration for tool execution types."""
    SERIAL = "Serial"
    PARALLEL = "Parallel"

class ToolExecutionStrategy(ABC):
    """Abstract base class for tool execution strategies."""
    
    @abstractmethod
    def execute(
        self,
        tool_calls: List[Dict[str, Any]],
        input_type: Literal["list", "dict", "tool_calls"],
        config: RunnableConfig,
        run_one_callback: callable,
    ) -> List[ToolMessage]:
        """Execute tool calls according to a specific strategy.
        
        Args:
            tool_calls: List of tool calls to execute
            input_type: Type of input (list, dict, or tool_calls)
            config: Runnable configuration
            run_one_callback: Callback function to execute a single tool
            
        Returns:
            List of ToolMessage results
        """
        pass
    
    @abstractmethod
    async def execute_async(
        self,
        tool_calls: List[Dict[str, Any]],
        input_type: Literal["list", "dict", "tool_calls"],
        config: RunnableConfig,
        run_one_callback: callable,
    ) -> List[ToolMessage]:
        """Execute tool calls asynchronously according to a specific strategy.
        
        Args:
            tool_calls: List of tool calls to execute
            input_type: Type of input (list, dict, or tool_calls)
            config: Runnable configuration
            run_one_callback: Callback function to execute a single tool
            
        Returns:
            List of ToolMessage results
        """
        pass

class SerialToolExecutionStrategy(ToolExecutionStrategy):
    """Strategy for executing tools sequentially."""
    
    def execute(
        self,
        tool_calls: List[Dict[str, Any]],
        input_type: Literal["list", "dict", "tool_calls"],
        config: RunnableConfig,
        run_one_callback: callable,
    ) -> List[ToolMessage]:
        """Execute tools sequentially in the order they appear.
        
        Args:
            tool_calls: List of tool calls to execute serially
            input_type: Type of input (list, dict, or tool_calls)
            config: Runnable configuration
            run_one_callback: Callback function to execute a single tool
            
        Returns:
            List of ToolMessage results
        """
        if not tool_calls:
            return []
            
        config_list = get_config_list(config, len(tool_calls))
        outputs = []
        
        # Process tools sequentially
        for i, tool_call in enumerate(tool_calls):
            # Get the individual config for this tool call
            tool_config = config_list[i] if i < len(config_list) else config_list[-1]
            
            # Run the tool with "Serial" call type
            output = run_one_callback(tool_call, input_type, tool_config, ExecutionType.SERIAL.value)
            outputs.append(output)
            
        return outputs
    
    async def execute_async(
        self,
        tool_calls: List[Dict[str, Any]],
        input_type: Literal["list", "dict", "tool_calls"],
        config: RunnableConfig,
        run_one_callback: callable,
    ) -> List[ToolMessage]:
        """Execute tools sequentially in the order they appear (async version).
        
        Args:
            tool_calls: List of tool calls to execute serially
            input_type: Type of input (list, dict, or tool_calls)
            config: Runnable configuration
            run_one_callback: Callback function to execute a single tool
            
        Returns:
            List of ToolMessage results
        """
        if not tool_calls:
            return []
            
        config_list = get_config_list(config, len(tool_calls))
        outputs = []
        
        # Process tools sequentially
        for i, tool_call in enumerate(tool_calls):
            # Get the individual config for this tool call
            tool_config = config_list[i] if i < len(config_list) else config_list[-1]
            
            # Run the tool with "Serial" call type
            output = await run_one_callback(tool_call, input_type, tool_config, ExecutionType.SERIAL.value)
            outputs.append(output)
            
        return outputs

class ParallelToolExecutionStrategy(ToolExecutionStrategy):
    """Strategy for executing tools concurrently."""
    
    def __init__(self, max_workers: Optional[int] = None):
        """Initialize the parallel execution strategy.
        
        Args:
            max_workers: Maximum number of worker threads to use for parallel execution.
                Defaults to None (uses ThreadPoolExecutor default).
        """
        self.max_workers = max_workers
    
    def execute(
        self,
        tool_calls: List[Dict[str, Any]],
        input_type: Literal["list", "dict", "tool_calls"],
        config: RunnableConfig,
        run_one_callback: callable,
    ) -> List[ToolMessage]:
        """Execute tools concurrently using ThreadPoolExecutor.
        
        Args:
            tool_calls: List of tool calls to execute in parallel
            input_type: Type of input (list, dict, or tool_calls)
            config: Runnable configuration
            run_one_callback: Callback function to execute a single tool
            
        Returns:
            List of ToolMessage results
        """
        if not tool_calls:
            return []
            
        config_list = get_config_list(config, len(tool_calls))
        outputs = []
        
        try:
            # Process tools in parallel using ThreadPoolExecutor
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # Create a dictionary to map futures to their corresponding tool calls and configs
                future_to_tool = {}
                
                # Submit all tool calls to the executor
                for i, tool_call in enumerate(tool_calls):
                    # Get the individual config for this tool call
                    tool_config = config_list[i] if i < len(config_list) else config_list[-1]
                    
                    # Submit the tool call to the executor with "Parallel" call type
                    future = executor.submit(
                        run_one_callback, tool_call, input_type, tool_config, ExecutionType.PARALLEL.value
                    )
                    future_to_tool[future] = (tool_call, tool_config)
                
                # Process completed futures as they finish
                for future in concurrent.futures.as_completed(future_to_tool):
                    try:
                        output = future.result()
                        outputs.append(output)
                    except Exception as exc:
                        # If an exception occurs in the thread, log it and create an error message
                        tool_call, _ = future_to_tool[future]
                        logger.error(f"Tool {tool_call['name']} generated an exception: {exc}")
                        error_message = ToolMessage(
                            content=f"Error executing tool {tool_call['name']}: {str(exc)}",
                            name=tool_call["name"],
                            tool_call_id=tool_call["id"],
                            status="error",
                        )
                        outputs.append(error_message)
        except Exception as e:
            # If ThreadPoolExecutor fails, log the error and fall back to sequential execution
            logger.error(f"Parallel execution failed, falling back to sequential: {e}")
            # Fall back to sequential execution
            serial_strategy = SerialToolExecutionStrategy()
            outputs = serial_strategy.execute(tool_calls, input_type, config, run_one_callback)
            
        return outputs
    
    async def execute_async(
        self,
        tool_calls: List[Dict[str, Any]],
        input_type: Literal["list", "dict", "tool_calls"],
        config: RunnableConfig,
        run_one_callback: callable,
    ) -> List[ToolMessage]:
        """Execute tools concurrently using asyncio.gather.
        
        Args:
            tool_calls: List of tool calls to execute in parallel
            input_type: Type of input (list, dict, or tool_calls)
            config: Runnable configuration
            run_one_callback: Callback function to execute a single tool
            
        Returns:
            List of ToolMessage results
        """
        if not tool_calls:
            return []
            
        config_list = get_config_list(config, len(tool_calls))
        outputs = []
        
        try:
            # Process tools in parallel using asyncio.gather
            tasks = []
            for i, tool_call in enumerate(tool_calls):
                # Get the individual config for this tool call
                tool_config = config_list[i] if i < len(config_list) else config_list[-1]
                
                # Create a task for each tool call with "Parallel" call type
                task = asyncio.create_task(
                    run_one_callback(tool_call, input_type, tool_config, ExecutionType.PARALLEL.value)
                )
                tasks.append(task)
            
            # Wait for all tasks to complete
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Process results
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    # If an exception occurred, create an error message
                    tool_call = tool_calls[i]
                    logger.error(f"Tool {tool_call['name']} generated an exception: {result}")
                    error_message = ToolMessage(
                        content=f"Error executing tool {tool_call['name']}: {str(result)}",
                        name=tool_call["name"],
                        tool_call_id=tool_call["id"],
                        status="error",
                    )
                    outputs.append(error_message)
                else:
                    outputs.append(result)
        except Exception as e:
            # If asyncio.gather fails, log the error and fall back to sequential execution
            logger.error(f"Parallel async execution failed, falling back to sequential: {e}")
            # Fall back to sequential execution
            serial_strategy = SerialToolExecutionStrategy()
            outputs = await serial_strategy.execute_async(tool_calls, input_type, config, run_one_callback)
            
        return outputs

class StrategyFactory:
    """Factory class for creating execution strategies."""
    
    @staticmethod
    def create_strategy(strategy_type: ExecutionType, max_workers: Optional[int] = None) -> ToolExecutionStrategy:
        """Create a strategy based on execution type.
        
        Args:
            strategy_type: Type of execution strategy (SERIAL or PARALLEL)
            max_workers: Maximum number of worker threads for parallel execution
            
        Returns:
            An instance of a ToolExecutionStrategy
        """
        if strategy_type == ExecutionType.PARALLEL:
            return ParallelToolExecutionStrategy(max_workers)
        return SerialToolExecutionStrategy()
</file>

<file path="hot.py">
#!/usr/bin/python3

import os
import re
import subprocess
import sys
import argparse
import time

verbose = 0

def get_verbose():
    global verbose
    return verbose

def get_device_name_from_args(args_name):
    name = os.path.basename(args_name)
    if not name:
        return None

    # Handle NVMe devices
    match = re.match(r"^(nvme\d+(n|p)\d+)", name)
    if match:
        block_device = match.group(1)  # e.g., nvme2n1 or nvme2p1
        block_path = os.path.join("/sys/block", block_device)
        
        try:
            # Read the symbolic link to get the device path
            device_path = os.readlink(block_path)
            
            # Extract the NVMe controller name (e.g., 'nvme5' from '.../nvme/nvme5/nvme2n1')
            nvme_controller = None
            for part in device_path.split('/'):
                if part.startswith('nvme') and re.match(r'nvme\d+$', part):
                    nvme_controller = part
                    break
            
            if nvme_controller:
                nvme_path = os.path.join("/sys/class/nvme", nvme_controller)
                # Check if the NVMe controller path exists
                if os.path.exists(nvme_path):
                    return nvme_controller
                else:
                    print(f"NVMe controller {nvme_path} does not exist")
            else:
                print(f"Could not find NVMe controller for {block_device}")
        except OSError:
            print(f"Failed to resolve NVMe device path for {block_device}")
        return None

    # Handle SCSI block devices (unchanged)
    match = re.match(r"^(sd[a-z]+)", name)
    if match:
        block_device = match.group(1)  # e.g., sda
        path = os.path.join("/sys/class/block", block_device)
        # Check if the SCSI block device exists
        if os.path.exists(path):
            return block_device
        else:
            print(f"SCSI block device {path} does not exist")
        return None

    return None

'''
def get_nvme_name_from_args(args_nvme_name):
    name = os.path.basename(args_nvme_name)
    if name:
        match = re.match(r"^(nvme[0-9]+)+", name)
        if match:
            path = "/sys/class/nvme/" + match.group(1)
            # check if specified nvme name exists
            if os.path.exists(path):
                return match.group(1)
    return

def get_disk_name_from_args(args_disk_name):
    name = os.path.basename(args_disk_name)
    if name:
        match = re.match(r"^(sd[a-z]+)", name)
        if match:
            path = "/sys/class/block/" + match.group(1)
            # check if specified nvme name exists
            if os.path.exists(path):
                return match.group(1)
    return
'''

def get_nvme_physlot(nvme_device_name):
    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
    # Validate NVMe device name (e.g., nvme2n1, nvme2p1)
    if not re.match(r'nvme\d+(n|p)\d+', nvme_device_name):
        return None  # Invalid NVMe device name

    # Construct path to the block device in /sys/block
    block_path = os.path.join('/sys/block', nvme_device_name)

    try:
        # Read the symbolic link to get the device path
        device_path = os.readlink(block_path)

        # Extract the NVMe controller name (e.g., 'nvme5' from '.../nvme/nvme5/nvme2n1')
        nvme_controller = None
        for part in device_path.split('/'):
            if part.startswith('nvme') and re.match(r'nvme\d+$', part):
                nvme_controller = part
                break

        if not nvme_controller:
            return None  # Could not find NVMe controller

        # Construct the path to the NVMe controller in /sys/class/nvme
        nvme_path = os.path.join('/sys/class/nvme', nvme_controller)

        # Read the symbolic link to get the PCI bus path
        bus_path = os.readlink(nvme_path)
        # Extract the bus ID (e.g., '0000:65:00.0')
        dirs = bus_path.split('/')
        bus_id = dirs[5]  # Assuming bus ID is at index 5

        # Run lspci to get physical slot information
        output = subprocess.check_output(['lspci', '-s', bus_id, '-vmm'])
        match = re.search(r'PhySlot:\s+(\d+)', output.decode())
        if match:
            return match.group(1)
        else:
            return None  # No physical slot found
    except (OSError, subprocess.CalledProcessError):
        return None  # Handle errors (e.g., path doesn't exist, lspci fails)

def get_nvme_pcibus(nvme_device_name):
    path = os.path.join('/sys/class/nvme', nvme_device_name)
    bus_path = os.readlink(path)
    dirs = bus_path.split('/')
    pcibus = f"{dirs[3]}/{dirs[4]}"
    return pcibus

def run_command(command):
    try:
        subprocess.run(command, shell=True, check=True)
        return 0
    except subprocess.CalledProcessError as e:
        print(f"command: '{command}' returned non-zero exit status {e.returncode}")
        print(f"output: {e.output}")
        print(f"error: {e.stderr}")
        return e.returncode

def nvme_power_control(physlot, onoff):
    if onoff == "on" :
        # not used yet, need to save physlot to somewhere before the poweroff
        command = f"echo 1 > /sys/bus/pci/slots/{physlot}/power"
    else:
        # poweroff -> pci rescan -> poweroff(fail), need to echo 1 and then echo 0
        # poweroff -> poweron -> poweroff(ok)
        subprocess.run(f"echo 1 > /sys/bus/pci/slots/{physlot}/power", shell=True, stdout = subprocess.DEVNULL, stderr = subprocess.DEVNULL)
        command = f"echo 0 > /sys/bus/pci/slots/{physlot}/power"
    if get_verbose() > 0:
        print(f"run cmd: {command}")
    return run_command(command)

def nvme_device_delete(nvme_device_name):
    command = f"echo 1 > /sys/class/nvme/{nvme_device_name}/device/remove"
    if get_verbose() > 0:
        print(f"run cmd: {command}")
    return run_command(command)

def scsi_disk_delete(scsi_disk_name):
    command = f"echo 1 > /sys/block/{scsi_disk_name}/device/delete"
    if get_verbose() > 0:
        print(f"run cmd: {command}")
    return run_command(command)

def nvme_device_rescan(pcibus):
    command = f"echo 1 > /sys/devices/{pcibus}/rescan"
    if get_verbose() > 0:
        print(f"run cmd: {command}")
    return run_command(command)

def nvme_device_rescan_all():
    # pci rescan can add back deleted devices and power off devices.
    command = "echo 1 > /sys/bus/pci/rescan"
    if get_verbose() > 0:
        print(f"run cmd: {command}")
    return run_command(command)

def scsi_disk_rescan():
    command = "for scan in `ls /sys/class/scsi_host/host*/scan`; do echo \"- - -\" > $scan;done"
    if get_verbose() > 0:
        print(command)
    return run_command(command)

def device_rescan(args):
    scan_all = False
    if not (args.scsi or args.nvme):
        scan_all = True
    if args.nvme or scan_all:
        if nvme_device_rescan_all() != 0:
            print("nvme rescan failed")
            return
        print("nvme disks rescan done")
    if args.scsi or scan_all:
        if scsi_disk_rescan() != 0:
            print("scsi host rescan failed")
            return
        print("scsi disks rescan done")
    print("success")
    return 

def device_off(args):
    name = get_device_name_from_args(args.disk)
    if not name:
        print(f"invalid disk name: {args.disk}")
        return
    if name.startswith("nvme"):
        # nvme disk poweroff
        if args.poweroff:
            physlot = get_nvme_physlot(name)
            if physlot:
                if nvme_power_control(physlot, "off") != 0:
                    print(f"failed to poweroff {name}")
                    return
                print(f"power off {name} done")
            else :
                print("failed to get nvme physlot")
                return
            if args.time > 0:
                print(f"sleeping({args.time}s)...")
                time.sleep(args.time)
                if nvme_power_control(physlot, "on") != 0:
                    print(f"failed to poweron {name}")
                    return
                print(f"power on {name} done")
            else:
                command1 = f"command1: {os.path.basename(sys.argv[0])} on -a poweron -p {physlot}"
                command2 = f"command2: {os.path.basename(sys.argv[0])} rescan"
                print(f"the physlot of {name} is {physlot}, to power on the disk, please run one of the below commands")
                print(command1)
                print(command2)
        # nvme disk delete
        if args.remove:
            pcibus = get_nvme_pcibus(name)
            if nvme_device_delete(name) != 0:
                print(f"failed to delete {name}")
                return
            print(f"delete {name} done")
            if args.time > 0:
                print(f"sleeping({args.time}s)...")
                time.sleep(args.time)
                if nvme_device_rescan(pcibus) != 0:
                    print(f"failed to rescan for {name}")
                    return
                print(f"rescan {name} done")
            else:
                command1 = f"command1: {os.path.basename(sys.argv[0])} on -a rescan -p {pcibus}"
                command2 = f"command2: {os.path.basename(sys.argv[0])} rescan"
                print(f"the pci bus of {name} is {pcibus}, to rescan the disk, please run one of the below commands")
                print(command1)
                print(command2)
    else :
        # scsi disk
        if args.poweroff:
            print("power off scsi disk is not supported.")
            return
        if args.remove:
            if scsi_disk_delete(name) != 0:
                print(f"failed to delete {name}")
                return
            print(f"delete {name} done")
            if args.time > 0:
                print(f"sleeping({args.time}s)...")
                time.sleep(args.time)
                if scsi_disk_rescan() != 0:
                    print(f"failed to rescan all scsi host")
                    return
                print("rescan all scsi host done")
            else:
                command = f"command: {os.path.basename(sys.argv[0])} rescan"
                print(f"to rescan the disk, please run below command")
                print(command)
    return

def device_on(args):
    if args.action == "poweron":
        path = f"/sys/bus/pci/slots/{args.param}"
        if not os.path.exists(path):
            print(f"invalid physlot {args.param}")
            return
        if nvme_power_control(args.param, "on") != 0:
            print(f"failed to power on physlot {args.param}")
            return
        print(f"power on physlot {args.param} done")
    if args.action == "rescan":
        path = f"/sys/devices/{args.param}"
        if not os.path.exists(path):
            print(f"invalid pci bus {args.param}")
            return
        if nvme_device_rescan(args.param) != 0:
            print(f"failed to rescan pci bus {args.param}")
            return
        print(f"rescan pci bus {args.param} done")
    return

def get_my_node_uuid():
    command = "kubectl get node -o=custom-columns=Name:'{metadata.name}',Uuid:'{metadata.labels.nodes\\.csi\-baremetal\\.dell\\.com/uuid}'|grep -i $HOSTNAME"
    result = subprocess.check_output(command, shell=True)
    node_name, node_uuid = result.decode().split('\n')[0].split()
    if get_verbose() > 0:
        print(command)
        print(result.decode())
    return node_uuid, node_name
    
def print_node_drive_list(node_uuid):
    command = f"kubectl get drive -o=custom-columns=NAME:.metadata.name,TYPE:.spec.Type,PATH:.spec.Path,NODE:.spec.NodeId,SYSTEM:.spec.IsSystem|grep -E \"{node_uuid}|^NAME\""
    if get_verbose() > 0:
        print(command)
    result = subprocess.check_output(command, shell=True)
    print(result.decode())

def list_drives(args):
    node_uuid, node_name = get_my_node_uuid()
    if node_uuid:
        print_node_drive_list(node_uuid)
    else:
        print("failed to get current node uuid")

def add_arguments(in_parser):
    subparsers = in_parser.add_subparsers(title='hotplug_type', dest='sub_parser')

    # command offline
    off_parser = subparsers.add_parser('off', description='make the specified disk offline', help='make the specified disk offline')
    off_mutually_exclusive_group = off_parser.add_mutually_exclusive_group(required=True)
    off_mutually_exclusive_group.add_argument('-p','--poweroff', action='store_true', help='poweroff nvme disk(only support nvme disk)')
    off_mutually_exclusive_group.add_argument('-r','--remove', action='store_true', help='remove disk(any type)')
    off_parser.add_argument('-d', '--disk', help='disk device name', required=True)
    off_parser.add_argument('-t', '--time', help='disks will be added back after specified seconds', type=int, default=0)
    off_parser.add_argument('-v', '--verbose', help='show what commands were executed', action='count', default=0)
    off_parser.set_defaults(func=device_off)

    # command online
    on_parser = subparsers.add_parser('on', description='make nvme disk online by scan pci bus or power on physlot(only support nvme disk)', help='make nvme disk online(nvme only)')
    '''
    on_mutually_exclusive_group = on_parser.add_mutually_exclusive_group(required=True)
    on_mutually_exclusive_group.add_argument('-p', '--pci', help='scan nvme on pci bus address')
    on_mutually_exclusive_group.add_argument('-s', '--slot', help='power on nvme on pci physlot')
    '''
    on_parser.add_argument('-a', '--action', choices=['poweron', 'rescan'], required=True, help='power on physlot or scan pci bus')
    on_parser.add_argument('-p', '--param', required=True, help='action param, slot or bus determined by action')
    on_parser.add_argument('-v', '--verbose', help='show what commands were executed', action='count', default=0)
    on_parser.set_defaults(func=device_on)

    # command rescan
    rescan_parser = subparsers.add_parser('rescan', description='add back all deleted/power off nvme or/and scsi disks by rescan pci bus/scsi host', help='add back all nvme or/and scsi disks')
    rescan_parser.add_argument('-s', '--scsi', help='add back all scsi disks', action='store_true')
    rescan_parser.add_argument('-n', '--nvme', help='add back all nvme disks', action='store_true')
    rescan_parser.add_argument('-v', '--verbose', help='show what commands were executed', action='count', default=0)
    rescan_parser.set_defaults(func=device_rescan)

    # command list
    list_parser = subparsers.add_parser('list', description='list all drives on current node', help='list all drives on current node')
    list_parser.add_argument('-v', '--verbose', help='show what commands were executed', action='count', default=0)
    list_parser.set_defaults(func=list_drives)

def main():
    parser = argparse.ArgumentParser(description='nvme hotplug error injection tool')
    add_arguments(parser)
    args = parser.parse_args()
    global verbose
    verbose = args.verbose

    args.func(args)


if __name__ == "__main__":
    main()
</file>

<file path="examples/standalone_react_example.py">
#!/usr/bin/env python3
"""
Standalone example of using the Plan Phase ReAct graph directly.

This script demonstrates how to use the Plan Phase ReAct graph implementation
in a standalone manner, without going through the full troubleshooting system.
"""

import asyncio
import logging
import yaml
import json
import sys
import os
from typing import Dict, List, Any
from langchain_core.messages import SystemMessage, HumanMessage

# Add parent directory to path to import modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from knowledge_graph import KnowledgeGraph
from phases.plan_phase_react import PlanPhaseReActGraph
from tests.mock_knowledge_graph import create_mock_knowledge_graph

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('standalone_react_example.log')
    ]
)

logger = logging.getLogger(__name__)

async def run_standalone_react_example():
    """Run a standalone example of the Plan Phase ReAct graph."""
    logger.info("Starting standalone ReAct graph example")
    
    # Load configuration
    try:
        with open('config.yaml', 'r') as f:
            config_data = yaml.safe_load(f)
    except Exception as e:
        logger.error(f"Failed to load configuration: {e}")
        config_data = {}
    
    # Create mock knowledge graph
    logger.info("Creating mock knowledge graph")
    knowledge_graph = create_mock_knowledge_graph()
    
    # Set test parameters
    pod_name = "example-pod"
    namespace = "default"
    volume_path = "/dev/sda"
    
    # Initialize the ReAct graph
    logger.info("Initializing ReAct graph")
    react_graph = PlanPhaseReActGraph(config_data)
    
    # Build the graph
    logger.info("Building ReAct graph")
    graph = react_graph.build_graph()
    
    # Prepare initial messages
    logger.info("Preparing initial messages")
    # Create system message with instructions for ReAct
    system_prompt = """You are an AI assistant tasked with generating an Investigation Plan for troubleshooting Kubernetes volume I/O errors.
You are operating in a ReAct (Reasoning and Acting) framework where you can:
1. REASON about the problem and identify knowledge gaps
2. ACT by calling external tools to gather information
3. OBSERVE the results and update your understanding
4. Continue this loop until you have enough information to create a comprehensive plan

Your goal is to create a detailed Investigation Plan that identifies potential problems and provides specific steps to diagnose and resolve volume read/write errors.

When you identify a knowledge gap, use the appropriate MCP tool to gather the information you need. Don't guess or make assumptions when you can use a tool to get accurate information.

When you've completed the Investigation Plan, include the marker [END_GRAPH] at the end of your message.
"""
    
    # Create user message with context
    kg_summary = knowledge_graph.get_summary() if knowledge_graph else {}
    issues = knowledge_graph.get_all_issues() if knowledge_graph else []
    
    kg_context = f"""
Knowledge Graph Summary:
{json.dumps(kg_summary, indent=2)}

Issues:
{json.dumps(issues, indent=2)}
"""
    
    user_prompt = f"""# INVESTIGATION PLAN GENERATION TASK
## TARGET: Volume read/write errors in pod {pod_name} (namespace: {namespace}, volume path: {volume_path})

I need you to create a comprehensive Investigation Plan for troubleshooting this volume I/O error.

## BACKGROUND INFORMATION

### KNOWLEDGE GRAPH CONTEXT
{kg_context}

## TASK
1. Analyze the available information to understand the context
2. Identify any knowledge gaps that need to be filled
3. Use MCP tools to gather additional information as needed
4. Create a comprehensive Investigation Plan with specific steps to diagnose and resolve the volume I/O error

Please start by analyzing the available information and identifying any knowledge gaps.
"""
    
    # Create message list
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ]
    
    # Prepare initial state
    logger.info("Preparing initial state")
    initial_state = {
        "messages": messages,
        "iteration_count": 0,
        "tool_call_count": 0,
        "knowledge_gathered": {},
        "plan_complete": False,
        "pod_name": pod_name,
        "namespace": namespace,
        "volume_path": volume_path,
        "knowledge_graph": knowledge_graph
    }
    
    # Run the graph
    logger.info("Running ReAct graph")
    try:
        final_state = graph.invoke(initial_state)
        
        # Extract the investigation plan
        investigation_plan = react_graph.extract_plan_from_state(final_state)
        
        # Log the results
        logger.info("ReAct graph execution completed successfully")
        logger.info(f"Investigation Plan:\n{investigation_plan}")
        
        # Print statistics
        logger.info(f"Iterations: {final_state['iteration_count']}")
        logger.info(f"Tool calls: {final_state['tool_call_count']}")
        logger.info(f"Knowledge gathered: {len(final_state['knowledge_gathered'])}")
        
        # Save the investigation plan to a file
        with open('standalone_react_example_output.txt', 'w') as f:
            f.write(investigation_plan)
        
        # Save the final state to a file
        with open('standalone_react_example_state.json', 'w') as f:
            # Convert messages to a serializable format
            serializable_state = final_state.copy()
            serializable_state['messages'] = [
                {
                    'role': msg.type if hasattr(msg, 'type') else 'unknown',
                    'content': msg.content if hasattr(msg, 'content') else str(msg)
                }
                for msg in final_state['messages']
            ]
            serializable_state['knowledge_graph'] = 'KnowledgeGraph instance (not serializable)'
            json.dump(serializable_state, f, indent=2)
        
        logger.info("Example completed successfully")
        return True
    except Exception as e:
        logger.error(f"Example failed: {e}")
        return False

def print_usage():
    """Print usage information."""
    print("Usage: python standalone_react_example.py")
    print("This script demonstrates how to use the Plan Phase ReAct graph in a standalone manner.")

if __name__ == "__main__":
    print("Standalone ReAct Graph Example")
    print("==============================")
    print("This example demonstrates how to use the Plan Phase ReAct graph directly,")
    print("without going through the full troubleshooting system.")
    print()
    
    # Create examples directory if it doesn't exist
    os.makedirs('examples', exist_ok=True)
    
    asyncio.run(run_standalone_react_example())
</file>

<file path="information_collector/volume_discovery.py">
"""
Volume Dependency Discovery

Handles discovery of volume dependency chains starting from target pods.
"""

import yaml
import logging
import time
from typing import Dict, List, Any
from .base import InformationCollectorBase

# Import LangGraph tools
from tools import kubectl_get


class VolumeDiscovery(InformationCollectorBase):
    """Volume dependency chain discovery functionality"""
    
    def _discover_volume_dependency_chain(self, target_pod: str, target_namespace: str) -> Dict[str, List[str]]:
        """
        Discover the volume dependency chain starting from target pod
        
        Args:
            target_pod: Target pod name
            target_namespace: Target pod namespace
            
        Returns:
            Dict[str, List[str]]: Volume dependency chain (pod -> pvcs -> pvs -> drives -> nodes)
        """
        chain = {
            'pods': [],
            'pvcs': [],
            'pvs': [],
            'volumes': [],
            'lvg': [],
            'drives': [],
            'nodes': [],
            'storage_classes': []
        }
        
        try:
            # Start with target pod
            if target_pod and target_namespace:
                chain['pods'].append(f"{target_namespace}/{target_pod}")
                
                # Get pod details to find PVCs
                pod_output = self._execute_tool_with_validation(
                    kubectl_get, {
                        'resource_type': 'pod',
                        'resource_name': target_pod,
                        'namespace': target_namespace,
                        'output_format': 'yaml'
                    },
                    'kubectl_get_pod', f'Get details for target pod {target_pod}'
                )
                
                # Parse pod output to find PVCs using yaml package
                if pod_output and not pod_output.startswith("Error:"):
                    try:
                        # Parse the YAML output
                        pod_data = yaml.safe_load(pod_output)
                        
                        # Extract PVC names from pod spec
                        if pod_data:
                            # Extract node name
                            node_name = pod_data.get('spec', {}).get('nodeName')
                            if node_name and node_name not in chain['nodes']:
                                chain['nodes'].append(node_name)
                            
                            # Extract volumes and PVCs
                            volumes = pod_data.get('spec', {}).get('volumes', [])
                            for volume in volumes:
                                if volume.get('persistentVolumeClaim', {}).get('claimName'):
                                    pvc_name = volume['persistentVolumeClaim']['claimName']
                                    if pvc_name:
                                        chain['pvcs'].append(f"{target_namespace}/{pvc_name}")
                    except Exception as e:
                        logging.warning(f"Error parsing pod YAML with yaml package: {e}")
                        # Fallback to the old method in case of parsing errors
                        lines = pod_output.split('\n')
                        for line in lines:
                            if 'claimName:' in line:
                                pvc_name = line.split('claimName:')[-1].strip()
                                if pvc_name:
                                    chain['pvcs'].append(f"{target_namespace}/{pvc_name}")
                            if 'nodeName:' in line:
                                node_name = line.split('nodeName:')[-1].strip()
                                if node_name:
                                    chain['nodes'].append(f"{node_name}")
               
                # For each PVC, find bound PV
                for pvc_key in chain['pvcs']:
                    pvc_name = pvc_key.split('/')[-1]
                    pvc_output = self._execute_tool_with_validation(
                        kubectl_get, {
                            'resource_type': 'pvc',
                            'resource_name': pvc_name,
                            'namespace': target_namespace,
                            'output_format': 'yaml'
                        },
                        'kubectl_get_pvc', f'Get PVC details for {pvc_name}'
                    )
                    
                    if pvc_output and not pvc_output.startswith("Error:"):
                        try:
                            # Parse the YAML output
                            pvc_data = yaml.safe_load(pvc_output)
                            
                            if pvc_data:
                                # Extract PV name
                                pv_name = pvc_data.get('spec', {}).get('volumeName')
                                if pv_name:
                                    chain['pvs'].append(pv_name)
                                    chain['volumes'].append(pv_name)
                                
                                # Extract storage class
                                sc_name = pvc_data.get('spec', {}).get('storageClassName')
                                if sc_name and sc_name not in chain['storage_classes']:
                                    chain['storage_classes'].append(sc_name)
                        except Exception as e:
                            logging.warning(f"Error parsing PVC YAML with yaml package: {e}")
                            # Fallback to the old method in case of parsing errors
                            lines = pvc_output.split('\n')
                            for line in lines:
                                if 'volumeName:' in line:
                                    pv_name = line.split('volumeName:')[-1].strip()
                                    if pv_name:
                                        chain['pvs'].append(pv_name)
                                        chain['volumes'].append(pv_name)
                                elif 'storageClassName:' in line:
                                    sc_name = line.split('storageClassName:')[-1].strip()
                                    if sc_name and sc_name not in chain['storage_classes']:
                                        chain['storage_classes'].append(sc_name)
                
                # For each PV, find associated drive and node
                for pv_name in chain['pvs']:
                    pv_output = self._execute_tool_with_validation(
                        kubectl_get, {
                            'resource_type': 'pv',
                            'resource_name': pv_name,
                            'output_format': 'yaml'
                        },
                        'kubectl_get_pv', f'Get PV details for {pv_name}'
                    )

                    if pv_output and not pv_output.startswith("Error:"):
                        try:
                            # Parse the YAML output
                            pv_data = yaml.safe_load(pv_output)
                            
                            if pv_data:
                                # Extract node affinity
                                node_selector = pv_data.get('spec', {}).get('nodeAffinity', {}).get(
                                    'required', {}).get('nodeSelectorTerms', [])
                                
                                if node_selector and len(node_selector) > 0:
                                    for term in node_selector:
                                        for expr in term.get('matchExpressions', []):
                                            if expr.get('key') == 'kubernetes.io/hostname':
                                                for node_name in expr.get('values', []):
                                                    if node_name and node_name not in chain['nodes']:
                                                        chain['nodes'].append(node_name)
                                
                                # Extract drive UUID from annotations
                                annotations = pv_data.get('metadata', {}).get('annotations', {})
                                drive_uuid = annotations.get('csi.storage.k8s.io/volume-id')
                                if drive_uuid and drive_uuid not in chain['drives']:
                                    chain['drives'].append(drive_uuid)
                        except Exception as e:
                            logging.warning(f"Error parsing PV YAML with yaml package: {e}")
                            # Fallback to the old method in case of parsing errors
                            lines = pv_output.split('\n')
                            for line in lines:
                                if 'nodeAffinity:' in line:
                                    # Extract node name from node affinity
                                    node_name = line.split('nodeAffinity:')[-1].strip()
                                    #if node_name and node_name not in chain['nodes']:
                                    #    chain['nodes'].append(node_name)
                                elif 'csi.storage.k8s.io/volume-id:' in line:
                                    # Extract drive UUID from PV annotations
                                    drive_uuid = line.split('csi.storage.k8s.io/volume-id:')[-1].strip()
                                    #if drive_uuid and drive_uuid not in chain['drives']:
                                    #    chain['drives'].append(drive_uuid)

                    vol_output = self._execute_tool_with_validation(
                        kubectl_get, {
                            'resource_type': 'volume',
                            'namespace': target_namespace,
                            'resource_name': pv_name,
                            'output_format': 'yaml'
                        },
                        'kubectl_get_volume', f'Get Volume details for {pv_name}'
                    )
                    
                    if vol_output and not vol_output.startswith("error:") and not vol_output.startswith("Error:"):
                        try:
                            # Parse the YAML output
                            vol_data = yaml.safe_load(vol_output)
                            
                            if vol_data:
                                # Handle both direct object and list of items
                                vol_spec = None
                                if isinstance(vol_data, dict) and 'spec' in vol_data:
                                    vol_spec = vol_data.get('spec', {})
                                elif isinstance(vol_data, dict) and 'items' in vol_data and len(vol_data['items']) > 0:
                                    vol_spec = vol_data['items'][0].get('spec', {})
                                
                                if vol_spec:
                                    # Extract location type and location
                                    locType = vol_spec.get('LocationType')
                                    location = vol_spec.get('Location')
                                    
                                    lvg_name = None
                                    if locType == 'DRIVE' and location:
                                        if location not in chain['drives']:
                                            chain['drives'].append(location)
                                    elif locType == 'LVG' and location:
                                        lvg_name = location
                                        if lvg_name not in chain['lvg']:
                                            chain['lvg'].append(lvg_name)
                        except Exception as e:
                            logging.warning(f"Error parsing Volume YAML with yaml package: {e}")
                            # Fallback to the old method in case of parsing errors
                            lines = vol_output.split('\n')
                            locType = None
                            for line in lines:
                                if 'LocationType:' in line:
                                    locType = line.split('LocationType:')[-1].strip()
                            
                            lvg_name = None
                            if locType and locType == 'DRIVE':
                                for line in lines:
                                    if 'Location:' in line:
                                        drive_name = line.split('Location:')[-1].strip()
                                        if drive_name and drive_name not in chain['drives']:
                                            chain['drives'].append(drive_name)
                            elif locType and locType == 'LVG':
                                for line in lines:
                                    if 'Location:' in line:
                                        lvg_name = line.split('Location:')[-1].strip()
                                        if lvg_name and lvg_name not in chain['lvg']:
                                            chain['lvg'].append(lvg_name)
                        
                        if lvg_name:
                            lvg_output = self._execute_tool_with_validation(
                                kubectl_get, {
                                    'resource_type': 'lvg',
                                    'namespace': target_namespace,
                                    'resource_name': lvg_name,
                                    'output_format': 'yaml'
                                },
                                'kubectl_get_lvg', f'Get LVG details for {lvg_name}'
                            )
                            if lvg_output and not lvg_output.startswith("Error:"):
                                try:
                                    # Parse the YAML output
                                    lvg_data = yaml.safe_load(lvg_output)
                                    
                                    if lvg_data:
                                        # Handle both direct object and list of items
                                        lvg_spec = None
                                        if isinstance(lvg_data, dict) and 'spec' in lvg_data:
                                            lvg_spec = lvg_data.get('spec', {})
                                        elif isinstance(lvg_data, dict) and 'items' in lvg_data and len(lvg_data['items']) > 0:
                                            lvg_spec = lvg_data['items'][0].get('spec', {})
                                        
                                        if lvg_spec:
                                            # Extract locations (drive UUIDs)
                                            locations = lvg_spec.get('Locations', [])
                                            if isinstance(locations, list):
                                                for drive_name in locations:
                                                    if drive_name and drive_name not in chain['drives']:
                                                        chain['drives'].append(drive_name)
                                except Exception as e:
                                    logging.warning(f"Error parsing LVG YAML with yaml package: {e}")
                                    # Fallback to the old method in case of parsing errors
                                    lines = lvg_output.split('\n')
                                    drivestart = False
                                    for line in lines:
                                        if 'Locations:' in line and not drivestart:
                                            drivestart = True
                                        elif drivestart and line.strip() != '':
                                            # - 0fcefbaa-7bc9-49b0-96de-bc8067445497
                                            drive_name = line.strip().lstrip('- ').strip()
                                            if drive_name and drive_name not in chain['drives']:
                                                chain['drives'].append(drive_name)
                                            else:
                                                drivestart = False

        except Exception as e:
            error_msg = f"Error discovering volume dependency chain: {str(e)}"
            logging.error(error_msg)
            self.collected_data['errors'].append(error_msg)
        
        logging.info(f"Volume dependency chain discovered: {len(chain['pvcs'])} PVCs, {len(chain['pvs'])} PVs, {len(chain['drives'])} drives")
        return chain
</file>

<file path="llm_graph/graphs/plan_llm_graph.py">
#!/usr/bin/env python3
"""
Plan Phase LangGraph Implementation for Kubernetes Volume Troubleshooting

This module implements the LangGraph workflow for the Plan phase
of the troubleshooting system using the Strategy Pattern.
"""

import logging
import asyncio
import json
from typing import Dict, List, Any, TypedDict, Optional, Union, Tuple, Set
from enum import Enum
from rich.console import Console
from rich.panel import Panel

from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import tools_condition
from langchain_core.messages import BaseMessage, ToolMessage, HumanMessage, SystemMessage, AIMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import BaseTool

from llm_graph.langgraph_interface import LangGraphInterface
from llm_graph.graph_utility import GraphUtility
from llm_graph.prompt_managers.plan_prompt_manager import PlanPromptManager
from phases.llm_factory import LLMFactory
from phases.utils import handle_exception, generate_basic_fallback_plan
from tools.core.mcp_adapter import get_mcp_adapter
from troubleshooting.execute_tool_node import ExecuteToolNode
from troubleshooting.strategies import ExecutionType
from knowledge_graph import KnowledgeGraph

logger = logging.getLogger(__name__)

# Define hook functions for PlanLLMGraph
def before_call_tools_hook(tool_name: str, args: Dict[str, Any], call_type: str = "Parallel") -> None:
    """Hook function called before a tool is executed in Plan Phase.
    
    Args:
        tool_name: Name of the tool being called
        args: Arguments passed to the tool
        call_type: Type of call execution ("Parallel" or "Serial")
    """
    try:
        # Format arguments for better readability
        formatted_args = json.dumps(args, indent=2) if args else "None"
        
        # Format the tool usage in a nice way
        if formatted_args != "None":
            # Print to console
            tool_panel = Panel(
                f"[bold yellow]Tool:[/bold yellow] [green]{tool_name}[/green] [bold cyan]({call_type})[/bold cyan]\n\n"
                f"[bold yellow]Arguments:[/bold yellow]\n[blue]{formatted_args}[/blue]",
                title="[bold magenta]Plan Phase Tool",
                border_style="magenta",
                safe_box=True
            )
            console = Console()
            console.print(tool_panel)
        else:
            # Simple version for tools without arguments
            tool_panel = Panel(
                f"[bold yellow]Tool:[/bold yellow] [green]{tool_name}[/green] [bold cyan]({call_type})[/bold cyan]\n\n"
                f"[bold yellow]Arguments:[/bold yellow] None",
                title="[bold magenta]Plan Phase Tool",
                border_style="magenta",
                safe_box=True
            )
            console = Console()
            console.print(tool_panel)

        # Log to standard logger
        logger.info(f"Plan Phase executing tool: {tool_name} ({call_type})")
        logger.info(f"Parameters: {formatted_args}")
    except Exception as e:
        logger.error(f"Error in before_call_tools_hook: {e}")

def after_call_tools_hook(tool_name: str, args: Dict[str, Any], result: Any, call_type: str = "Parallel") -> None:
    """Hook function called after a tool is executed in Plan Phase.
    
    Args:
        tool_name: Name of the tool that was called
        args: Arguments that were passed to the tool
        result: Result returned by the tool
        call_type: Type of call execution ("Parallel" or "Serial")
    """
    try:
        # Format result for better readability
        if isinstance(result, ToolMessage):
            result_content = result.content
            result_status = result.status if hasattr(result, 'status') else 'success'
            formatted_result = f"Status: {result_status}\nContent: {result_content[:1000]}"
        else:
            formatted_result = str(result)[:1000]
        
        # Print tool result to console
        tool_panel = Panel(
            f"[bold cyan]Tool completed:[/bold cyan] [green]{tool_name}[/green] [bold cyan]({call_type})[/bold cyan]\n"
            f"[bold cyan]Result:[/bold cyan]\n[yellow]{formatted_result}[/yellow]",
            title="[bold magenta]Plan Phase Result",
            border_style="magenta",
            safe_box=True
        )
        console = Console()
        console.print(tool_panel)

        # Log to standard logger
        logger.info(f"Plan Phase tool completed: {tool_name} ({call_type})")
        logger.info(f"Result: {formatted_result}")
    except Exception as e:
        logger.error(f"Error in after_call_tools_hook: {e}")

class PlanPhaseState(TypedDict):
    """State for the Plan Phase ReAct graph"""
    messages: List[BaseMessage]  # Conversation history
    iteration_count: int  # Track iterations
    tool_call_count: int  # Track tool calls
    knowledge_gathered: Dict[str, Any]  # Knowledge gathered from tools
    plan_complete: bool  # Whether the plan is complete
    pod_name: str  # Pod name for context
    namespace: str  # Namespace for context
    volume_path: str  # Volume path for context
    knowledge_graph: Optional[KnowledgeGraph]  # Knowledge graph for context

class ReActStage(Enum):
    """Stages in the ReAct process"""
    REASONING = "reasoning"  # LLM analyzing and reasoning about the problem
    ACTING = "acting"  # Calling tools to gather information
    OBSERVING = "observing"  # Processing tool outputs
    PLANNING = "planning"  # Generating the final plan

class PlanLLMGraph(LangGraphInterface):
    """
    LangGraph implementation for the Plan phase
    
    Implements the LangGraphInterface for the Plan phase,
    which generates an Investigation Plan for Phase 1.
    """
    
    def __init__(self, config_data: Dict[str, Any] = None, messages: List[BaseMessage] = None):
        """
        Initialize the Plan LLM Graph
        
        Args:
            config_data: Configuration data for the system
            messages: Initial messages for the graph (optional)
        """
        self.config_data = config_data or {}
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.graph_utility = GraphUtility(config_data)
        self.prompt_manager = PlanPromptManager(config_data)
        
        # Initialize LLM
        self.llm = self._initialize_llm()
        
        # Get MCP tools
        self.mcp_tools = self._get_mcp_tools_for_plan_phase()
        
        # Maximum iterations to prevent infinite loops
        self.max_iterations = self.config_data.get("max_iterations", 15)
        
        # Store initial messages
        self.init_messages = messages
    
    def _initialize_llm(self):
        """
        Initialize the LLM for the ReAct graph
        
        Returns:
            BaseChatModel: Initialized LLM instance
        """
        try:
            # Create LLM using the factory
            llm_factory = LLMFactory(self.config_data)
            
            # Check if streaming is enabled in config
            streaming_enabled = self.config_data.get('llm', {}).get('streaming', False)
            
            # Create LLM with streaming if enabled
            return llm_factory.create_llm(
                streaming=streaming_enabled,
                phase_name="plan_phase"
            )
        except Exception as e:
            error_msg = handle_exception("_initialize_llm", e, self.logger)
            raise ValueError(f"Failed to initialize LLM: {error_msg}")
    
    def _get_mcp_tools_for_plan_phase(self) -> List[BaseTool]:
        """
        Get MCP tools for the plan phase
        
        Returns:
            List[BaseTool]: List of MCP tools for the plan phase
        """
        # Get MCP adapter
        mcp_adapter = get_mcp_adapter()
        
        if not mcp_adapter:
            self.logger.warning("MCP adapter not initialized, no MCP tools will be available")
            return []
        
        if not mcp_adapter.mcp_enabled:
            self.logger.warning("MCP integration is disabled, no MCP tools will be available")
            return []
        
        # Get MCP tools for plan phase
        mcp_tools = mcp_adapter.get_tools_for_phase('plan_phase')
        
        if not mcp_tools:
            self.logger.warning("No MCP tools available for plan phase")
            return []
        
        self.logger.info(f"Loaded {len(mcp_tools)} MCP tools for Plan Phase")
        return mcp_tools
    
    def initialize_graph(self) -> StateGraph:
        """
        Initialize and return the LangGraph StateGraph
        
        Returns:
            StateGraph: Compiled LangGraph StateGraph
        """
        # Build state graph
        self.logger.info("Building Plan Phase ReAct graph")
        builder = StateGraph(PlanPhaseState)
        
        # Add nodes
        self.logger.info("Adding node: call_model")
        builder.add_node("call_model", self.call_model)
        
        # Initialize ExecuteToolNode with all MCP tools set to parallel execution
        execute_tools_node = self._initialize_execute_tool_node()
        self.logger.info("Adding node: execute_tools")
        builder.add_node("execute_tools", execute_tools_node)
        
        self.logger.info("Adding node: check_end")
        builder.add_node("check_end", self.check_end_conditions)
        
        # Add edges
        self.logger.info("Adding edge: START -> call_model")
        builder.add_edge(START, "call_model")
        
        # Add conditional edges for tools
        self.logger.info("Adding conditional edges for tools")
        builder.add_conditional_edges(
            "call_model",
            tools_condition,
            {
                "tools": "execute_tools",   # Route to execute_tools node
                "none": "check_end",        # If no tools, go to check_end
                "__end__": "check_end"
            }
        )
        
        # Add edge from execute_tools to call_model
        self.logger.info("Adding edge: execute_tools -> call_model")
        builder.add_edge("execute_tools", "call_model")
        
        # Add conditional edges from check_end node
        self.logger.info("Adding conditional edges from check_end node")
        builder.add_conditional_edges(
            "check_end",
            lambda state: self.check_end_conditions(state)["result"],
            {
                "end": END,
                "__end__": END,
                "continue": "call_model"  # Loop back if conditions not met
            }
        )
        
        # Compile graph
        self.logger.info("Compiling graph")
        return builder.compile()
    
    def call_model(self, state: PlanPhaseState) -> PlanPhaseState:
        """
        LLM reasoning node that analyzes current state and decides next action
        
        Args:
            state: Current state of the graph
            
        Returns:
            PlanPhaseState: Updated state after LLM reasoning
        """
        # Increment iteration count
        state["iteration_count"] += 1
        
        # Check if we've reached max iterations
        if state["iteration_count"] > self.max_iterations:
            self.logger.info(f"Reached max iterations ({self.max_iterations}), marking plan as complete")
            state["plan_complete"] = True
            
            # Add a final message indicating max iterations reached
            final_message = AIMessage(content=f"[MAX_ITERATIONS_REACHED] Completed {self.max_iterations} iterations. Finalizing plan with current information.")
            state["messages"].append(final_message)
            
            return state
        
        self.logger.info(f"Calling model (iteration {state['iteration_count']})")
        
        try:
            # Prepare messages if this is the first iteration or if messages need to be initialized
            state = self._prepare_messages(state)
            
            # Call the model with tools
            response = None
            if len(self.mcp_tools) != 0:
                response = self.llm.bind_tools(self.mcp_tools).invoke(state["messages"])
            else:
                response = self.llm.invoke(state["messages"])
            
            # Add response to messages
            state["messages"].append(response)
            
            # Log the response
            self.logger.info(f"Model response: {response.content[:100]}...")
            
            return state
        except Exception as e:
            error_msg = handle_exception("call_model", e, self.logger)
            
            # Add error message to state
            error_message = SystemMessage(content=f"Error calling model: {error_msg}")
            state["messages"].append(error_message)
            
            # Mark plan as complete to exit the loop
            state["plan_complete"] = True
            
            return state
    
    def _initialize_execute_tool_node(self):
        """
        Initialize ExecuteToolNode with all MCP tools set to parallel execution
        
        Returns:
            ExecuteToolNode: Configured ExecuteToolNode instance
        """
        if not self.mcp_tools:
            self.logger.warning("No MCP tools available for ExecuteToolNode")
            # Return empty ExecuteToolNode if no tools available
            return ExecuteToolNode(
                tools=[],
                parallel_tools=set(),
                serial_tools=set()
            )
        
        # Get all tool names
        tool_names = {tool.name for tool in self.mcp_tools}
        
        # Configure all MCP tools to run in parallel by default
        parallel_tools = tool_names
        serial_tools = set()
        
        self.logger.info(f"Configuring ExecuteToolNode with {len(parallel_tools)} parallel tools")
        
        # Create ExecuteToolNode with all tools set to parallel execution
        execute_tool_node = ExecuteToolNode(
            tools=self.mcp_tools,
            parallel_tools=parallel_tools,
            serial_tools=serial_tools,
            handle_tool_errors=True,
            messages_key="messages"
        )
        
        # Create a hook manager for console output
        from troubleshooting.hook_manager import HookManager
        
        console = Console()
        file_console = Console(file=open('plan_phase.log', 'a'))
        hook_manager = HookManager(console=console, file_console=file_console)
        
        # Register custom hook functions with the hook manager
        hook_manager.register_before_call_hook(before_call_tools_hook)
        hook_manager.register_after_call_hook(after_call_tools_hook)
        
        # Register hook manager with the ExecuteToolNode
        execute_tool_node.register_before_call_hook(hook_manager.run_before_hook)
        execute_tool_node.register_after_call_hook(hook_manager.run_after_hook)
        
        return execute_tool_node
    
    def check_end_conditions(self, state: PlanPhaseState) -> Dict[str, str]:
        """
        Check if plan generation is complete
        
        Args:
            state: Current state of the graph
            
        Returns:
            Dict[str, str]: Result indicating whether to end or continue
        """
        self.logger.info("Checking end conditions for Plan Phase ReAct graph")

        # If plan is already marked as complete, end the graph
        if state["plan_complete"]:
            self.logger.info("Plan marked as complete, ending graph")
            return {"result": "end"}
        
        # Check if we've reached max iterations
        if state["iteration_count"] >= self.max_iterations:
            self.logger.info(f"Reached max iterations ({self.max_iterations}), ending graph")
            return {"result": "end"}
        
        # Get the last message
        messages = state["messages"]
        if not messages:
            return {"result": "continue"}
        
        last_message = messages[-1]
        
        # Skip content checks if the last message isn't from the AI
        if getattr(last_message, "type", "") != "ai":
            return {"result": "continue"}
        
        content = getattr(last_message, "content", "")
        if not content:
            return {"result": "continue"}
        
        # Check for explicit end markers using LLM
        llm_end_markers = self._check_explicit_end_markers(content)
        if llm_end_markers:
            self.logger.info("Detected end markers from LLM, ending graph")
            state["plan_complete"] = True
            return {"result": "end"}
            
        # Check for explicit end markers in the content
        end_markers = ["[END_GRAPH]", "[END]", "End of graph", "GRAPH END", "Investigation Plan:", "Fix Plan:", "Step by Step"]
        if any(marker in content for marker in end_markers):
            self.logger.info(f"Detected end marker in content, ending graph")
            state["plan_complete"] = True
            return {"result": "end"}
        
        # Check for completion indicators
        if "Summary of Findings:" in content and "Root Cause:" in content and "Fix Plan:" in content:
            self.logger.info("Detected completion indicators in content, ending graph")
            state["plan_complete"] = True
            return {"result": "end"}
        
        # Check for convergence (model repeating itself)
        ai_messages = [m for m in messages if getattr(m, "type", "") == "ai"]
        if len(ai_messages) > 3:
            # Compare the last message with the third-to-last message
            last_content = content
            third_to_last_content = getattr(ai_messages[-3], "content", "")
            
            # Simple similarity check - if they start with the same paragraph
            if last_content and third_to_last_content:
                # Get first 100 chars of each message
                last_start = last_content[:100] if len(last_content) > 100 else last_content
                third_start = third_to_last_content[:100] if len(third_to_last_content) > 100 else third_to_last_content
                
                if last_start == third_start:
                    self.logger.info("Detected convergence (model repeating itself), ending graph")
                    state["plan_complete"] = True
                    return {"result": "end"}
        
        # Default: continue execution
        return {"result": "continue"}
    
    def extract_plan_from_state(self, state: PlanPhaseState) -> str:
        """
        Extract the investigation plan from the final state
        
        Args:
            state: Final state of the graph
            
        Returns:
            str: Investigation plan as a formatted string
        """
        # Get the last AI message
        ai_messages = [m for m in state["messages"] if getattr(m, "type", "") == "ai"]
        
        if not ai_messages:
            self.logger.warning("No AI messages found in final state")
            return generate_basic_fallback_plan(
                state["pod_name"], state["namespace"], state["volume_path"]
            )
        
        # Get the content of the last AI message
        last_ai_message = ai_messages[-1]
        content = getattr(last_ai_message, "content", "")
        
        # Extract the investigation plan
        if "Investigation Plan:" in content:
            plan_index = content.find("Investigation Plan:")
            plan = content[plan_index:]
            
            # Remove end markers
            for marker in ["[END_GRAPH]", "[END]", "End of graph", "GRAPH END"]:
                plan = plan.replace(marker, "")
                
            return plan.strip()
        
        # If no plan found, return the entire content
        return content.strip()
    
    async def execute(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Execute the graph with the provided state
        
        Args:
            state: Initial state for the graph execution
            
        Returns:
            Dict[str, Any]: Final state after graph execution
        """
        try:
            # Initialize the graph
            graph = self.initialize_graph()
            
            # Prepare initial state
            initial_state = {
                "messages": state.get("messages", []),
                "iteration_count": 0,
                "tool_call_count": 0,
                "knowledge_gathered": {},
                "plan_complete": False,
                "pod_name": state.get("pod_name", ""),
                "namespace": state.get("namespace", ""),
                "volume_path": state.get("volume_path", ""),
                "knowledge_graph": state.get("knowledge_graph", None)
            }
            
            # Run the graph
            self.logger.info("Running Plan Phase ReAct graph")
            final_state = graph.invoke(initial_state)
            
            # Extract the investigation plan
            investigation_plan = self.extract_plan_from_state(final_state)
            
            # Add the investigation plan to the final state
            final_state["investigation_plan"] = investigation_plan
            
            return final_state
            
        except Exception as e:
            error_msg = handle_exception("execute", e, self.logger)
            fallback_plan = generate_basic_fallback_plan(
                state.get("pod_name", ""), 
                state.get("namespace", ""), 
                state.get("volume_path", "")
            )
            
            return {
                "status": "error",
                "error_message": error_msg,
                "investigation_plan": fallback_plan,
                "messages": state.get("messages", [])
            }
    
    def _prepare_messages(self, state: PlanPhaseState) -> PlanPhaseState:
        """
        Prepare messages for the LLM with system prompt and context
        
        Args:
            state: Current state with messages
            
        Returns:
            PlanPhaseState: Updated state with prepared messages
        """
        user_messages = []
        if state["messages"]:
            if isinstance(state["messages"], list):
                for msg in state["messages"]:
                    if not isinstance(msg, SystemMessage) and not isinstance(msg, HumanMessage):
                        user_messages.append(msg)
                
                # Create new message list with system message, context message, and existing user messages
                state["messages"] = self.init_messages + user_messages
            else:
                state["messages"] = [self.init_messages, state["messages"]]
        else:
            state["messages"] = self.init_messages

        self.logger.info("Prepared initial messages with system prompt and user query")
        return state
    
    def _check_explicit_end_markers(self, content: str) -> bool:
        """
        Use LLM to check if content contains explicit or implicit end markers.
        
        Args:
            content: The content to check for end markers
            
        Returns:
            bool: True if end markers detected, False otherwise
        """
        # Create a focused prompt for the LLM
        system_prompt = """
        You are an AI assistant tasked with determining if a text contains explicit or implicit markers 
        indicating the end of a process or conversation. Your task is to analyze the given text and 
        determine if it contains phrases or markers that suggest completion or termination.
        
        Examples of explicit end markers include:
        - "[END_GRAPH]", "[END]", "End of graph", "GRAPH END"
        - "This concludes the analysis"
        - "Final report"
        - "Step by Step"
        - "Step XXX: [Description and Reason] | Tool: [tool_name(parameters)] | Expected: [expected]"
        - "Investigation complete"
        - " Would you like to"
        - A question from AI that indicates the end of the process, such as " Would you like to proceed with planning the disk replacement or further investigate filesystem integrity?"
        - If just a call tools result, then return 'NO'

        Examples of implicit end markers include:
        - A summary followed by recommendations with no further questions
        - A conclusion paragraph that wraps up all findings
        - A complete analysis with all required sections present
        - A question from AI that indicates the end of the process, such as "Is there anything else I can help you with?" or "Do you have any further questions?"
        
        Respond with "YES" if you detect end markers, or "NO" if you don't.
        """
        
        user_prompt = f"""
        Analyze the following text and determine if it contains explicit or implicit end markers:
        
        {content[:1000]}  # Limit content length to avoid token limits
        
        Does this text contain markers indicating it's the end of the process? Respond with only YES or NO.
        """
        
        try:
            # Create messages for the LLM
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            
            # Call the LLM
            response = self.llm.invoke(messages)
            
            # Check if the response indicates end markers
            response_text = response.content.strip().upper()
            
            # Log the LLM's response
            logger.info(f"LLM end marker detection response: {response_text}")
            
            # Return True if the LLM detected end markers
            return "YES" in response_text
        except Exception as e:
            # Log any errors and fall back to the original behavior
            logger.error(f"Error in LLM end marker detection: {e}")
            
            # Fall back to simple string matching
            return any(marker in content for marker in ["[END_GRAPH]", "[END]", "End of graph", "GRAPH END", "Fix Plan", "FIX PLAN"])
    
    def get_prompt_manager(self):
        """
        Return the prompt manager for this graph
        
        Returns:
            PlanPromptManager: Prompt manager for the Plan phase
        """
        return self.prompt_manager
</file>

<file path="monitoring/monitor.py">
#!/usr/bin/env python3
"""
Kubernetes Volume I/O Error Monitoring Script

This script monitors all pods in a Kubernetes cluster for volume I/O errors
by checking for the 'volume-io-error' annotation. When an error is detected,
it invokes the troubleshooting workflow.
"""

import os
import time
import yaml
import logging
import subprocess
import sys
import json
import tempfile
import glob
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# Dictionary to track ongoing troubleshooting processes
# Key: "{namespace}/{pod_name}/{volume_path}", Value: (process, start_time)
active_troubleshooting = {}

# Directory where troubleshooting results are stored
RESULTS_DIR = os.path.join(tempfile.gettempdir(), "k8s-troubleshooting-results")

def load_config():
    """Load configuration from config.yaml"""
    try:
        with open('config.yaml', 'r') as f:
            return yaml.safe_load(f)
    except Exception as e:
        logging.error(f"Failed to load configuration: {e}")
        sys.exit(1)

def setup_logging(config_data):
    """Configure logging based on configuration"""
    log_file = config_data['logging']['file']
    log_to_stdout = config_data['logging']['stdout']
    
    handlers = []
    if log_file:
        handlers.append(logging.FileHandler(log_file))
    if log_to_stdout:
        handlers.append(logging.StreamHandler())
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=handlers
    )

def init_kubernetes_client():
    """Initialize Kubernetes client"""
    try:
        # Try to load in-cluster config first (when running inside a pod)
        if 'KUBERNETES_SERVICE_HOST' in os.environ:
            config.load_incluster_config()
            logging.info("Using in-cluster Kubernetes configuration")
        else:
            # Fall back to kubeconfig file
            config.load_kube_config()
            logging.info("Using kubeconfig file for Kubernetes configuration")
        
        return client.CoreV1Api()
    except Exception as e:
        logging.error(f"Failed to initialize Kubernetes client: {e}")
        sys.exit(1)

def add_troubleshooting_result_annotation(kube_client, pod_name, namespace, result_summary):
    """
    Add the troubleshooting result as an annotation to a pod
    
    Args:
        kube_client: Kubernetes API client
        pod_name: Name of the pod
        namespace: Namespace of the pod
        result_summary: Summary of the investigation result
    
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        # Get the current pod
        pod = kube_client.read_namespaced_pod(name=pod_name, namespace=namespace)
        
        # Ensure annotations dictionary exists
        if not pod.metadata.annotations:
            pod.metadata.annotations = {}
        
        # Add the result annotation
        pod.metadata.annotations['volume-io-troubleshooting-result'] = result_summary
        
        # Update the pod
        kube_client.patch_namespaced_pod(
            name=pod_name,
            namespace=namespace,
            body={"metadata": {"annotations": pod.metadata.annotations}}
        )
        
        logging.info(f"Successfully added troubleshooting result annotation to pod {namespace}/{pod_name}")
        return True
    except ApiException as e:
        logging.error(f"Kubernetes API error while adding result annotation: {e}")
        return False
    except Exception as e:
        logging.error(f"Unexpected error while adding result annotation: {e}")
        return False

def remove_volume_io_error_annotation(kube_client, pod_name, namespace):
    """
    Remove the 'volume-io-error' annotation from a pod
    
    Args:
        kube_client: Kubernetes API client
        pod_name: Name of the pod
        namespace: Namespace of the pod
    
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        # Get the current pod
        pod = kube_client.read_namespaced_pod(name=pod_name, namespace=namespace)
        
        # If the pod has no annotations or no volume-io-error annotation, nothing to do
        if not pod.metadata.annotations or 'volume-io-error' not in pod.metadata.annotations:
            logging.debug(f"No 'volume-io-error' annotation found on pod {namespace}/{pod_name}")
            return True
        
        # Create a JSON patch to explicitly remove the annotation
        patch_body = {
            "metadata": {
                "annotations": {
                    "volume-io-error": None  # Setting to None explicitly removes the key
                }
            }
        }
        
        # Update the pod with explicit removal
        kube_client.patch_namespaced_pod(
            name=pod_name,
            namespace=namespace,
            body=patch_body
        )
        
        # Verify the annotation was actually removed
        updated_pod = kube_client.read_namespaced_pod(name=pod_name, namespace=namespace)
        if updated_pod.metadata.annotations and 'volume-io-error' in updated_pod.metadata.annotations:
            logging.warning(f"Failed to remove 'volume-io-error' annotation from pod {namespace}/{pod_name} - annotation still exists after patch")
            return False
            
        logging.info(f"Successfully removed 'volume-io-error' annotation from pod {namespace}/{pod_name}")
        return True
    except ApiException as e:
        logging.error(f"Kubernetes API error while removing annotation: {e}")
        return False
    except Exception as e:
        logging.error(f"Unexpected error while removing annotation: {e}")
        return False

def find_troubleshooting_result(namespace, pod_name, volume_path):
    """
    Find the troubleshooting result file for a specific pod and volume
    
    Args:
        namespace: Namespace of the pod
        pod_name: Name of the pod
        volume_path: Path of the volume
    
    Returns:
        tuple: (result_summary, filepath) or (None, None) if not found
    """
    try:
        # Create the expected filename pattern
        filename = f"{namespace}_{pod_name}_{volume_path.replace('/', '_')}.json"
        filepath = os.path.join(RESULTS_DIR, filename)
        
        # Check if the file exists
        if os.path.exists(filepath):
            # Read the file
            with open(filepath, 'r') as f:
                result_data = json.load(f)
                
            # Return the result summary
            return result_data.get('result_summary'), filepath
    except Exception as e:
        logging.error(f"Error reading troubleshooting result file: {e}")
    
    return None, None

def check_completed_troubleshooting(kube_client):
    """
    Check for completed troubleshooting processes and clean up
    
    Args:
        kube_client: Kubernetes API client
    """
    global active_troubleshooting
    
    # List of keys to remove
    completed = []
    
    # Check each active troubleshooting process
    for key, (process, _) in active_troubleshooting.items():
        # Check if process has completed (poll() returns None if still running)
        if process.poll() is not None:
            # Process has completed
            namespace, pod_name, volume_path = key.split('/', 2)
            
            # Look for troubleshooting result
            result_summary, result_filepath = find_troubleshooting_result(namespace, pod_name, volume_path)
            
            # Add the result as an annotation if found
            if result_summary:
                if add_troubleshooting_result_annotation(kube_client, pod_name, namespace, result_summary):
                    logging.info(f"Added troubleshooting result annotation to pod {namespace}/{pod_name}")
                    
                    # Remove the result file
                    try:
                        os.remove(result_filepath)
                        logging.debug(f"Removed result file {result_filepath}")
                    except Exception as e:
                        logging.warning(f"Failed to remove result file {result_filepath}: {e}")
                else:
                    logging.warning(f"Failed to add troubleshooting result annotation to pod {namespace}/{pod_name}")
            
            # Remove the error annotation
            if remove_volume_io_error_annotation(kube_client, pod_name, namespace):
                logging.info(f"Troubleshooting completed for {key}, annotation removed")
            else:
                logging.warning(f"Failed to remove annotation for {key} after troubleshooting completed")
            
            # Mark for removal
            completed.append(key)
    
    # Remove completed processes from tracking
    for key in completed:
        del active_troubleshooting[key]
        logging.debug(f"Removed {key} from active troubleshooting tracking")

def monitor_pods(kube_client, config_data):
    """
    Monitor all pods for volume I/O errors
    
    Args:
        kube_client: Kubernetes API client
        config_data: Configuration data from config.yaml
    """
    retry_count = 0
    max_retries = config_data['monitor']['api_retries']
    backoff_seconds = config_data['monitor']['retry_backoff_seconds']
    
    # First, check for any completed troubleshooting processes
    check_completed_troubleshooting(kube_client)
    
    while retry_count <= max_retries:
        try:
            pods = kube_client.list_pod_for_all_namespaces(watch=False)
            logging.debug(f"Found {len(pods.items)} pods in the cluster")
            
            for pod in pods.items:
                if pod.metadata.annotations and 'volume-io-error' in pod.metadata.annotations:
                    volume_path = pod.metadata.annotations['volume-io-error']
                    pod_name = pod.metadata.name
                    namespace = pod.metadata.namespace
                    
                    logging.info(f"Detected volume I/O error in pod {namespace}/{pod_name} at path {volume_path}")
                    invoke_troubleshooting(kube_client, pod_name, namespace, volume_path)
            
            # If we get here, the API call was successful
            return
            
        except ApiException as e:
            retry_count += 1
            if retry_count <= max_retries:
                wait_time = backoff_seconds * (2 ** (retry_count - 1))  # Exponential backoff
                logging.warning(f"Kubernetes API error: {e}. Retrying in {wait_time} seconds (attempt {retry_count}/{max_retries})")
                time.sleep(wait_time)
            else:
                logging.error(f"Failed to monitor pods after {max_retries} retries: {e}")
                return
        except Exception as e:
            logging.error(f"Unexpected error monitoring pods: {e}")
            return

def invoke_troubleshooting(kube_client, pod_name, namespace, volume_path):
    """
    Invoke the troubleshooting workflow for a pod with volume I/O error
    
    Args:
        kube_client: Kubernetes API client
        pod_name: Name of the pod with the error
        namespace: Namespace of the pod
        volume_path: Path of the volume with I/O error
    """
    global active_troubleshooting
    
    # Create a unique key for this volume
    key = f"{namespace}/{pod_name}/{volume_path}"
    
    # Check if troubleshooting is already in progress for this volume
    if key in active_troubleshooting:
        process, start_time = active_troubleshooting[key]
        
        # Check if process is still running
        if process.poll() is None:
            # Process is still running
            elapsed = time.time() - start_time
            logging.info(f"Troubleshooting already in progress for {key} (started {elapsed:.1f} seconds ago)")
            return
        else:
            # Process has completed but wasn't cleaned up
            logging.info(f"Previous troubleshooting for {key} completed, removing annotation and tracking")
            remove_volume_io_error_annotation(kube_client, pod_name, namespace)
            del active_troubleshooting[key]
    
    try:
        cmd = ["python3", "troubleshooting/troubleshoot.py", pod_name, namespace, volume_path]
        logging.info(f"Invoking troubleshooting: {' '.join(cmd)}")
        
        # Use Popen to run the troubleshooting script in the background
        process = subprocess.Popen(cmd)
        
        # Track the process
        active_troubleshooting[key] = (process, time.time())
        
        logging.info(f"Troubleshooting workflow started for pod {namespace}/{pod_name}, volume {volume_path}")
        logging.info(f"Two-phase process will run: Analysis followed by Remediation (if approved or auto_fix is enabled)")
    except Exception as e:
        logging.error(f"Failed to invoke troubleshooting: {e}")

def ensure_results_dir():
    """Ensure the results directory exists"""
    try:
        if not os.path.exists(RESULTS_DIR):
            os.makedirs(RESULTS_DIR)
            logging.debug(f"Created results directory: {RESULTS_DIR}")
    except Exception as e:
        logging.error(f"Failed to create results directory: {e}")

def main():
    """Main function"""
    # Load configuration
    config_data = load_config()
    
    # Set up logging
    setup_logging(config_data)
    
    # Ensure results directory exists
    ensure_results_dir()
    
    logging.info("Starting Kubernetes volume I/O error monitoring")
    
    # Initialize Kubernetes client
    kube_client = init_kubernetes_client()
    
    # Get monitoring interval
    interval = config_data['monitor']['interval_seconds']
    logging.info(f"Monitoring interval: {interval} seconds")
    
    # Log troubleshooting mode settings
    interactive_mode = config_data['troubleshoot']['interactive_mode']
    auto_fix = config_data['troubleshoot']['auto_fix']
    logging.info(f"Troubleshooting settings: interactive_mode={interactive_mode}, auto_fix={auto_fix}")
    
    # Main monitoring loop
    try:
        while True:
            monitor_pods(kube_client, config_data)
            # Log active troubleshooting count
            if active_troubleshooting:
                logging.debug(f"Active troubleshooting processes: {len(active_troubleshooting)}")
            time.sleep(interval)
    except KeyboardInterrupt:
        logging.info("Monitoring stopped by user")
    except Exception as e:
        logging.error(f"Fatal error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
</file>

<file path="phases/chat_mode.py">
#!/usr/bin/env python3
"""
Chat Mode for Kubernetes Volume Troubleshooting

This module implements the chat mode functionality for the troubleshooting system,
allowing users to approve plans, provide instructions, or exit at two entry points:
1. After Plan Phase: Review and refine the Investigation Plan
2. After Phase1: Review and refine the Fix Plan
"""

import sys
import logging
from typing import Dict, List, Any, Tuple
from rich.console import Console

logger = logging.getLogger(__name__)

class ChatMode:
    """
    Implements chat mode functionality for the troubleshooting system
    
    This class provides methods for entering chat mode at two entry points:
    1. After Plan Phase: Review and refine the Investigation Plan
    2. After Phase1: Review and refine the Fix Plan
    
    The chat mode allows users to:
    - Approve plans to proceed to the next phase
    - Provide instructions to refine the plans
    - Exit the program
    """
    
    def __init__(self):
        """Initialize the Chat Mode"""
        self.console = Console()
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def chat_after_plan_phase(self, message_list: List[Dict[str, str]], investigation_plan: str) -> Tuple[List[Dict[str, str]], bool]:
        """
        Enter chat mode after Plan Phase to review and refine the Investigation Plan
        
        Args:
            message_list: Message list for the Plan Phase
            investigation_plan: Generated Investigation Plan
            
        Returns:
            Tuple[List[Dict[str, str]], bool]: (Updated message list, Exit flag)
        """
        self.logger.info("Entering chat mode after Plan Phase")
        
        # Display prompt
        self.console.print("[bold cyan]Please review the Investigation Plan. Enter '[green]approve[/green]' to proceed to Phase1, provide new instructions to refine the plan, or enter '[red]exit[/red]' to terminate the program.[/bold cyan]")
        
        # Get user input
        user_input = input("User Input: ").strip()
        
        # Process user input
        if user_input.lower() == "approve":
            self.logger.info("User approved the Investigation Plan")
            return message_list, False
        elif user_input.lower() == "exit":
            self.logger.info("User requested to exit the program")
            return message_list, True
        else:
            if user_input == '':
                self.logger.info("User requested to exit the program")
                return message_list, True

            self.logger.info(f"User provided instructions: {user_input}")
            
            # Add user input to message list
            if message_list is None:
                # Initialize message list with system prompt and user input
                system_prompt = """You are an expert Kubernetes storage troubleshooter. Your task is to refine a draft Investigation Plan for troubleshooting volume read/write errors in Kubernetes.

TASK:
1. Review the draft plan containing preliminary steps from rule-based analysis and mandatory static steps
2. Analyze the Knowledge Graph and historical experience data
3. Refine the plan by:
   - Respecting existing steps (do not remove or modify static steps)
   - Adding necessary additional steps using only the provided Phase1 tools
   - Reordering steps if needed for logical flow
   - Adding fallback steps for error handling

CONSTRAINTS:
- You must NOT invoke any tools - only reference them in your plan
- You must include all static steps from the draft plan without modification
- You must only reference tools available in the Phase1 tool registry
- All tool references must match the exact name and parameter format shown in the tools registry

OUTPUT FORMAT:
Your response must be a refined Investigation Plan with steps in this format:
Step X: [Description] | Tool: [tool_name(parameters)] | Expected: [expected]

You may include fallback steps for error handling in this format:
Fallback Steps (if main steps fail):
Step FX: [Description] | Tool: [tool_name(parameters)] | Expected: [expected] | Trigger: [failure_condition]

The plan must be comprehensive, logically structured, and include all necessary steps to investigate the volume I/O errors.
"""
                message_list = [
                    {"role": "system", "content": system_prompt},
                    {"role": "assistant", "content": investigation_plan},
                    {"role": "user", "content": f"User Input: {user_input}"}
                ]
            else:
                # Add user input to existing message list
                message_list.append({"role": "user", "content": f"User Input: {user_input}"})
            
            return message_list, False
    
    def chat_after_phase1(self, message_list: List[Dict[str, str]], fix_plan: str) -> Tuple[List[Dict[str, str]], bool]:
        """
        Enter chat mode after Phase1 to review and refine the Fix Plan
        
        Args:
            message_list: Message list for Phase1
            fix_plan: Generated Fix Plan
            
        Returns:
            Tuple[List[Dict[str, str]], bool]: (Updated message list, Exit flag)
        """
        self.logger.info("Entering chat mode after Phase1")
        
        # Display prompt
        self.console.print("[bold cyan]Need user approval for the Fix Plan. Enter '[green]approve[/green]' to proceed to Phase2, provide new instructions to refine the plan, or enter '[red]exit[/red]' to terminate the program.[/bold cyan]")
        
        # Get user input
        user_input = input("User Input: ").strip()
        
        # Process user input
        if user_input.lower() == "approve":
            self.logger.info("User approved the Fix Plan")
            return message_list, False
        elif user_input.lower() == "exit":
            self.logger.info("User requested to exit the program")
            return message_list, True
        else:
            if user_input == '':
                self.logger.info("User requested to exit the program")
                return message_list, True

            self.logger.info(f"User provided instructions: {user_input}")
            
            # Add user input to message list
            if message_list is None:
                # Initialize message list with system prompt and user input
                system_prompt = """You are an expert Kubernetes storage troubleshooter. Your task is to execute the Investigation Plan to actively investigate volume I/O errors in Kubernetes pods and generate a comprehensive Fix Plan.

TASK:
1. Execute the Investigation Plan to identify the root cause of volume I/O errors
2. Analyze the results of the investigation
3. Generate a comprehensive Fix Plan to resolve the identified issues

CONSTRAINTS:
- Follow the Investigation Plan step by step
- Use only the tools available in the Phase1 tool registry
- Provide a detailed root cause analysis
- Generate a clear, actionable Fix Plan

OUTPUT FORMAT:
Your response must include:
1. Summary of Findings
2. Detailed Analysis
3. Root Cause
4. Fix Plan
"""
                message_list = [
                    {"role": "system", "content": system_prompt},
                    {"role": "assistant", "content": fix_plan},
                    {"role": "user", "content": f"User Input: {user_input}"}
                ]
            else:
                # Add user input to existing message list
                message_list.append({"role": "user", "content": f"User Input: {user_input}"})
            
            return message_list, False
</file>

<file path="phases/llm_factory.py">
#!/usr/bin/env python3
"""
LLM Factory for Multiple Provider Support

This module provides a factory for initializing different LLM providers
(OpenAI, Google Gemini, Ollama) based on configuration.
"""

import logging
from typing import Dict, Any, Optional
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import SystemMessage, HumanMessage

logger = logging.getLogger(__name__)

class LLMFactory:
    """
    Factory class for creating LLM instances based on provider configuration
    
    Supports multiple LLM providers:
    - OpenAI (ChatGPT)
    - Google (Gemini)
    - Ollama (Local models)
    """
    
    def __init__(self, config_data: Dict[str, Any] = None):
        """
        Initialize the LLM Factory
        
        Args:
            config_data: Configuration data for the LLM
        """
        self.config_data = config_data or {}
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    def create_llm(self, streaming=False, phase_name=None) -> Optional[BaseChatModel]:
        """
        Create an LLM instance based on the provider specified in config
        
        Args:
            streaming: Whether to enable streaming for the LLM
            phase_name: Name of the current phase for streaming callbacks
            
        Returns:
            BaseChatModel: Initialized LLM instance or None if initialization fails
        """
        try:
            # Get LLM configuration
            llm_config = self.config_data.get('llm', {})
            provider = llm_config.get('provider', 'openai').lower()
            
            # Check if streaming is enabled in config
            if streaming:
                # Check if streaming is enabled for this specific phase
                streaming_phases = llm_config.get('streaming_phases', {})
                if phase_name and not streaming_phases.get(phase_name, True):
                    # Streaming is disabled for this phase
                    streaming = False
            
            # Create LLM based on provider
            if provider == 'openai':
                return self._create_openai_llm(llm_config, streaming, phase_name)
            elif provider == 'google':
                return self._create_google_llm(llm_config, streaming, phase_name)
            elif provider == 'ollama':
                return self._create_ollama_llm(llm_config, streaming, phase_name)
            else:
                self.logger.error(f"Unsupported LLM provider: {provider}")
                return None
        except Exception as e:
            self.logger.error(f"Error creating LLM: {str(e)}")
            return None
    
    def _create_openai_llm(self, llm_config: Dict[str, Any], streaming=False, phase_name=None) -> Optional[BaseChatModel]:
        """
        Create an OpenAI LLM instance
        
        Args:
            llm_config: LLM configuration data
            streaming: Whether to enable streaming for the LLM
            phase_name: Name of the current phase for streaming callbacks
            
        Returns:
            BaseChatModel: Initialized OpenAI LLM instance or None if initialization fails
        """
        try:
            from langchain_openai import ChatOpenAI
            
            # Get OpenAI-specific configuration
            openai_config = llm_config.get('openai', {})
            if not openai_config:
                # If no specific OpenAI config is provided, use the top-level config
                # This maintains backward compatibility with the old config format
                if streaming:
                    from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
                    from .streaming_callbacks import StreamingCallbackHandler
                    
                    # Create streaming callback handler
                    callbacks = [StreamingCallbackHandler(phase_name)] if phase_name else [StreamingStdOutCallbackHandler()]
                    
                    return ChatOpenAI(
                        model=llm_config.get('model', 'gpt-4'),
                        api_key=llm_config.get('api_key', None),
                        base_url=llm_config.get('api_endpoint', None),
                        temperature=llm_config.get('temperature', 0.1),
                        max_tokens=llm_config.get('max_tokens', 4000),
                        streaming=True,
                        callbacks=callbacks
                    )
                else:
                    return ChatOpenAI(
                        model=llm_config.get('model', 'gpt-4'),
                        api_key=llm_config.get('api_key', None),
                        base_url=llm_config.get('api_endpoint', None),
                        temperature=llm_config.get('temperature', 0.1),
                        max_tokens=llm_config.get('max_tokens', 4000)
                    )
            
            # Use OpenAI-specific configuration
            if streaming:
                from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
                from .streaming_callbacks import StreamingCallbackHandler
                
                # Create streaming callback handler
                callbacks = [StreamingCallbackHandler(phase_name)] if phase_name else [StreamingStdOutCallbackHandler()]
                
                return ChatOpenAI(
                    model=openai_config.get('model', 'gpt-4'),
                    api_key=openai_config.get('api_key', None),
                    base_url=openai_config.get('api_endpoint', None),
                    temperature=openai_config.get('temperature', 0.1),
                    max_tokens=openai_config.get('max_tokens', 4000),
                    streaming=True,
                    callbacks=callbacks
                )
            else:
                return ChatOpenAI(
                    model=openai_config.get('model', 'gpt-4'),
                    api_key=openai_config.get('api_key', None),
                    base_url=openai_config.get('api_endpoint', None),
                    temperature=openai_config.get('temperature', 0.1),
                    max_tokens=openai_config.get('max_tokens', 4000)
                )
            
        except Exception as e:
            self.logger.error(f"Error creating OpenAI LLM: {str(e)}")
            return None
    
    def _create_google_llm(self, llm_config: Dict[str, Any], streaming=False, phase_name=None) -> Optional[BaseChatModel]:
        """
        Create a Google Gemini LLM instance
        
        Args:
            llm_config: LLM configuration data
            streaming: Whether to enable streaming for the LLM
            phase_name: Name of the current phase for streaming callbacks
            
        Returns:
            BaseChatModel: Initialized Google LLM instance or None if initialization fails
        """
        try:
            from langchain_google_genai import ChatGoogleGenerativeAI
            
            # Get Google-specific configuration
            google_config = llm_config.get('google', {})
            
            # Use Google-specific configuration
            if streaming:
                from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
                from .streaming_callbacks import StreamingCallbackHandler
                
                # Create streaming callback handler
                callbacks = [StreamingCallbackHandler(phase_name)] if phase_name else [StreamingStdOutCallbackHandler()]
                
                return ChatGoogleGenerativeAI(
                    model=google_config.get('model', 'gemini-2.5-pro'),
                    google_api_key=google_config.get('api_key', None),
                    temperature=google_config.get('temperature', 0.1),
                    max_output_tokens=google_config.get('max_tokens', 4000),
                    streaming=True,
                    callbacks=callbacks
                )
            else:
                return ChatGoogleGenerativeAI(
                    model=google_config.get('model', 'gemini-2.5-pro'),
                    google_api_key=google_config.get('api_key', None),
                    temperature=google_config.get('temperature', 0.1),
                    max_output_tokens=google_config.get('max_tokens', 4000)
                )
            
        except Exception as e:
            self.logger.error(f"Error creating Google LLM: {str(e)}")
            return None
    
    def _create_ollama_llm(self, llm_config: Dict[str, Any], streaming=False, phase_name=None) -> Optional[BaseChatModel]:
        """
        Create an Ollama LLM instance
        
        Args:
            llm_config: LLM configuration data
            streaming: Whether to enable streaming for the LLM
            phase_name: Name of the current phase for streaming callbacks
            
        Returns:
            BaseChatModel: Initialized Ollama LLM instance or None if initialization fails
        """
        try:
            from langchain_ollama import ChatOllama
            
            # Get Ollama-specific configuration
            ollama_config = llm_config.get('ollama', {})
            
            # Use Ollama-specific configuration
            if streaming:
                from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
                from .streaming_callbacks import StreamingCallbackHandler
                
                # Create streaming callback handler
                callbacks = [StreamingCallbackHandler(phase_name)] if phase_name else [StreamingStdOutCallbackHandler()]
                
                return ChatOllama(
                    model=ollama_config.get('model', 'llama3'),
                    base_url=ollama_config.get('base_url', 'http://localhost:11434'),
                    temperature=ollama_config.get('temperature', 0.1),
                    num_predict=ollama_config.get('max_tokens', 4000),
                    streaming=True,
                    callbacks=callbacks
                )
            else:
                return ChatOllama(
                    model=ollama_config.get('model', 'llama3'),
                    base_url=ollama_config.get('base_url', 'http://localhost:11434'),
                    temperature=ollama_config.get('temperature', 0.1),
                    num_predict=ollama_config.get('max_tokens', 4000)
                )
            
        except Exception as e:
            self.logger.error(f"Error creating Ollama LLM: {str(e)}")
            return None
    
    def test_llm_connection(self, streaming=False) -> bool:
        """
        Test the LLM connection by sending a simple message
        
        Args:
            streaming: Whether to enable streaming for the test
            
        Returns:
            bool: True if the connection is successful, False otherwise
        """
        llm = self.create_llm(streaming=streaming)
        if not llm:
            return False
            
        try:
            # Simple test message
            messages = [
                SystemMessage(content="You are a helpful assistant."),
                HumanMessage(content="Hello, are you working?")
            ]
            
            # Test the LLM
            response = llm.invoke(messages)
            
            # If we get here, the connection is working
            return True
            
        except Exception as e:
            self.logger.error(f"Error testing LLM connection: {str(e)}")
            return False
</file>

<file path="phases/static_plan_step_reader.py">
#!/usr/bin/env python3
"""
Static Plan Step Reader for Investigation Planning

This module reads static plan steps from a JSON file and integrates them
into the investigation plan.
"""

import logging
import json
import os
from typing import Dict, List, Any

logger = logging.getLogger(__name__)

class StaticPlanStepReader:
    """
    Reads static plan steps from a JSON file and integrates them into the investigation plan
    """
    
    def __init__(self, config_data: Dict[str, Any] = None):
        """
        Initialize the Static Plan Step Reader
        
        Args:
            config_data: Configuration data for the reader
        """
        self.config_data = config_data or {}
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.static_plan_step_path = self.config_data.get('plan_phase', {}).get(
            'static_plan_step_path', 'static_plan_step.json'
        )
    
    def read_static_steps(self) -> List[Dict[str, Any]]:
        """
        Read static plan steps from the configured JSON file
        
        Returns:
            List[Dict[str, Any]]: List of static plan steps
        """
        try:
            # Check if file exists
            if not os.path.exists(self.static_plan_step_path):
                self.logger.error(f"Static plan step file not found: {self.static_plan_step_path}")
                return []
            
            # Read and parse JSON file
            with open(self.static_plan_step_path, 'r') as f:
                static_steps = json.load(f)
            
            # Validate static steps
            if not isinstance(static_steps, list):
                self.logger.error(f"Invalid static plan step file format: {self.static_plan_step_path}")
                return []
            
            # Validate each step
            valid_steps = []
            for i, step in enumerate(static_steps):
                if not isinstance(step, dict):
                    self.logger.error(f"Invalid step format at index {i}: {step}")
                    continue
                
                if 'description' not in step or 'tool' not in step or 'expected' not in step:
                    self.logger.error(f"Missing required fields in step at index {i}: {step}")
                    continue
                
                # Check for priority and priority_score, set defaults if not present
                if 'priority' not in step:
                    self.logger.warning(f"Priority not found for step at index {i}, setting default priority 'medium'")
                    step['priority'] = 'medium'
                
                if 'priority_score' not in step:
                    self.logger.warning(f"Priority score not found for step at index {i}, setting default priority score 50")
                    step['priority_score'] = 50
                
                valid_steps.append(step)
            
            self.logger.info(f"Successfully read {len(valid_steps)} static plan steps")
            return valid_steps
            
        except Exception as e:
            self.logger.error(f"Error reading static plan steps: {str(e)}")
            return []
    
    def add_static_steps(self, preliminary_steps: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Add static plan steps to the preliminary steps
        
        Args:
            preliminary_steps: Preliminary investigation steps from rule-based generator
            
        Returns:
            List[Dict[str, Any]]: Combined list of preliminary and static steps
        """
        static_steps = self.read_static_steps()
        
        if not static_steps:
            self.logger.warning("No static plan steps found, returning only preliminary steps")
            return preliminary_steps
        
        # Create a set of tool names already used in preliminary steps
        used_tools = set()
        for step in preliminary_steps:
            # Extract the base tool name (without arguments)
            tool = step.get('tool', '')
            if '(' in tool:
                tool = tool.split('(')[0]
            used_tools.add(tool)
        
        self.logger.info(f"Found {len(used_tools)} unique tools in preliminary steps: {used_tools}")
        
        # Filter out static steps that use tools already present in preliminary steps
        filtered_static_steps = []
        for step in static_steps:
            tool = step.get('tool', '')
            if '(' in tool:
                tool = tool.split('(')[0]
            
            if tool not in used_tools:
                filtered_static_steps.append(step)
            else:
                self.logger.info(f"Skipping static step with duplicate tool: {tool}")
        
        self.logger.info(f"Filtered out {len(static_steps) - len(filtered_static_steps)} static steps with duplicate tools")
        
        if not filtered_static_steps:
            self.logger.warning("No unique static steps found after filtering, returning only preliminary steps")
            return preliminary_steps
        
        # Sort static steps by priority_score (higher numbers have higher priority)
        filtered_static_steps.sort(key=lambda x: x.get('priority_score', 0), reverse=True)
        self.logger.info(f"Sorted {len(filtered_static_steps)} static steps by priority_score")
        
        # Add step numbers to static steps
        step_number = len(preliminary_steps) + 1
        for step in filtered_static_steps:
            step['step'] = step_number
            step['source'] = 'static'  # Mark the source for later reference
            step_number += 1
        
        # Combine preliminary and static steps
        combined_steps = preliminary_steps + filtered_static_steps
        self.logger.info(f"Combined {len(preliminary_steps)} preliminary steps with {len(filtered_static_steps)} static steps")
        
        return combined_steps
</file>

<file path="phases/utils.py">
#!/usr/bin/env python3
"""
Common Utilities for Kubernetes Volume Troubleshooting Phases

This module contains common utility functions used across different phases
of the troubleshooting process to reduce code duplication and improve maintainability.
"""

import logging
from typing import Dict, List, Any, Optional

logger = logging.getLogger(__name__)


def validate_knowledge_graph(knowledge_graph: Any, caller_name: str = "Unknown") -> bool:
    """
    Validate that the provided object is a valid KnowledgeGraph instance

    Args:
        knowledge_graph: Object to validate
        caller_name: Name of the calling module/class for logging

    Returns:
        bool: True if valid, False otherwise

    Raises:
        ValueError: If the knowledge graph is invalid
    """
    logger.debug(f"Validating knowledge graph from {caller_name}")
    
    if not hasattr(knowledge_graph, 'graph'):
        error_msg = "Invalid Knowledge Graph: missing 'graph' attribute"
        logger.error(f"{caller_name}: {error_msg}")
        raise ValueError(error_msg)
    
    if not hasattr(knowledge_graph, 'get_all_issues'):
        error_msg = "Invalid Knowledge Graph: missing 'get_all_issues' method"
        logger.error(f"{caller_name}: {error_msg}")
        raise ValueError(error_msg)
    
    return True


def format_historical_experiences(knowledge_graph: Any) -> str:
    """
    Format historical experience data from a Knowledge Graph

    Args:
        knowledge_graph: Knowledge Graph containing historical experience data

    Returns:
        str: Formatted historical experience data for LLM consumption
    """
    try:
        # Check if knowledge graph is valid
        if not knowledge_graph or not hasattr(knowledge_graph, 'graph'):
            return "No historical experience data available."
        
        # Find historical experience nodes
        historical_experience_nodes = []
        for node_id, attrs in knowledge_graph.graph.nodes(data=True):
            if attrs.get('gnode_subtype') == 'HistoricalExperience':
                historical_experience_nodes.append((node_id, attrs))
        
        if not historical_experience_nodes:
            return "No historical experience data available."
        
        # Format historical experiences in a clear, structured way
        formatted_entries = []
        
        for idx, (node_id, attrs) in enumerate(historical_experience_nodes, 1):
            # Get attributes from the experience
            phenomenon = attrs.get('phenomenon', 'Unknown phenomenon')
            root_cause = attrs.get('root_cause', 'Unknown root cause')
            localization_method = attrs.get('localization_method', 'No localization method provided')
            resolution_method = attrs.get('resolution_method', 'No resolution method provided')
            
            # Format the entry
            entry = f"""HISTORICAL EXPERIENCE #{idx}:
Phenomenon: {phenomenon}
Root Cause: {root_cause}
Localization Method: {localization_method}
Resolution Method: {resolution_method}
"""
            formatted_entries.append(entry)
        
        return "\n".join(formatted_entries)
        
    except Exception as e:
        logger.warning(f"Error formatting historical experiences: {str(e)}")
        return "Error formatting historical experience data."


def format_historical_experiences_from_collected_info(collected_info: Dict[str, Any]) -> str:
    """
    Format historical experience data from collected information

    Args:
        collected_info: Pre-collected diagnostic information from Phase 0

    Returns:
        str: Formatted historical experience data for LLM consumption
    """
    try:
        # Extract historical experiences from knowledge graph in collected_info
        kg = collected_info.get('knowledge_graph', None)
        return format_historical_experiences(kg)
        
    except Exception as e:
        logger.warning(f"Error formatting historical experiences from collected info: {str(e)}")
        return "Error formatting historical experience data."


def generate_basic_fallback_plan(pod_name: str, namespace: str, volume_path: str) -> str:
    """
    Generate a basic fallback investigation plan when all else fails

    Args:
        pod_name: Name of the pod with the error
        namespace: Namespace of the pod
        volume_path: Path of the volume with I/O error

    Returns:
        str: Basic fallback Investigation Plan
    """
    basic_plan = f"""Investigation Plan:
Target: Pod {namespace}/{pod_name}, Volume Path: {volume_path}
Generated Steps: 4 basic steps (fallback mode)

Step 1: Get all critical issues from Knowledge Graph | Tool: kg_get_all_issues(severity='primary') | Expected: List of critical issues affecting the system
Step 2: Analyze existing issues and patterns | Tool: kg_analyze_issues() | Expected: Root cause analysis and issue relationships  
Step 3: Get system overview | Tool: kg_get_summary() | Expected: Overall system health and entity statistics
Step 4: Print complete Knowledge Graph for manual analysis | Tool: kg_print_graph(include_details=True, include_issues=True) | Expected: Full system visualization for troubleshooting

Fallback Steps (if main steps fail):
Step F1: Search for any Pod entities | Tool: kg_get_related_entities(entity_type='Pod', entity_id='any', max_depth=1) | Expected: List of all Pods | Trigger: entity_not_found
Step F2: Search for any Drive entities | Tool: kg_get_related_entities(entity_type='Drive', entity_id='any', max_depth=1) | Expected: List of all Drives | Trigger: no_target_found
"""
    return basic_plan


def handle_exception(func_name: str, exception: Exception, logger_instance: Optional[logging.Logger] = None) -> str:
    """
    Standardized exception handling with proper logging

    Args:
        func_name: Name of the function where the exception occurred
        exception: The exception that was raised
        logger_instance: Logger instance to use, defaults to module logger if None

    Returns:
        str: Formatted error message
    """
    log = logger_instance or logger
    error_msg = f"Error in {func_name}: {str(exception)}"
    log.error(error_msg)
    return error_msg


def format_json_safely(data: Any, indent: int = 2, fallback_message: str = "Unable to format data") -> str:
    """
    Safely format data as JSON with fallback for non-serializable objects

    Args:
        data: Data to format as JSON
        indent: JSON indentation level
        fallback_message: Message to use if formatting fails

    Returns:
        str: JSON-formatted string or fallback message
    """
    import json
    
    try:
        # Try to convert to JSON using custom serializer
        def json_serializer(obj):
            """Custom JSON serializer to handle non-serializable objects"""
            try:
                # Try to convert to a simple dict first
                if hasattr(obj, "__dict__"):
                    return obj.__dict__
                # Handle sets
                elif isinstance(obj, set):
                    return list(obj)
                # Handle other non-serializable types
                else:
                    return str(obj)
            except:
                return str(obj)
        
        return json.dumps(data, indent=indent, default=json_serializer)
    except Exception as e:
        logger.warning(f"Error serializing data to JSON: {str(e)}")
        # Fallback to a simpler representation
        return fallback_message
</file>

<file path="tests/test_plan_phase_react.py">
#!/usr/bin/env python3
"""
Test script for the Plan Phase ReAct graph implementation.

This script demonstrates how to use the ReAct graph implementation for the plan phase
of the Kubernetes volume troubleshooting system.
"""

import asyncio
import logging
import yaml
import json
import sys
import os
from langchain_core.messages import SystemMessage, HumanMessage

# Add parent directory to path to import modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from knowledge_graph import KnowledgeGraph
from phases.plan_phase_react import run_plan_phase_react, PlanPhaseReActGraph
from tests.mock_knowledge_graph import create_mock_knowledge_graph

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('test_plan_phase_react.log')
    ]
)

logger = logging.getLogger(__name__)

async def test_plan_phase_react():
    """Test the Plan Phase ReAct graph implementation."""
    logger.info("Starting Plan Phase ReAct test")
    
    # Load configuration
    try:
        with open('config.yaml', 'r') as f:
            config_data = yaml.safe_load(f)
    except Exception as e:
        logger.error(f"Failed to load configuration: {e}")
        config_data = {}
    
    # Ensure ReAct is enabled
    if 'plan_phase' not in config_data:
        config_data['plan_phase'] = {}
    config_data['plan_phase']['use_react'] = True
    
    # Create mock knowledge graph
    logger.info("Creating mock knowledge graph")
    knowledge_graph = create_mock_knowledge_graph()
    
    # Set test parameters
    pod_name = "test-pod"
    namespace = "default"
    volume_path = "/dev/sda"
    
    # Prepare messages for the ReAct graph
    logger.info("Preparing messages for the ReAct graph")
    # Create system message with instructions for ReAct
    system_prompt = """You are an AI assistant tasked with generating an Investigation Plan for troubleshooting Kubernetes volume I/O errors.
You are operating in a ReAct (Reasoning and Acting) framework where you can:
1. REASON about the problem and identify knowledge gaps
2. ACT by calling external tools to gather information
3. OBSERVE the results and update your understanding
4. Continue this loop until you have enough information to create a comprehensive plan

Your goal is to create a detailed Investigation Plan that identifies potential problems and provides specific steps to diagnose and resolve volume read/write errors.

When you identify a knowledge gap, use the appropriate MCP tool to gather the information you need. Don't guess or make assumptions when you can use a tool to get accurate information.

When you've completed the Investigation Plan, include the marker [END_GRAPH] at the end of your message.
"""
    
    # Create user message with context
    kg_summary = knowledge_graph.get_summary() if knowledge_graph else {}
    issues = knowledge_graph.get_all_issues() if knowledge_graph else []
    
    kg_context = f"""
Knowledge Graph Summary:
{json.dumps(kg_summary, indent=2)}

Issues:
{json.dumps(issues, indent=2)}
"""
    
    user_prompt = f"""# INVESTIGATION PLAN GENERATION TASK
## TARGET: Volume read/write errors in pod {pod_name} (namespace: {namespace}, volume path: {volume_path})

I need you to create a comprehensive Investigation Plan for troubleshooting this volume I/O error.

## BACKGROUND INFORMATION

### KNOWLEDGE GRAPH CONTEXT
{kg_context}

## TASK
1. Analyze the available information to understand the context
2. Identify any knowledge gaps that need to be filled
3. Use MCP tools to gather additional information as needed
4. Create a comprehensive Investigation Plan with specific steps to diagnose and resolve the volume I/O error

Please start by analyzing the available information and identifying any knowledge gaps.
"""
    
    # Create message list
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ]
    
    # Run the ReAct graph
    logger.info(f"Running Plan Phase ReAct for {namespace}/{pod_name} volume {volume_path}")
    try:
        investigation_plan, message_list = await run_plan_phase_react(
            pod_name, namespace, volume_path, messages, config_data
        )
        
        # Log the results
        logger.info("Plan Phase ReAct completed successfully")
        logger.info(f"Investigation Plan:\n{investigation_plan}")
        
        # Save the investigation plan to a file
        with open('test_plan_phase_react_output.txt', 'w') as f:
            f.write(investigation_plan)
        
        # Save the message list to a file
        with open('test_plan_phase_react_messages.json', 'w') as f:
            json.dump(message_list, f, indent=2)
        
        logger.info("Test completed successfully")
        return True
    except Exception as e:
        logger.error(f"Test failed: {e}")
        return False

def print_usage():
    """Print usage information."""
    print("Usage: python test_plan_phase_react.py")
    print("This script tests the Plan Phase ReAct graph implementation.")

if __name__ == "__main__":
    print("Testing Plan Phase ReAct graph implementation")
    asyncio.run(test_plan_phase_react())
</file>

<file path="tools/testing/pod_creation.py">
#!/usr/bin/env python3
"""
Pod and resource creation tools for testing volume functionality.

This module provides tools for creating test pods, PVCs, and storage classes
to validate volume functionality during troubleshooting.
"""

import json
import yaml
from typing import Dict, Any
from langchain_core.tools import tool
from tools.core.config import validate_command, execute_command

@tool
def create_test_pod(pod_name: str, namespace: str = "default", 
                   pvc_name: str = None, mount_path: str = "Need a mount path",
                   image: str = "busybox:latest", storage_class: str = None) -> str:
    """
    Create a test pod with volume mount for testing volume functionality
    
    Args:
        pod_name: Name for the test pod
        namespace: Kubernetes namespace (default: default)
        pvc_name: Name of PVC to mount (if None, creates one)
        mount_path: Path to mount the volume in the pod, must have a valid path
        image: Container image to use (default: busybox:latest)
        storage_class: Storage class for PVC creation
        
    Returns:
        str: Result of pod creation
    """
    # If no PVC name provided, generate one
    if not pvc_name:
        pvc_name = f"{pod_name}-pvc"
    
    # Create PVC first if storage_class is provided
    pvc_yaml = None
    if storage_class:
        pvc_yaml = f"""
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: {pvc_name}
  namespace: {namespace}
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: {storage_class}
"""
    
    # Create test pod YAML
    pod_yaml = f"""
apiVersion: v1
kind: Pod
metadata:
  name: {pod_name}
  namespace: {namespace}
  labels:
    app: volume-test
    test-type: troubleshooting
spec:
  containers:
  - name: test-container
    image: {image}
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo 'Test pod running...'; sleep 30; done"]
    volumeMounts:
    - name: test-volume
      mountPath: {mount_path}
  volumes:
  - name: test-volume
    persistentVolumeClaim:
      claimName: {pvc_name}
  restartPolicy: Never
"""
    
    results = []
    
    # Create PVC if needed
    if pvc_yaml:
        try:
            cmd = ["kubectl", "apply", "-f", "-"]
            # Create a process with stdin pipe to pass the YAML
            import subprocess
            process = subprocess.Popen(
                cmd,
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                universal_newlines=True
            )
            stdout, stderr = process.communicate(input=pvc_yaml)
            result = stdout if process.returncode == 0 else f"Error: {stderr}"
            results.append(f"PVC Creation: {result}")
        except Exception as e:
            return f"Error creating PVC: {str(e)}"
    
    # Create pod
    try:
        cmd = ["kubectl", "apply", "-f", "-"]
        # Create a process with stdin pipe to pass the YAML
        import subprocess
        process = subprocess.Popen(
            cmd,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            universal_newlines=True
        )
        stdout, stderr = process.communicate(input=pod_yaml)
        result = stdout if process.returncode == 0 else f"Error: {stderr}"
        results.append(f"Pod Creation: {result}")
        
        # Wait for pod to be ready (optional check)
        cmd = ["kubectl", "wait", "--for=condition=Ready", f"pod/{pod_name}", 
               "-n", namespace, "--timeout=60s"]
        wait_result = execute_command(cmd, purpose="Waiting for pod to be ready")
        results.append(f"Pod Ready Status: {wait_result}")
        
        return "\n".join(results)
        
    except Exception as e:
        return f"Error creating test pod: {str(e)}"

@tool
def create_test_pvc(pvc_name: str, namespace: str = "default", 
                   storage_class: str = "csi-baremetal-sc-ssd", 
                   size: str = "1Gi", access_mode: str = "ReadWriteOnce") -> str:
    """
    Create a test PVC for volume testing
    
    Args:
        pvc_name: Name for the PVC
        namespace: Kubernetes namespace
        storage_class: Storage class to use
        size: Storage size (e.g., 1Gi, 500Mi)
        access_mode: Access mode (ReadWriteOnce, ReadWriteMany, etc.)
        
    Returns:
        str: Result of PVC creation
    """
    pvc_yaml = f"""
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: {pvc_name}
  namespace: {namespace}
  labels:
    test-type: troubleshooting
spec:
  accessModes:
    - {access_mode}
  resources:
    requests:
      storage: {size}
  storageClassName: {storage_class}
"""
    
    try:
        cmd = ["kubectl", "apply", "-f", "-"]
        # Create a process with stdin pipe to pass the YAML
        import subprocess
        process = subprocess.Popen(
            cmd,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            universal_newlines=True
        )
        stdout, stderr = process.communicate(input=pvc_yaml)
        result = stdout if process.returncode == 0 else f"Error: {stderr}"
        
        # Check PVC status
        cmd = ["kubectl", "get", "pvc", pvc_name, "-n", namespace, "-o", "yaml"]
        status_result = execute_command(cmd, purpose="Checking PVC status")
        
        return f"PVC Creation: {result}\n\nPVC Status:\n{status_result}"
        
    except Exception as e:
        return f"Error creating test PVC: {str(e)}"

@tool
def create_test_storage_class(sc_name: str, provisioner: str = "csi-baremetal.dell.com",
                             drive_type: str = "SSD", fs_type: str = "ext4") -> str:
    """
    Create a test storage class for CSI Baremetal testing
    
    Args:
        sc_name: Name for the storage class
        provisioner: CSI provisioner (default: csi-baremetal.dell.com)
        drive_type: Drive type (SSD, HDD, NVMe)
        fs_type: Filesystem type (ext4, xfs)
        
    Returns:
        str: Result of storage class creation
    """
    sc_yaml = f"""
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: {sc_name}
  labels:
    test-type: troubleshooting
provisioner: {provisioner}
parameters:
  driveType: {drive_type}
  fsType: {fs_type}
allowVolumeExpansion: false
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Delete
"""
    
    try:
        cmd = ["kubectl", "apply", "-f", "-"]
        # Create a process with stdin pipe to pass the YAML
        import subprocess
        process = subprocess.Popen(
            cmd,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            universal_newlines=True
        )
        stdout, stderr = process.communicate(input=sc_yaml)
        result = stdout if process.returncode == 0 else f"Error: {stderr}"
        
        # Verify storage class
        cmd = ["kubectl", "get", "storageclass", sc_name, "-o", "yaml"]
        verify_result = execute_command(cmd, purpose="Verifying storage class")
        
        return f"Storage Class Creation: {result}\n\nStorage Class Details:\n{verify_result}"
        
    except Exception as e:
        return f"Error creating test storage class: {str(e)}"
</file>

<file path="tools/testing/volume_testing_analysis.py">
#!/usr/bin/env python3
"""
Analysis-related volume testing tools.

This module provides tools for analyzing volume space usage
and checking data integrity on pod volumes.
"""

import re
from datetime import datetime
from typing import Dict, Any, Optional
from langchain_core.tools import tool
from tools.core.config import validate_command, execute_command

@tool
def analyze_volume_space_usage(pod_name: str, namespace: str = "default",
                              mount_path: str = "Need to specify mount path",
                              detect_large_files: bool = True) -> str:
    """
    Analyze volume space usage within a pod, identifying large files and usage patterns
    
    Args:
        pod_name: Name of the pod
        namespace: Kubernetes namespace
        mount_path: Volume mount path must be specified
        detect_large_files: Whether to identify large files (may be slower for large volumes)
        
    Returns:
        str: Volume space usage analysis results
    """
    results = []
    
    try:
        # Check overall space usage
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "df", "-h", mount_path]
        df_result = execute_command(cmd)
        results.append(f"Volume Space Overview:\n{df_result}")
        
        # Check inode usage
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "df", "-i", mount_path]
        inode_result = execute_command(cmd)
        results.append(f"Inode Usage:\n{inode_result}")
        
        # Get directory usage summary
        du_cmd = f"du -h --max-depth=1 {mount_path} | sort -hr"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", du_cmd]
        du_result = execute_command(cmd)
        results.append(f"Directory Usage Summary:\n{du_result}")
        
        # Find largest directories (top 5)
        du_dirs_cmd = f"find {mount_path} -type d -exec du -h --max-depth=0 {{}} \\; 2>/dev/null | sort -hr | head -5"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", du_dirs_cmd]
        du_dirs_result = execute_command(cmd)
        results.append(f"Largest Directories (Top 5):\n{du_dirs_result}")
        
        # Find largest files if requested
        if detect_large_files:
            large_files_cmd = f"find {mount_path} -type f -exec ls -lh {{}} \\; 2>/dev/null | sort -k 5 -hr | head -10"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", large_files_cmd]
            large_files_result = execute_command(cmd)
            results.append(f"Largest Files (Top 10):\n{large_files_result}")
        
        # Check for files that might grow quickly (logs)
        try:
            log_files_cmd = f"find {mount_path} -name '*.log' -o -name '*.log.*' -o -path '*/logs/*' | xargs ls -lh 2>/dev/null | sort -k 5 -hr | head -5"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", log_files_cmd]
            log_files_result = execute_command(cmd)
            
            if log_files_result and not "No such file" in log_files_result:
                results.append(f"Potential Log Files (may grow over time):\n{log_files_result}")
        except Exception as e:
            results.append(f"Error checking log files: {str(e)}")
        
        # Check for temporary files
        try:
            temp_files_cmd = f"find {mount_path} -name '*.tmp' -o -name 'temp*' -o -name 'tmp*' | xargs ls -lh 2>/dev/null | sort -k 5 -hr | head -5"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", temp_files_cmd]
            temp_files_result = execute_command(cmd)
            
            if temp_files_result and not "No such file" in temp_files_result:
                results.append(f"Temporary Files:\n{temp_files_result}")
        except Exception as e:
            results.append(f"Error checking temporary files: {str(e)}")
        
        # File type distribution
        file_types_cmd = f"find {mount_path} -type f | grep -v '^$' | grep -o '\\.[^\\./]*$' | sort | uniq -c | sort -nr | head -10"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", file_types_cmd]
        file_types_result = execute_command(cmd)
        results.append(f"File Type Distribution (Top 10):\n{file_types_result}")
        
        return "\n" + "="*50 + "\n".join(results)
        
    except Exception as e:
        return f"Error analyzing volume space usage: {str(e)}"

@tool
def check_volume_data_integrity(pod_name: str, namespace: str = "default",
                               mount_path: str = "/test-volume",
                               file_pattern: str = None,
                               create_baseline: bool = False) -> str:
    """
    Perform a checksum-based integrity check on critical files in the pod volume
    
    Args:
        pod_name: Name of the pod
        namespace: Kubernetes namespace
        mount_path: Volume mount path, must be specified
        file_pattern: Optional file pattern to check (e.g., "*.db" or "data/*.json")
        create_baseline: Whether to create a new baseline rather than verifying
        
    Returns:
        str: Data integrity check results
    """
    results = []
    
    try:
        # Determine files to check
        file_list_cmd = None
        if file_pattern:
            file_list_cmd = f"find {mount_path} -path '{mount_path}/{file_pattern}' -type f | sort"
        else:
            # Limit to reasonable number of files if no pattern provided
            file_list_cmd = f"find {mount_path} -type f -size -10M | grep -v '.checksum' | head -50 | sort"
        
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", file_list_cmd]
        file_list_result = execute_command(cmd)
        
        if not file_list_result or "No such file" in file_list_result:
            return f"No files found matching pattern in {mount_path}"
        
        files = file_list_result.strip().split('\n')
        if not files or files[0] == '':
            return f"No files found matching pattern in {mount_path}"
        
        results.append(f"Found {len(files)} files to check for integrity")
        
        # Check if checksum tools are available
        check_cmd = "which sha256sum || which md5sum || echo 'No checksum tools found'"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", check_cmd]
        check_result = execute_command(cmd)
        
        if "No checksum tools found" in check_result:
            return "Error: No checksum tools (sha256sum or md5sum) found in the pod"
        
        # Determine which tool to use
        checksum_tool = "sha256sum"  # prefer sha256sum
        if "sha256sum" not in check_result:
            checksum_tool = "md5sum"
        
        results.append(f"Using {checksum_tool} for integrity verification")
        
        # Define checksum file location
        checksum_file = f"{mount_path}/.volume_checksums_{checksum_tool.replace('sum', '')}"
        
        if create_baseline:
            # Create new baseline checksums
            checksum_cmd = f"{checksum_tool} {' '.join(files)} > {checksum_file}"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", checksum_cmd]
            checksum_result = execute_command(cmd)
            
            # Verify the checksum file was created
            verify_cmd = f"ls -la {checksum_file}"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", verify_cmd]
            verify_result = execute_command(cmd)
            
            if "No such file" in verify_result:
                results.append(f"Error: Failed to create checksum baseline file {checksum_file}")
            else:
                results.append(f"Successfully created checksum baseline with {len(files)} files")
                results.append(f"Baseline stored at: {checksum_file}")
                
                # Show sample of created checksums
                sample_cmd = f"head -5 {checksum_file}"
                cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", sample_cmd]
                sample_result = execute_command(cmd)
                results.append(f"Sample checksums:\n{sample_result}")
        else:
            # Verify against existing checksums
            check_exists_cmd = f"test -f {checksum_file} && echo 'exists' || echo 'not found'"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", check_exists_cmd]
            exists_result = execute_command(cmd).strip()
            
            if exists_result != "exists":
                results.append(f"No baseline checksum file found at {checksum_file}")
                results.append("Run this tool with create_baseline=True to create a baseline first")
                return "\n" + "="*50 + "\n".join(results)
            
            # Verify files against baseline
            verify_cmd = f"cd / && {checksum_tool} -c {checksum_file} 2>&1"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", verify_cmd]
            verify_result = execute_command(cmd)
            
            # Analyze results
            failed_count = verify_result.count("FAILED")
            ok_count = verify_result.count("OK")
            
            results.append(f"Integrity Check Results: {ok_count} files OK, {failed_count} files FAILED")
            
            if failed_count > 0:
                # Extract and show failed files
                failed_files_cmd = f"cd / && {checksum_tool} -c {checksum_file} 2>&1 | grep FAILED"
                cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", failed_files_cmd]
                failed_files_result = execute_command(cmd)
                results.append(f"Failed Files:\n{failed_files_result}")
            elif "No such file" in verify_result:
                results.append(f"Warning: Some files in the baseline no longer exist")
                
            # Check for new files not in the baseline
            new_files_cmd = f"find {mount_path} -type f -newer {checksum_file} | grep -v '{checksum_file}' | head -10"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", new_files_cmd]
            new_files_result = execute_command(cmd)
            
            if new_files_result and new_files_result.strip():
                results.append(f"Warning: New files found that are not in the baseline (created after baseline):\n{new_files_result}")
        
        return "\n" + "="*50 + "\n".join(results)
        
    except Exception as e:
        return f"Error checking volume data integrity: {str(e)}"
</file>

<file path="tools/testing/volume_testing_filesystem.py">
#!/usr/bin/env python3
"""
Filesystem-related volume testing tools.

This module provides tools for checking filesystem health
and performing filesystem-related diagnostics on pod volumes.
"""

import re
from datetime import datetime
from typing import Dict, Any, Optional
from langchain_core.tools import tool
from tools.core.config import validate_command, execute_command

@tool
def check_pod_volume_filesystem(pod_name: str, namespace: str = "default",
                               mount_path: str = "/test-volume",
                               device_path: str = None) -> str:
    """
    Perform a non-destructive filesystem check on pod volume with XFS filesystem
    
    Args:
        pod_name: Name of the pod with mounted volume
        namespace: Kubernetes namespace
        mount_path: Path where volume is mounted, must be specified
        device_path: Optional device path for the volume. If not provided, it will be detected
        
    Returns:
        str: Filesystem check results
    """
    results = []
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    try:
        # Add timestamp to log
        results.append(f"[{timestamp}] Filesystem Check on Pod Volume - Pod: {pod_name}")
        
        # If device path is not provided, try to detect it
        if not device_path:
            try:
                # Get device path from mount
                find_device_cmd = f"mount | grep '{mount_path}' | awk '{{print $1}}'"
                cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", find_device_cmd]
                device_result = execute_command(cmd)
                
                if not device_result or "Error" in device_result:
                    return f"Error: Unable to detect device path for mount {mount_path}. Please provide device_path explicitly."
                
                device_path = device_result.strip()
                results.append(f"Detected volume device path: {device_path}")
            except Exception as e:
                return f"Error detecting device path: {str(e)}"
        
        # Verify it's an XFS filesystem
        fs_type_cmd = f"df -T {mount_path} | tail -n 1 | awk '{{print $2}}'"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", fs_type_cmd]
        fs_type_result = execute_command(cmd).strip()
        
        results.append(f"Filesystem type: {fs_type_result}")
        
        if fs_type_result.lower() != "xfs":
            results.append(f"Warning: Filesystem is not XFS ({fs_type_result}). XFS check may not be appropriate.")
        
        # Check if xfs_repair is available
        check_tool_cmd = "which xfs_repair || echo 'xfs_repair not found'"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", check_tool_cmd]
        tool_check_result = execute_command(cmd)
        
        if "not found" in tool_check_result:
            results.append(f"Error: xfs_repair tool not found in the pod. Please install xfs utilities.")
            return "\n" + "="*50 + "\n".join(results)
        
        # Run non-destructive filesystem check
        check_cmd = f"xfs_repair -n {device_path} 2>&1 || echo 'xfs_repair failed with error code $?'"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", check_cmd]
        check_result = execute_command(cmd)
        
        # Process and analyze results
        if "xfs_repair failed" in check_result:
            results.append(f"XFS Filesystem Check Failed:\n{check_result}")
        elif "No modify flag set" in check_result and "would have been fixed" in check_result:
            results.append(f"XFS Filesystem Check found issues that need repair:\n{check_result}")
        elif "Phase" in check_result and not "bad" in check_result.lower() and not "error" in check_result.lower():
            results.append(f"XFS Filesystem Check Results:\nPhase checks completed. No issues found. Filesystem is clean.")
        else:
            results.append(f"XFS Filesystem Check Results:\n{check_result}")
        
        return "\n" + "="*50 + "\n".join(results)
        
    except Exception as e:
        return f"Error checking pod volume filesystem: {str(e)}"
</file>

<file path="troubleshooting/__init__.py">
"""
Troubleshooting module for Kubernetes Volume Issues

This module provides troubleshooting functionality and LangGraph implementations
for comprehensive root cause analysis and fix plan generation in the CSI Baremetal
driver troubleshooting system.

The module has been refactored according to Martin Fowler's "Refactoring: Improving
the Design of Existing Code" principles to improve maintainability, readability,
and extensibility.
"""

# Don't import at module level to avoid circular imports
# Functions can be imported directly from troubleshooting.graph when needed

__all__ = [
    # Functions available from troubleshooting.graph
    "create_troubleshooting_graph_with_context",
    # Classes available from troubleshooting.strategies
    "ExecutionType",
    "ToolExecutionStrategy",
    "SerialToolExecutionStrategy", 
    "ParallelToolExecutionStrategy",
    "StrategyFactory",
    # Classes available from troubleshooting.hook_manager
    "HookManager",
    # Classes available from troubleshooting.end_conditions
    "EndConditionChecker",
    "LLMBasedEndConditionChecker",
    "SimpleEndConditionChecker",
    "EndConditionFactory",
]

# Import when the module is imported directly
from troubleshooting.graph import create_troubleshooting_graph_with_context
from troubleshooting.strategies import (
    ExecutionType, 
    ToolExecutionStrategy, 
    SerialToolExecutionStrategy, 
    ParallelToolExecutionStrategy,
    StrategyFactory
)
from troubleshooting.hook_manager import HookManager
from troubleshooting.end_conditions import (
    EndConditionChecker, 
    LLMBasedEndConditionChecker,
    SimpleEndConditionChecker,
    EndConditionFactory
)
</file>

<file path="troubleshooting/prompt_manager.py">
#!/usr/bin/env python3
"""
Prompt Manager for Kubernetes Volume I/O Error Troubleshooting

This module provides backward compatibility with the original PromptManager class
by importing and using the new LegacyPromptManager from the llm_graph package.
"""

import logging
import json
import os
from typing import Dict, List, Any, Optional
from llm_graph.prompt_managers.legacy_prompt_manager import LegacyPromptManager

logger = logging.getLogger(__name__)

# For backward compatibility, provide the original PromptManager class
# that delegates to the new LegacyPromptManager
class PromptManager:
    """
    Manages all prompts used in the troubleshooting system
    
    Centralizes prompt generation and management to improve maintainability
    and make it easier to update prompts across the system.
    
    This class is maintained for backward compatibility and delegates
    to the new LegacyPromptManager class.
    """
    
    def __init__(self, config_data: Dict[str, Any] = None):
        """
        Initialize the Prompt Manager
        
        Args:
            config_data: Configuration data for the system
        """
        self.config_data = config_data or {}
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # Create an instance of the new LegacyPromptManager
        self.legacy_prompt_manager = LegacyPromptManager(config_data)
    
    def get_phase_specific_guidance(self, phase: str, final_output_example: str = "") -> str:
        """
        Get phase-specific guidance prompt
        
        Args:
            phase: Current troubleshooting phase ("phase1" for investigation, "phase2" for action)
            final_output_example: Example of final output format
            
        Returns:
            str: Phase-specific guidance prompt
        """
        return self.legacy_prompt_manager.get_phase_specific_guidance(phase, final_output_example)
    
    def get_system_prompt(self, phase: str, final_output_example: str = "") -> str:
        """
        Get the system prompt for a specific phase
        
        Args:
            phase: Current troubleshooting phase
            final_output_example: Example of final output format
            
        Returns:
            str: System prompt for the specified phase
        """
        return self.legacy_prompt_manager.get_system_prompt(phase, final_output_example)
    
    def get_context_summary(self, collected_info: Dict[str, Any]) -> str:
        """
        Get context summary from collected information
        
        Args:
            collected_info: Pre-collected diagnostic information
            
        Returns:
            str: Formatted context summary
        """
        return self.legacy_prompt_manager.get_context_summary(collected_info)
</file>

<file path="information_collector/metadata_parsers.py">
"""
Metadata Parsers

Contains methods for parsing metadata from tool outputs.
"""

import yaml
import logging
from typing import Dict, List, Any
from .base import InformationCollectorBase


class MetadataParsers(InformationCollectorBase):
    """Metadata parsing methods for different entity types"""
    
    def _parse_pod_metadata(self, pod_name: str, namespace: str) -> Dict[str, Any]:
        """Parse pod metadata from tool outputs using yaml package"""
        metadata = {
            'RestartCount': 0,
            'Phase': 'Unknown',
            'SecurityContext': {},
            'fsGroup': None
        }
        
        pod_output = self.collected_data.get('kubernetes', {}).get('target_pod', '')
        if pod_output:
            try:
                # Parse the YAML output
                pod_data = yaml.safe_load(pod_output)
                
                if pod_data:
                    # Extract pod phase
                    metadata['Phase'] = pod_data.get('status', {}).get('phase', 'Unknown')
                    
                    # Extract restart count from the first container status
                    container_statuses = pod_data.get('status', {}).get('containerStatuses', [])
                    if container_statuses and len(container_statuses) > 0:
                        metadata['RestartCount'] = container_statuses[0].get('restartCount', 0)
                    
                    # Extract security context and fsGroup
                    security_context = pod_data.get('spec', {}).get('securityContext', {})
                    metadata['SecurityContext'] = security_context
                    metadata['fsGroup'] = security_context.get('fsGroup')
                    
            except Exception as e:
                logging.warning(f"Error parsing pod metadata with yaml package: {e}")
                # Fallback to the old method in case of parsing errors
                try:
                    lines = pod_output.split('\n')
                    for line in lines:
                        if 'restartCount:' in line:
                            try:
                                count = int(line.split('restartCount:')[-1].strip())
                                metadata['RestartCount'] = count
                            except (ValueError, TypeError):
                                pass
                        elif 'phase:' in line:
                            metadata['Phase'] = line.split('phase:')[-1].strip()
                        elif 'fsGroup:' in line:
                            try:
                                group = int(line.split('fsGroup:')[-1].strip())
                                metadata['fsGroup'] = group
                            except (ValueError, TypeError):
                                pass
                except Exception as fallback_error:
                    logging.warning(f"Fallback parsing also failed for pod metadata: {fallback_error}")
        
        return metadata
    
    def _parse_pvc_metadata(self, pvc_name: str, namespace: str) -> Dict[str, Any]:
        """Parse PVC metadata from tool outputs using yaml package"""
        metadata = {
            'AccessModes': '',
            'StorageSize': '',
            'VolumeMode': 'Filesystem',
            'Phase': 'Unknown'
        }
        
        pvcs_output = self.collected_data.get('kubernetes', {}).get('pvcs', '')
        if pvcs_output:
            try:
                # Parse the YAML output
                pvc_data = yaml.safe_load(pvcs_output)
                
                # Find the PVC with matching name
                target_pvc = None
                if isinstance(pvc_data, dict) and 'items' in pvc_data and isinstance(pvc_data['items'], list):
                    # List of PVCs case
                    for pvc in pvc_data['items']:
                        if pvc.get('metadata', {}).get('name') == pvc_name:
                            target_pvc = pvc
                            break
                elif isinstance(pvc_data, dict) and pvc_data.get('metadata', {}).get('name') == pvc_name:
                    # Single PVC case
                    target_pvc = pvc_data
                elif isinstance(pvc_data, list):
                    # Direct list of PVCs
                    for pvc in pvc_data:
                        if pvc.get('metadata', {}).get('name') == pvc_name:
                            target_pvc = pvc
                            break
                
                if target_pvc:
                    # Extract PVC phase
                    metadata['Phase'] = target_pvc.get('status', {}).get('phase', 'Unknown')
                    
                    # Extract access modes
                    access_modes = target_pvc.get('status', {}).get('accessModes', [])
                    if access_modes and isinstance(access_modes, list) and len(access_modes) > 0:
                        metadata['AccessModes'] = access_modes[0]
                    
                    # Extract volume mode
                    metadata['VolumeMode'] = target_pvc.get('spec', {}).get('volumeMode', 'Filesystem')
                    
                    # Extract storage size
                    resources = target_pvc.get('spec', {}).get('resources', {})
                    metadata['StorageSize'] = resources.get('requests', {}).get('storage', '')
                    
            except Exception as e:
                logging.warning(f"Error parsing PVC metadata with yaml package: {e}")
                # Fallback to the old method in case of parsing errors
                try:
                    pvc_section = self._extract_yaml_section(pvcs_output, pvc_name)
                    access_mode_start = False
                    for line in pvc_section:
                        if access_mode_start:
                            # If we are in access modes section, get the next line
                            if '- ' in line:
                                access_mode = line.split(' ')[-1].strip()
                                metadata['AccessModes'] = access_mode
                                access_mode_start = False
                        elif 'accessModes:' in line:
                            # example: 
                            #  status:
                            #     accessModes:
                            #     - ReadWriteOnce
                            # the access mode in the next line write code to get the access mode
                            access_mode_start = True
                        elif 'storage:' in line and 'requests:' in pvcs_output:
                            metadata['StorageSize'] = line.split('storage:')[-1].strip()
                        elif 'phase:' in line:
                            metadata['Phase'] = line.split('phase:')[-1].strip()
                except Exception as fallback_error:
                    logging.warning(f"Fallback parsing also failed for PVC metadata: {fallback_error}")
        
        return metadata
    
    def _parse_pv_metadata(self, pv_name: str) -> Dict[str, Any]:
        """Parse PV metadata from tool outputs using yaml package"""
        metadata = {
            'Phase': 'Unknown',
            'ReclaimPolicy': 'Unknown',
            'AccessModes': [],
            'Capacity': '',
            'diskPath': '',
            'nodeAffinity': ''
        }
        
        pvs_output = self.collected_data.get('kubernetes', {}).get('pvs', '')
        if pvs_output:
            try:
                # Parse the YAML output
                pv_data = yaml.safe_load(pvs_output)
                
                # Find the PV with matching name
                target_pv = None
                if isinstance(pv_data, dict) and 'items' in pv_data and isinstance(pv_data['items'], list):
                    # List of PVs case
                    for pv in pv_data['items']:
                        if pv.get('metadata', {}).get('name') == pv_name:
                            target_pv = pv
                            break
                elif isinstance(pv_data, dict) and pv_data.get('metadata', {}).get('name') == pv_name:
                    # Single PV case
                    target_pv = pv_data
                elif isinstance(pv_data, list):
                    # Direct list of PVs
                    for pv in pv_data:
                        if pv.get('metadata', {}).get('name') == pv_name:
                            target_pv = pv
                            break
                
                if target_pv:
                    # Extract PV phase
                    metadata['Phase'] = target_pv.get('status', {}).get('phase', 'Unknown')
                    
                    # Extract reclaim policy
                    metadata['ReclaimPolicy'] = target_pv.get('spec', {}).get('persistentVolumeReclaimPolicy', 'Unknown')
                    
                    # Extract access modes
                    metadata['AccessModes'] = target_pv.get('spec', {}).get('accessModes', [])
                    
                    # Extract capacity
                    metadata['Capacity'] = target_pv.get('spec', {}).get('capacity', {}).get('storage', '')
                    
                    # Extract disk path (if available)
                    if 'local' in target_pv.get('spec', {}):
                        metadata['diskPath'] = target_pv.get('spec', {}).get('local', {}).get('path', '')
                    elif 'hostPath' in target_pv.get('spec', {}):
                        metadata['diskPath'] = target_pv.get('spec', {}).get('hostPath', {}).get('path', '')
                    
                    # Extract node affinity
                    node_selector = target_pv.get('spec', {}).get('nodeAffinity', {}).get('required', {}).get('nodeSelectorTerms', [])
                    if node_selector and len(node_selector) > 0:
                        expressions = node_selector[0].get('matchExpressions', [])
                        for expr in expressions:
                            if expr.get('key') == 'kubernetes.io/hostname' and expr.get('operator') == 'In':
                                values = expr.get('values', [])
                                if values and len(values) > 0:
                                    metadata['nodeAffinity'] = values[0]
                                    break
            except Exception as e:
                logging.warning(f"Error parsing PV metadata with yaml package: {e}")
                # Fallback to the old method in case of parsing errors
                try:
                    pv_section = self._extract_yaml_section(pvs_output, pv_name)
                    for line in pv_section:
                        if 'phase:' in line:
                            metadata['Phase'] = line.split('phase:')[-1].strip()
                        elif 'persistentVolumeReclaimPolicy:' in line:
                            metadata['ReclaimPolicy'] = line.split('persistentVolumeReclaimPolicy:')[-1].strip()
                        elif 'storage:' in line and 'capacity:' in pvs_output:
                            metadata['Capacity'] = line.split('storage:')[-1].strip()
                        elif 'path:' in line:
                            metadata['diskPath'] = line.split('path:')[-1].strip()
                        elif 'kubernetes.io/hostname:' in line:
                            metadata['nodeAffinity'] = line.split('kubernetes.io/hostname:')[-1].strip()
                except Exception as fallback_error:
                    logging.warning(f"Fallback parsing also failed for PV metadata: {fallback_error}")
        
        return metadata
    
    def _parse_vol_metadata(self, vol_name: str) -> Dict[str, Any]:
        """Parse volume metadata from tool outputs using yaml package"""
        metadata = {
            'CSIStatus': 'CSI may not support Volume',
            'Health': 'CSI may not support Volume',
            'Id': '',
            'Location': '',
            'LocationType': 'CSI may not support Volume',
            'Mode': 'CSI may not support Volume',
            'NodeId': '',
            'OperationalStatus': 'CSI may not support Volume',
            'Owners': [],
            'Size': 0,
            'StorageClass': '',
            'Type': '',
            'Usage': 'CSI may not support Volume'
        }
    
        volumes_output = self.collected_data.get('csi_baremetal', {}).get('volumes', '')
        if volumes_output and vol_name in volumes_output:
            try:
                # Parse the YAML output
                volumes_data = yaml.safe_load(volumes_output)
                
                # Find the volume with matching name
                target_volume = None
                if isinstance(volumes_data, dict) and 'items' in volumes_data and isinstance(volumes_data['items'], list):
                    # List of volumes case
                    for volume in volumes_data['items']:
                        if volume.get('metadata', {}).get('name') == vol_name:
                            target_volume = volume
                            break
                elif isinstance(volumes_data, dict) and volumes_data.get('metadata', {}).get('name') == vol_name:
                    # Single volume case
                    target_volume = volumes_data
                elif isinstance(volumes_data, list):
                    # Direct list of volumes
                    for volume in volumes_data:
                        if volume.get('metadata', {}).get('name') == vol_name:
                            target_volume = volume
                            break
                
                if target_volume:
                    # Extract volume spec properties
                    spec = target_volume.get('spec', {})
                    metadata['CSIStatus'] = spec.get('CSIStatus', 'CSI may not support Volume')
                    metadata['Health'] = spec.get('Health', 'CSI may not support Volume')
                    metadata['Id'] = spec.get('Id', '')
                    metadata['Location'] = spec.get('Location', '')
                    metadata['LocationType'] = spec.get('LocationType', 'CSI may not support Volume')
                    metadata['Mode'] = spec.get('Mode', 'CSI may not support Volume')
                    metadata['NodeId'] = spec.get('NodeId', '')
                    metadata['OperationalStatus'] = spec.get('OperationalStatus', 'CSI may not support Volume')
                    metadata['Owners'] = spec.get('Owners', [])
                    metadata['Size'] = spec.get('Size', 0)
                    metadata['StorageClass'] = spec.get('StorageClass', '')
                    metadata['Type'] = spec.get('Type', '')
                    metadata['Usage'] = spec.get('Usage', 'CSI may not support Volume')
                else:
                    logging.warning(f"Volume {vol_name} not found in parsed YAML data")
                    
            except Exception as e:
                logging.warning(f"Error parsing volume metadata for {vol_name} with yaml package: {e}")
                # Fallback to the old method in case of parsing errors
                try:
                    vol_section = self._extract_yaml_section(volumes_output, vol_name)
                    for line in vol_section:
                        if 'CSIStatus:' in line:
                            metadata['CSIStatus'] = line.split('CSIStatus:')[-1].strip()
                        elif 'Health:' in line:
                            metadata['Health'] = line.split('Health:')[-1].strip()
                        elif 'Id:' in line:
                            metadata['Id'] = line.split('Id:')[-1].strip()
                        elif 'Location:' in line:
                            metadata['Location'] = line.split('Location:')[-1].strip()
                        elif 'LocationType:' in line:
                            metadata['LocationType'] = line.split('LocationType:')[-1].strip()
                        elif 'Mode:' in line:
                            metadata['Mode'] = line.split('Mode:')[-1].strip()
                        elif 'NodeId:' in line:
                            metadata['NodeId'] = line.split('NodeId:')[-1].strip()
                        elif 'OperationalStatus:' in line:
                            metadata['OperationalStatus'] = line.split('OperationalStatus:')[-1].strip()
                        elif 'Owners:' in line:
                            owners = line.split('Owners:')[-1].strip()
                            if owners.startswith('- '):
                                metadata['Owners'] = [owner.strip() for owner in owners.split('\n') if owner.strip()]
                            else:   
                                metadata['Owners'] = [owners.strip()]
                        elif 'Size:' in line:
                            try:
                                size_str = line.split('Size:')[-1].strip()
                                metadata['Size'] = int(size_str) if size_str.isdigit() else size_str
                            except (ValueError, TypeError):
                                pass
                        elif 'StorageClass:' in line:
                            metadata['StorageClass'] = line.split('StorageClass:')[-1].strip()
                        elif 'Type:' in line:
                            metadata['Type'] = line.split('Type:')[-1].strip()
                        elif 'Usage:' in line:
                            metadata['Usage'] = line.split('Usage:')[-1].strip()
                except Exception as fallback_error:
                    logging.warning(f"Fallback parsing also failed for volume metadata: {fallback_error}")
        else:
            logging.warning(f"Volume {vol_name} not found in CSI Baremetal volumes output")

        return metadata

    def _extract_yaml_section(self, yaml_output: str, entity_name: str) -> List[str]:
        """
        Extract YAML section for a specific entity using yaml package
        
        Args:
            yaml_output: YAML string to parse
            entity_name: Name of the entity to extract
            
        Returns:
            List of lines from the extracted section (for backward compatibility)
        """
        try:
            # Parse the YAML output
            yaml_data = yaml.safe_load(yaml_output)
            
            # Handle different YAML structures
            if yaml_data is None:
                return []
                
            # Case 1: List of items (most common Kubernetes output format)
            if isinstance(yaml_data, dict) and 'items' in yaml_data and isinstance(yaml_data['items'], list):
                for item in yaml_data['items']:
                    if item.get('metadata', {}).get('name') == entity_name:
                        # Convert back to YAML string for backward compatibility
                        entity_yaml = yaml.dump(item, default_flow_style=False)
                        return entity_yaml.split('\n')
            
            # Case 2: Single item
            elif isinstance(yaml_data, dict) and yaml_data.get('metadata', {}).get('name') == entity_name:
                entity_yaml = yaml.dump(yaml_data, default_flow_style=False)
                return entity_yaml.split('\n')
            
            # Case 3: Direct list of items
            elif isinstance(yaml_data, list):
                for item in yaml_data:
                    if isinstance(item, dict) and item.get('metadata', {}).get('name') == entity_name:
                        entity_yaml = yaml.dump(item, default_flow_style=False)
                        return entity_yaml.split('\n')
            
            # Fallback to the old method if we couldn't find the entity
            logging.warning(f"Entity {entity_name} not found in YAML using structured parsing, falling back to line-by-line method")
            lines = yaml_output.split('\n')
            section_lines = []
            in_section = False
            indent_level = 0
            
            for line in lines:
                if f'name: {entity_name}' in line:
                    in_section = True
                    indent_level = len(line) - len(line.lstrip())
                    section_lines.append(line)
                elif in_section:
                    current_indent = len(line) - len(line.lstrip())
                    if line.strip() and current_indent <= indent_level and 'name:' in line:
                        # New entity started
                        break
                    section_lines.append(line)
            
            return section_lines
            
        except Exception as e:
            logging.warning(f"Error parsing YAML for entity {entity_name}: {e}")
            # Fallback to the old method in case of parsing errors
            lines = yaml_output.split('\n')
            section_lines = []
            in_section = False
            indent_level = 0
            
            for line in lines:
                if f'name: {entity_name}' in line:
                    in_section = True
                    indent_level = len(line) - len(line.lstrip())
                    section_lines.append(line)
                elif in_section:
                    current_indent = len(line) - len(line.lstrip())
                    if line.strip() and current_indent <= indent_level and 'name:' in line:
                        # New entity started
                        break
                    section_lines.append(line)
            
            return section_lines
    
    def _parse_comprehensive_drive_info(self, drive_uuid: str) -> Dict[str, Any]:
        """Parse comprehensive drive information from CSI Baremetal tool outputs using yaml package"""
        drive_info = {
            'Health': 'UNKNOWN',
            'Status': 'UNKNOWN',
            'Type': 'UNKNOWN',
            'Size': 0,
            'Usage': 'UNKNOWN',
            'IsSystem': False,
            'Path': '',
            'SerialNumber': '',
            'Firmware': '',
            'VID': '',
            'PID': '',
            'NodeId': ''
        }
        
        drives_output = self.collected_data.get('csi_baremetal', {}).get('drives', '')
        if drives_output and drive_uuid in drives_output:
            try:
                # Parse the YAML output
                drives_data = yaml.safe_load(drives_output)
                
                # Find the drive with matching UUID
                target_drive = None
                if isinstance(drives_data, dict) and 'items' in drives_data and isinstance(drives_data['items'], list):
                    # List of drives case
                    for drive in drives_data['items']:
                        if drive.get('metadata', {}).get('name') == drive_uuid:
                            target_drive = drive
                            break
                elif isinstance(drives_data, dict) and drives_data.get('metadata', {}).get('name') == drive_uuid:
                    # Single drive case
                    target_drive = drives_data
                elif isinstance(drives_data, list):
                    # Direct list of drives
                    for drive in drives_data:
                        if drive.get('metadata', {}).get('name') == drive_uuid:
                            target_drive = drive
                            break
                
                if target_drive:
                    # Extract drive spec properties
                    spec = target_drive.get('spec', {})
                    drive_info['Health'] = spec.get('Health', 'UNKNOWN')
                    drive_info['Status'] = spec.get('Status', 'UNKNOWN')
                    drive_info['Type'] = spec.get('Type', 'UNKNOWN')
                    drive_info['Size'] = spec.get('Size', 0)
                    drive_info['Usage'] = spec.get('Usage', 'UNKNOWN')
                    drive_info['IsSystem'] = spec.get('IsSystem', False)
                    drive_info['Path'] = spec.get('Path', '')
                    drive_info['SerialNumber'] = spec.get('SerialNumber', '')
                    drive_info['Firmware'] = spec.get('Firmware', '')
                    drive_info['VID'] = spec.get('VID', '')
                    drive_info['PID'] = spec.get('PID', '')
                    drive_info['NodeId'] = spec.get('NodeId', '')
                else:
                    logging.warning(f"Drive {drive_uuid} not found in parsed YAML data")
                    
            except Exception as e:
                logging.warning(f"Error parsing drive metadata for {drive_uuid} with yaml package: {e}")
                # Fallback to the old method in case of parsing errors
                try:
                    drive_section = self._extract_yaml_section(drives_output, drive_uuid)
                    if drive_section:
                        for line in drive_section:
                            line = line.strip()
                            if 'Health:' in line:
                                drive_info['Health'] = line.split('Health:')[-1].strip()
                            elif 'Status:' in line:
                                drive_info['Status'] = line.split('Status:')[-1].strip()
                            elif 'Type:' in line:
                                drive_info['Type'] = line.split('Type:')[-1].strip()
                            elif 'Size:' in line:
                                try:
                                    size_str = line.split('Size:')[-1].strip()
                                    drive_info['Size'] = int(size_str) if size_str.isdigit() else size_str
                                except (ValueError, TypeError):
                                    pass
                            elif 'Usage:' in line:
                                drive_info['Usage'] = line.split('Usage:')[-1].strip()
                            elif 'IsSystem:' in line:
                                system_str = line.split('IsSystem:')[-1].strip().lower()
                                drive_info['IsSystem'] = system_str in ['true', 'yes', '1']
                            elif 'Path:' in line:
                                drive_info['Path'] = line.split('Path:')[-1].strip()
                            elif 'SerialNumber:' in line:
                                drive_info['SerialNumber'] = line.split('SerialNumber:')[-1].strip()
                            elif 'Firmware:' in line:
                                drive_info['Firmware'] = line.split('Firmware:')[-1].strip()
                            elif 'VID:' in line:
                                drive_info['VID'] = line.split('VID:')[-1].strip()
                            elif 'PID:' in line:
                                drive_info['PID'] = line.split('PID:')[-1].strip()
                            elif 'NodeId:' in line:
                                drive_info['NodeId'] = line.split('NodeId:')[-1].strip()
                except Exception as fallback_error:
                    logging.warning(f"Fallback parsing also failed for drive metadata: {fallback_error}")
        
        return drive_info
    
    def _parse_volume_metadata(self, volume_name: str, namespace: str = None) -> Dict[str, Any]:
        """Parse CSI Baremetal Volume metadata from tool outputs using yaml package"""
        volume_info = {
            'Health': 'UNKNOWN',
            'LocationType': 'UNKNOWN',
            'Size': 0,
            'StorageClass': '',
            'Location': '',  # This is the key field - Drive UUID or LVG name
            'Usage': 'UNKNOWN',
            'Mode': 'UNKNOWN',
            'Type': 'UNKNOWN',
            'NodeId': ''
        }
        
        volumes_output = self.collected_data.get('csi_baremetal', {}).get('volumes', '')
        if volumes_output and volume_name in volumes_output:
            try:
                # Parse the YAML output
                volumes_data = yaml.safe_load(volumes_output)
                
                # Find the volume with matching name
                target_volume = None
                if isinstance(volumes_data, dict) and 'items' in volumes_data and isinstance(volumes_data['items'], list):
                    # List of volumes case
                    for volume in volumes_data['items']:
                        if volume.get('metadata', {}).get('name') == volume_name:
                            target_volume = volume
                            break
                elif isinstance(volumes_data, dict) and volumes_data.get('metadata', {}).get('name') == volume_name:
                    # Single volume case
                    target_volume = volumes_data
                elif isinstance(volumes_data, list):
                    # Direct list of volumes
                    for volume in volumes_data:
                        if volume.get('metadata', {}).get('name') == volume_name:
                            target_volume = volume
                            break
                
                if target_volume:
                    # Extract volume spec properties
                    spec = target_volume.get('spec', {})
                    volume_info['Health'] = spec.get('health', 'UNKNOWN')
                    volume_info['LocationType'] = spec.get('locationType', 'UNKNOWN')
                    volume_info['Size'] = spec.get('size', 0)
                    volume_info['StorageClass'] = spec.get('storageClass', '')
                    volume_info['Location'] = spec.get('location', '')
                    volume_info['Usage'] = spec.get('usage', 'UNKNOWN')
                    volume_info['Mode'] = spec.get('mode', 'UNKNOWN')
                    volume_info['Type'] = spec.get('type', 'UNKNOWN')
                    volume_info['NodeId'] = spec.get('nodeId', '')
                else:
                    logging.warning(f"Volume {volume_name} not found in parsed YAML data")
                    
            except Exception as e:
                logging.warning(f"Error parsing volume metadata for {volume_name} with yaml package: {e}")
                # Fallback to the old method in case of parsing errors
                try:
                    volume_section = self._extract_yaml_section(volumes_output, volume_name)
                    if volume_section:
                        for line in volume_section:
                            line = line.strip()
                            if 'health:' in line:
                                volume_info['Health'] = line.split('health:')[-1].strip()
                            elif 'locationType:' in line:
                                volume_info['LocationType'] = line.split('locationType:')[-1].strip()
                            elif 'size:' in line:
                                try:
                                    size_str = line.split('size:')[-1].strip()
                                    volume_info['Size'] = int(size_str) if size_str.isdigit() else size_str
                                except (ValueError, TypeError):
                                    pass
                            elif 'storageClass:' in line:
                                volume_info['StorageClass'] = line.split('storageClass:')[-1].strip()
                            elif 'location:' in line:
                                volume_info['Location'] = line.split('location:')[-1].strip()
                            elif 'usage:' in line:
                                volume_info['Usage'] = line.split('usage:')[-1].strip()
                            elif 'mode:' in line:
                                volume_info['Mode'] = line.split('mode:')[-1].strip()
                            elif 'type:' in line:
                                volume_info['Type'] = line.split('type:')[-1].strip()
                            elif 'nodeId:' in line:
                                volume_info['NodeId'] = line.split('nodeId:')[-1].strip()
                except Exception as fallback_error:
                    logging.warning(f"Fallback parsing also failed for volume metadata: {fallback_error}")
        
        return volume_info
    
    def _parse_lvg_metadata(self, lvg_name: str) -> Dict[str, Any]:
        """Parse LVG metadata from CSI Baremetal tool outputs using yaml package"""
        lvg_info = {
            'Health': 'UNKNOWN',
            'Size': 0,
            'VolumeGroup': '',
            'Node': '',
            'Locations': []  # Array of Drive UUIDs
        }
        
        lvgs_output = self.collected_data.get('csi_baremetal', {}).get('lvgs', '')
        if lvgs_output and lvg_name in lvgs_output:
            try:
                # Parse the YAML output
                lvgs_data = yaml.safe_load(lvgs_output)
                
                # Find the LVG with matching name
                target_lvg = None
                if isinstance(lvgs_data, dict) and 'items' in lvgs_data and isinstance(lvgs_data['items'], list):
                    # List of LVGs case
                    for lvg in lvgs_data['items']:
                        if lvg.get('metadata', {}).get('name') == lvg_name:
                            target_lvg = lvg
                            break
                elif isinstance(lvgs_data, dict) and lvgs_data.get('metadata', {}).get('name') == lvg_name:
                    # Single LVG case
                    target_lvg = lvgs_data
                elif isinstance(lvgs_data, list):
                    # Direct list of LVGs
                    for lvg in lvgs_data:
                        if lvg.get('metadata', {}).get('name') == lvg_name:
                            target_lvg = lvg
                            break
                
                if target_lvg:
                    # Extract LVG spec properties
                    spec = target_lvg.get('spec', {})
                    lvg_info['Health'] = spec.get('health', 'UNKNOWN')
                    lvg_info['Size'] = spec.get('size', 0)
                    lvg_info['VolumeGroup'] = spec.get('volumeGroup', '')
                    lvg_info['Node'] = spec.get('node', '')
                    lvg_info['Locations'] = spec.get('locations', [])
                else:
                    logging.warning(f"LVG {lvg_name} not found in parsed YAML data")
                    
            except Exception as e:
                logging.warning(f"Error parsing LVG metadata for {lvg_name} with yaml package: {e}")
                # Fallback to the old method in case of parsing errors
                try:
                    lvg_section = self._extract_yaml_section(lvgs_output, lvg_name)
                    if lvg_section:
                        in_locations_array = False
                        for line in lvg_section:
                            line = line.strip()
                            if 'health:' in line:
                                lvg_info['Health'] = line.split('health:')[-1].strip()
                            elif 'size:' in line:
                                try:
                                    size_str = line.split('size:')[-1].strip()
                                    lvg_info['Size'] = int(size_str) if size_str.isdigit() else size_str
                                except (ValueError, TypeError):
                                    pass
                            elif 'volumeGroup:' in line:
                                lvg_info['VolumeGroup'] = line.split('volumeGroup:')[-1].strip()
                            elif 'node:' in line:
                                lvg_info['Node'] = line.split('node:')[-1].strip()
                            elif 'locations:' in line:
                                in_locations_array = True
                            elif in_locations_array and line.startswith('- '):
                                # Extract drive UUID from array item
                                drive_uuid = line[2:].strip()
                                if drive_uuid:
                                    lvg_info['Locations'].append(drive_uuid)
                            elif in_locations_array and not line.startswith('- ') and not line.startswith(' '):
                                # End of locations array
                                in_locations_array = False
                except Exception as fallback_error:
                    logging.warning(f"Fallback parsing also failed for LVG metadata: {fallback_error}")
        
        return lvg_info
    
    def _parse_ac_metadata(self, ac_name: str) -> Dict[str, Any]:
        """Parse Available Capacity metadata from CSI Baremetal tool outputs using yaml package"""
        ac_info = {
            'Size': 0,
            'StorageClass': '',
            'Location': '',  # Drive UUID or LVG name
            'Node': '',
            'NodeId': ''
        }
        
        ac_output = self.collected_data.get('csi_baremetal', {}).get('available_capacity', '')
        if ac_output and ac_name in ac_output:
            try:
                # Parse the YAML output
                ac_data = yaml.safe_load(ac_output)
                
                # Find the AC with matching name
                target_ac = None
                if isinstance(ac_data, dict) and 'items' in ac_data and isinstance(ac_data['items'], list):
                    # List of ACs case
                    for ac in ac_data['items']:
                        if ac.get('metadata', {}).get('name') == ac_name:
                            target_ac = ac
                            break
                elif isinstance(ac_data, dict) and ac_data.get('metadata', {}).get('name') == ac_name:
                    # Single AC case
                    target_ac = ac_data
                elif isinstance(ac_data, list):
                    # Direct list of ACs
                    for ac in ac_data:
                        if ac.get('metadata', {}).get('name') == ac_name:
                            target_ac = ac
                            break
                
                if target_ac:
                    # Extract AC spec properties
                    spec = target_ac.get('spec', {})
                    ac_info['Size'] = spec.get('size', 0)
                    ac_info['StorageClass'] = spec.get('storageClass', '')
                    ac_info['Location'] = spec.get('location', '')
                    ac_info['Node'] = spec.get('node', '')
                    ac_info['NodeId'] = spec.get('nodeId', '')
                else:
                    logging.warning(f"AC {ac_name} not found in parsed YAML data")
                    
            except Exception as e:
                logging.warning(f"Error parsing AC metadata for {ac_name} with yaml package: {e}")
                # Fallback to the old method in case of parsing errors
                try:
                    ac_section = self._extract_yaml_section(ac_output, ac_name)
                    if ac_section:
                        for line in ac_section:
                            line = line.strip()
                            if 'size:' in line:
                                try:
                                    size_str = line.split('size:')[-1].strip()
                                    ac_info['Size'] = int(size_str) if size_str.isdigit() else size_str
                                except (ValueError, TypeError):
                                    pass
                            elif 'storageClass:' in line:
                                ac_info['StorageClass'] = line.split('storageClass:')[-1].strip()
                            elif 'location:' in line:
                                ac_info['Location'] = line.split('location:')[-1].strip()
                            elif 'node:' in line:
                                ac_info['Node'] = line.split('node:')[-1].strip()
                            elif 'nodeId:' in line:
                                ac_info['NodeId'] = line.split('nodeId:')[-1].strip()
                except Exception as fallback_error:
                    logging.warning(f"Fallback parsing also failed for AC metadata: {fallback_error}")
        
        return ac_info
    
    def _parse_csibmnode_mapping(self) -> Dict[str, str]:
        """Parse CSI Baremetal node mapping to get UUID to hostname mapping using yaml package"""
        node_mapping = {}  # UUID -> hostname
        
        csibm_nodes_output = self.collected_data.get('csi_baremetal', {}).get('nodes', '')
        if csibm_nodes_output:
            try:
                # Parse the YAML output
                nodes_data = yaml.safe_load(csibm_nodes_output)
                
                # Process CSI Baremetal node data
                if nodes_data:
                    # Handle different YAML structures
                    node_items = []
                    if isinstance(nodes_data, dict) and 'items' in nodes_data and isinstance(nodes_data['items'], list):
                        # List of nodes case
                        node_items = nodes_data['items']
                    elif isinstance(nodes_data, list):
                        # Direct list of nodes
                        node_items = nodes_data
                    
                    # Extract UUID to hostname mapping
                    for node in node_items:
                        if isinstance(node, dict):
                            node_uuid = node.get('metadata', {}).get('name', '')
                            # Check if it's a UUID format (typically long string with hyphens)
                            if len(node_uuid) > 30:
                                hostname = node.get('spec', {}).get('hostname', '')
                                if hostname:
                                    node_mapping[node_uuid] = hostname
                    
            except Exception as e:
                logging.warning(f"Error parsing CSI Baremetal node mapping with yaml package: {e}")
                # Fallback to the old method in case of parsing errors
                try:
                    lines = csibm_nodes_output.split('\n')
                    current_uuid = None
                    current_hostname = None
                    
                    for line in lines:
                        line = line.strip()
                        if 'name:' in line and len(line.split('name:')[-1].strip()) > 30:  # UUID format
                            current_uuid = line.split('name:')[-1].strip()
                        elif 'hostname:' in line:
                            current_hostname = line.split('hostname:')[-1].strip()
                            if current_uuid and current_hostname:
                                node_mapping[current_uuid] = current_hostname
                                current_uuid = None
                                current_hostname = None
                except Exception as fallback_error:
                    logging.warning(f"Fallback parsing also failed for CSI Baremetal node mapping: {fallback_error}")
        
        return node_mapping
    
    def _parse_smart_data(self, drive_uuid: str) -> Dict[str, Any]:
        """Parse SMART data for drive health analysis"""
        smart_info = {
            'PowerOnHours': 0,
            'PowerCycleCount': 0,
            'ReallocatedSectorCount': 0,
            'CurrentPendingSectorCount': 0,
            'UncorrectableErrorCount': 0,
            'Temperature': 0,
            'OverallHealth': 'UNKNOWN',
            'SmartStatus': 'UNKNOWN'
        }
        
        smart_data = self.collected_data.get('smart_data', {}).get(drive_uuid, '')
        if smart_data:
            try:
                lines = smart_data.split('\n')
                for line in lines:
                    line = line.strip()
                    line_lower = line.lower()
                    
                    # Overall SMART status
                    if 'smart overall-health self-assessment test result:' in line_lower:
                        smart_info['SmartStatus'] = line.split(':')[-1].strip()
                        smart_info['OverallHealth'] = 'GOOD' if 'passed' in line_lower else 'BAD'
                    
                    # Power on hours
                    elif 'power_on_hours' in line_lower or '9 power_on_hours' in line_lower:
                        parts = line.split()
                        if len(parts) >= 10:
                            try:
                                smart_info['PowerOnHours'] = int(parts[9])
                            except (ValueError, IndexError):
                                pass
                    
                    # Power cycle count
                    elif 'power_cycle_count' in line_lower or '12 power_cycle_count' in line_lower:
                        parts = line.split()
                        if len(parts) >= 10:
                            try:
                                smart_info['PowerCycleCount'] = int(parts[9])
                            except (ValueError, IndexError):
                                pass
                    
                    # Reallocated sector count
                    elif 'reallocated_sector_ct' in line_lower or '5 reallocated_sector_ct' in line_lower:
                        parts = line.split()
                        if len(parts) >= 10:
                            try:
                                smart_info['ReallocatedSectorCount'] = int(parts[9])
                            except (ValueError, IndexError):
                                pass
                    
                    # Current pending sector count
                    elif 'current_pending_sector' in line_lower or '197 current_pending_sector' in line_lower:
                        parts = line.split()
                        if len(parts) >= 10:
                            try:
                                smart_info['CurrentPendingSectorCount'] = int(parts[9])
                            except (ValueError, IndexError):
                                pass
                    
                    # Uncorrectable error count
                    elif 'offline_uncorrectable' in line_lower or '198 offline_uncorrectable' in line_lower:
                        parts = line.split()
                        if len(parts) >= 10:
                            try:
                                smart_info['UncorrectableErrorCount'] = int(parts[9])
                            except (ValueError, IndexError):
                                pass
                    
                    # Temperature
                    elif 'temperature_celsius' in line_lower or '194 temperature_celsius' in line_lower:
                        parts = line.split()
                        if len(parts) >= 10:
                            try:
                                smart_info['Temperature'] = int(parts[9])
                            except (ValueError, IndexError):
                                pass
            except Exception as e:
                logging.warning(f"Error parsing SMART data for {drive_uuid}: {e}")
        
        return smart_info
    
    def _parse_comprehensive_node_info(self, node_name: str) -> Dict[str, Any]:
        """
        Parse comprehensive node information from tool outputs using YAML parser
        
        Args:
            node_name: Name of the node to parse information for
            
        Returns:
            Dictionary containing parsed node information
        """
        node_info = {
            'Ready': False,
            'DiskPressure': False,
            'MemoryPressure': False,
            'PIDPressure': False,
            'NetworkUnavailable': False,
            'KubeletVersion': '',
            'ContainerRuntimeVersion': '',
            'KernelVersion': '',
            'OSImage': '',
            'Architecture': '',
            'Capacity': {},
            'Allocatable': {}
        }
        
        nodes_output = self.collected_data.get('kubernetes', {}).get('nodes', '')
        if nodes_output and node_name in nodes_output:
            try:
                # Parse the YAML output
                nodes_data = yaml.safe_load(nodes_output)
                
                # Find the node with matching name
                target_node = None
                if isinstance(nodes_data, dict) and 'items' in nodes_data and isinstance(nodes_data['items'], list):
                    # List of nodes case
                    for node in nodes_data['items']:
                        if node.get('metadata', {}).get('name') == node_name:
                            target_node = node
                            break
                elif isinstance(nodes_data, list):
                    # Direct list of nodes
                    for node in nodes_data:
                        if node.get('metadata', {}).get('name') == node_name:
                            target_node = node
                            break
                
                if not target_node:
                    logging.warning(f"Node {node_name} not found in the provided YAML output")
                    return node_info
                
                # Extract status information
                status = target_node.get('status', {})
                
                # Extract conditions
                conditions = status.get('conditions', [])
                for condition in conditions:
                    condition_type = condition.get('type', '')
                    condition_status = condition.get('status', '').lower() == 'true'
                    
                    if condition_type == 'Ready':
                        node_info['Ready'] = condition_status
                    elif condition_type == 'DiskPressure':
                        node_info['DiskPressure'] = condition_status
                    elif condition_type == 'MemoryPressure':
                        node_info['MemoryPressure'] = condition_status
                    elif condition_type == 'PIDPressure':
                        node_info['PIDPressure'] = condition_status
                    elif condition_type == 'NetworkUnavailable':
                        node_info['NetworkUnavailable'] = condition_status
                
                # Extract node info
                node_info_data = target_node.get('status', {}).get('nodeInfo', {})
                node_info['KubeletVersion'] = node_info_data.get('kubeletVersion', '')
                node_info['ContainerRuntimeVersion'] = node_info_data.get('containerRuntimeVersion', '')
                node_info['KernelVersion'] = node_info_data.get('kernelVersion', '')
                node_info['OSImage'] = node_info_data.get('osImage', '')
                node_info['Architecture'] = node_info_data.get('architecture', '')
                
                # Extract capacity and allocatable resources
                node_info['Capacity'] = status.get('capacity', {})
                node_info['Allocatable'] = status.get('allocatable', {})
                
            except Exception as e:
                logging.warning(f"Error parsing node metadata for {node_name} using YAML parser: {e}")
                # Fallback to the old method in case of parsing errors
                try:
                    node_section = nodes_output.split('\n')
                    for line in node_section:
                        line = line.strip()
                        if 'ready' in line and 'status:' in line:
                            node_info['Ready'] = 'True' in line
                        elif 'diskPressure' in line and 'status:' in line:
                            node_info['DiskPressure'] = 'True' in line
                        elif 'memoryPressure' in line and 'status:' in line:
                            node_info['MemoryPressure'] = 'True' in line
                        elif 'PIDPressure' in line and 'status:' in line:
                            node_info['PIDPressure'] = 'True' in line
                        elif 'networkUnavailable' in line and 'status:' in line:
                            node_info['NetworkUnavailable'] = 'True' in line
                        elif 'kubeletVersion:' in line:
                            node_info['KubeletVersion'] = line.split('kubeletVersion:')[-1].strip()
                        elif 'containerRuntimeVersion:' in line:
                            node_info['ContainerRuntimeVersion'] = line.split('containerRuntimeVersion:')[-1].strip()
                        elif 'kernelVersion:' in line:
                            node_info['KernelVersion'] = line.split('kernelVersion:')[-1].strip()
                        elif 'osImage:' in line:
                            node_info['OSImage'] = line.split('osImage:')[-1].strip()
                        elif 'architecture:' in line:
                            node_info['Architecture'] = line.split('architecture:')[-1].strip()
                except Exception as fallback_error:
                    logging.warning(f"Fallback parsing also failed for node {node_name}: {fallback_error}")
        
        return node_info
    
    def _parse_dmesg_issues(self) -> List[Dict[str, Any]]:
        """Parse dmesg logs to identify storage-related issues"""
        issues = []
        dmesg_output = self.collected_data.get('system', {}).get('kernel_logs', '')
        
        if not dmesg_output:
            return issues
        
        try:
            lines = dmesg_output.split('\n')
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # Parse timestamp and message
                issue = self._extract_dmesg_issue(line)
                if issue:
                    issues.append(issue)
        
        except Exception as e:
            logging.warning(f"Error parsing dmesg issues: {e}")
        
        return issues
    
    def _extract_dmesg_issue(self, line: str) -> Dict[str, Any]:
        """Extract issue information from a dmesg log line"""
        issue = None
        line_lower = line.lower()
        
        # Disk/Drive hardware errors
        if any(keyword in line_lower for keyword in ['disk error', 'drive error', 'bad sector', 'i/o error']):
            severity = 'critical' if 'bad sector' in line_lower else 'high'
            issue = {
                'type': 'disk_hardware_error',
                'severity': severity,
                'description': f"Hardware disk error detected: {line}",
                'raw_log': line,
                'source': 'dmesg'
            }
        
        # NVMe/SSD specific issues
        elif any(keyword in line_lower for keyword in ['nvme', 'ssd']) and any(error in line_lower for error in ['error', 'fail', 'timeout']):
            issue = {
                'type': 'nvme_ssd_error',
                'severity': 'high',
                'description': f"NVMe/SSD error detected: {line}",
                'raw_log': line,
                'source': 'dmesg'
            }
        
        # Filesystem errors
        elif any(keyword in line_lower for keyword in ['xfs', 'ext4']) and any(error in line_lower for error in ['error', 'corrupt', 'fail']):
            issue = {
                'type': 'filesystem_error',
                'severity': 'high',
                'description': f"Filesystem error detected: {line}",
                'raw_log': line,
                'source': 'dmesg'
            }
        
        # I/O timeout issues
        elif 'timeout' in line_lower and any(keyword in line_lower for keyword in ['i/o', 'io', 'disk', 'drive']):
            issue = {
                'type': 'io_timeout',
                'severity': 'medium',
                'description': f"I/O timeout detected: {line}",
                'raw_log': line,
                'source': 'dmesg'
            }
        
        # Controller/SCSI/SATA issues
        elif any(keyword in line_lower for keyword in ['controller', 'scsi', 'sata']) and any(error in line_lower for error in ['error', 'fail', 'reset']):
            issue = {
                'type': 'controller_error',
                'severity': 'high',
                'description': f"Storage controller error detected: {line}",
                'raw_log': line,
                'source': 'dmesg'
            }
        
        return issue
    
    def _parse_journal_issues(self) -> List[Dict[str, Any]]:
        """Parse systemd journal logs to identify storage and service issues"""
        issues = []
        
        # Parse different journal log types
        storage_logs = self.collected_data.get('system', {}).get('journal_storage_logs', '')
        kubelet_logs = self.collected_data.get('system', {}).get('journal_kubelet_logs', '')
        boot_logs = self.collected_data.get('system', {}).get('journal_boot_logs', '')
        
        # Parse storage-related journal logs
        if storage_logs:
            issues.extend(self._extract_journal_storage_issues(storage_logs))
        
        # Parse kubelet service logs
        if kubelet_logs:
            issues.extend(self._extract_journal_kubelet_issues(kubelet_logs))
        
        # Parse boot-time hardware detection issues
        if boot_logs:
            issues.extend(self._extract_journal_boot_issues(boot_logs))
        
        return issues
    
    def _extract_journal_storage_issues(self, logs: str) -> List[Dict[str, Any]]:
        """Extract storage-related issues from journal logs"""
        issues = []
        
        try:
            lines = logs.split('\n')
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                line_lower = line.lower()
                
                # Storage service failures
                if any(keyword in line_lower for keyword in ['failed', 'error', 'timeout']) and any(service in line_lower for service in ['mount', 'umount', 'filesystem']):
                    issues.append({
                        'type': 'storage_service_error',
                        'severity': 'high',
                        'description': f"Storage service error: {line}",
                        'raw_log': line,
                        'source': 'journal_storage'
                    })
                
                # Disk/drive detection issues
                elif any(keyword in line_lower for keyword in ['disk', 'drive', 'nvme', 'ssd']) and 'detected' in line_lower:
                    issues.append({
                        'type': 'hardware_detection',
                        'severity': 'low',
                        'description': f"Hardware detection event: {line}",
                        'raw_log': line,
                        'source': 'journal_storage'
                    })
        
        except Exception as e:
            logging.warning(f"Error parsing journal storage issues: {e}")
        
        return issues
    
    def _extract_journal_kubelet_issues(self, logs: str) -> List[Dict[str, Any]]:
        """Extract kubelet volume-related issues from journal logs"""
        issues = []
        
        try:
            lines = logs.split('\n')
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                line_lower = line.lower()
                
                # Volume mount/attach failures
                if any(keyword in line_lower for keyword in ['volume', 'mount', 'attach']) and any(error in line_lower for error in ['failed', 'error', 'timeout']):
                    issues.append({
                        'type': 'kubelet_volume_error',
                        'severity': 'high',
                        'description': f"Kubelet volume error: {line}",
                        'raw_log': line,
                        'source': 'journal_kubelet'
                    })
                
                # CSI driver issues
                elif 'csi' in line_lower and any(error in line_lower for error in ['failed', 'error', 'timeout']):
                    issues.append({
                        'type': 'csi_driver_error',
                        'severity': 'high',
                        'description': f"CSI driver error: {line}",
                        'raw_log': line,
                        'source': 'journal_kubelet'
                    })
        
        except Exception as e:
            logging.warning(f"Error parsing journal kubelet issues: {e}")
        
        return issues
    
    def _extract_journal_boot_issues(self, logs: str) -> List[Dict[str, Any]]:
        """Extract boot-time hardware and storage initialization issues"""
        issues = []
        
        try:
            lines = logs.split('\n')
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                line_lower = line.lower()
                
                # Hardware initialization failures
                if any(keyword in line_lower for keyword in ['failed to initialize', 'hardware error', 'device not found']):
                    issues.append({
                        'type': 'boot_hardware_error',
                        'severity': 'critical',
                        'description': f"Boot-time hardware error: {line}",
                        'raw_log': line,
                        'source': 'journal_boot'
                    })
                
                # Drive/controller detection issues
                elif any(keyword in line_lower for keyword in ['drive', 'controller', 'nvme', 'ssd']) and any(issue in line_lower for issue in ['not found', 'failed', 'error']):
                    issues.append({
                        'type': 'boot_storage_detection',
                        'severity': 'high',
                        'description': f"Boot-time storage detection issue: {line}",
                        'raw_log': line,
                        'source': 'journal_boot'
                    })
        
        except Exception as e:
            logging.warning(f"Error parsing journal boot issues: {e}")
        
        return issues
</file>

<file path="phases/kg_context_builder.py">
#!/usr/bin/env python3
"""
Knowledge Graph Context Builder for Investigation Planning

This module contains utilities for preparing Knowledge Graph context
for consumption by the Investigation Planner and LLM.
"""

import logging
from typing import Dict, List, Any, Set
from knowledge_graph import KnowledgeGraph
from phases.utils import validate_knowledge_graph

logger = logging.getLogger(__name__)

class KGContextBuilder:
    """
    Prepares Knowledge Graph context for Investigation Planning
    
    Extracts relevant nodes, relationships, and issues from the Knowledge Graph
    to provide context for investigation planning.
    """
    
    def __init__(self, knowledge_graph):
        """
        Initialize the Knowledge Graph Context Builder
        
        Args:
            knowledge_graph: KnowledgeGraph instance from Phase 0
        """
        self.kg = knowledge_graph
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # Validate knowledge_graph is a KnowledgeGraph instance
        validate_knowledge_graph(self.kg, self.__class__.__name__)
    
    def prepare_kg_context(self, pod_name: str, namespace: str, volume_path: str) -> Dict[str, Any]:
        """
        Prepare Knowledge Graph context for LLM consumption
        
        Args:
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod
            volume_path: Path of the volume with I/O error
            
        Returns:
            Dict[str, Any]: Structured Knowledge Graph context
        """
        # Extract relevant nodes and relationships from Knowledge Graph
        target_entities = self.identify_target_entities(pod_name, namespace)
        issues_analysis = self.analyze_existing_issues()
        
        # Format Knowledge Graph data for LLM
        kg_context = {
            "nodes": [],
            "relationships": [],
            "issues": self.kg.get_all_issues(),
            "historical_experiences": []
        }
        
        # Add target pod and related entities
        if "pod" in target_entities:
            pod_id = target_entities["pod"]
            kg_context["nodes"].append(self.format_node_for_llm(pod_id))
            
            # Add PVC, PV, Drive chain
            for gnode_subtype in ["pvc", "pv", "drive", "node"]:
                if gnode_subtype in target_entities:
                    entity_id = target_entities[gnode_subtype]
                    kg_context["nodes"].append(self.format_node_for_llm(entity_id))
                    
                    # Add relationships
                    if gnode_subtype == "pvc" and "pod" in target_entities:
                        kg_context["relationships"].append({
                            "source": target_entities["pod"],
                            "target": entity_id,
                            "type": "uses"
                        })
                    elif gnode_subtype == "pv" and "pvc" in target_entities:
                        kg_context["relationships"].append({
                            "source": target_entities["pvc"],
                            "target": entity_id,
                            "type": "bound_to"
                        })
                    elif gnode_subtype == "drive" and "pv" in target_entities:
                        kg_context["relationships"].append({
                            "source": target_entities["pv"],
                            "target": entity_id,
                            "type": "maps_to"
                        })
                    elif gnode_subtype == "node" and "drive" in target_entities:
                        kg_context["relationships"].append({
                            "source": target_entities["drive"],
                            "target": entity_id,
                            "type": "located_on"
                        })
        
        # Add critical and high severity issues
        critical_issues = issues_analysis["by_severity"]["critical"]
        high_issues = issues_analysis["by_severity"]["high"]
        
        # Add nodes with issues
        issue_node_ids = set()
        for issue in critical_issues + high_issues:
            node_id = issue.get('node_id', '')
            if node_id and node_id not in issue_node_ids and self.kg.graph.has_node(node_id):
                kg_context["nodes"].append(self.format_node_for_llm(node_id))
                issue_node_ids.add(node_id)
        
        # Add all historical experience data
        historical_experience_nodes = self.kg.find_nodes_by_type('HistoricalExperience')
        for node_id in historical_experience_nodes:
            historical_exp = self.format_node_for_llm(node_id)
            kg_context["historical_experiences"].append(historical_exp)
            
            # Also add to nodes list
            kg_context["nodes"].append(historical_exp)
            
            # Add relationships between historical experience and related system components
            # These relationships will be created when loading the historical experience data
            related_nodes = self.kg.find_connected_nodes(node_id)
            for related_id in related_nodes:
                kg_context["relationships"].append({
                    "source": node_id,
                    "target": related_id,
                    "type": "related_to"
                })
        
        # Add summary statistics
        kg_context["summary"] = self.kg.get_summary()
        
        return kg_context
    
    def format_node_for_llm(self, node_id: str) -> Dict[str, Any]:
        """
        Format a node for LLM consumption
        
        Args:
            node_id: Node ID
            
        Returns:
            Dict[str, Any]: Formatted node
        """
        if not self.kg.graph.has_node(node_id):
            return {"id": node_id, "type": "unknown", "attributes": {}}
        
        node_attrs = dict(self.kg.graph.nodes[node_id])
        return {
            "id": node_id,
            "type": node_attrs.get("gnode_subtype", "unknown"),
            "attributes": {k: v for k, v in node_attrs.items() 
                          if k not in ["gnode_subtype", "issues"]},
            "issues": node_attrs.get("issues", [])
        }
    
    def analyze_existing_issues(self) -> Dict[str, Any]:
        """
        Analyze existing issues in the Knowledge Graph
        
        Returns:
            Dict[str, Any]: Analysis of current issues by severity and type
        """
        try:
            all_issues = self.kg.get_all_issues()
            return self._categorize_issues(all_issues)
            
        except Exception as e:
            self.logger.warning(f"Error analyzing existing issues: {str(e)}")
            return {"by_severity": {"critical": [], "high": [], "medium": [], "low": []}, 
                   "by_type": {}, "total_count": 0, "entities_with_issues": []}
    
    def _categorize_issues(self, issues: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Categorize issues by severity and type
        
        Args:
            issues: List of issues to categorize
            
        Returns:
            Dict[str, Any]: Categorized issues
        """
        issue_analysis = {
            "by_severity": {"critical": [], "high": [], "medium": [], "low": []},
            "by_type": {},
            "total_count": len(issues),
            "entities_with_issues": []
        }
        
        for issue in issues:
            severity = issue.get('severity', 'unknown')
            issue_type = issue.get('type', 'unknown')
            node_id = issue.get('node_id', '')
            
            # Group by severity
            if severity in issue_analysis["by_severity"]:
                issue_analysis["by_severity"][severity].append(issue)
            
            # Group by type
            if issue_type not in issue_analysis["by_type"]:
                issue_analysis["by_type"][issue_type] = []
            issue_analysis["by_type"][issue_type].append(issue)
            
            # Track entities with issues
            if node_id and node_id not in issue_analysis["entities_with_issues"]:
                issue_analysis["entities_with_issues"].append(node_id)
        
        return issue_analysis
    
    def identify_target_entities(self, pod_name: str, namespace: str) -> Dict[str, str]:
        """
        Identify target entities in the Knowledge Graph for the given pod
        
        Args:
            pod_name: Name of the pod
            namespace: Namespace of the pod
            
        Returns:
            Dict[str, str]: Dictionary mapping entity types to their IDs
        """
        target_entities = {"pod": f"gnode:Pod:{namespace}/{pod_name}"}
        
        try:
            # Find the pod node ID
            pod_node_id = self._find_pod_node_id(pod_name, namespace)
            target_entities["pod"] = pod_node_id
            
            # Trace the volume chain if pod exists
            if self.kg.graph.has_node(pod_node_id):
                self._trace_volume_chain(pod_node_id, target_entities)

            for _, target, _ in self.kg.graph.out_edges(pod_node_id, data=True):
                target_attrs = self.kg.graph.nodes[target]
                if target_attrs.get('gnode_subtype') == 'Node':
                    target_entities["node"] = target
                    break

        except Exception as e:
            self.logger.warning(f"Error identifying target entities: {str(e)}")
        
        return target_entities
    
    def _find_pod_node_id(self, pod_name: str, namespace: str) -> str:
        """
        Find the node ID for a pod in the knowledge graph
        
        Args:
            pod_name: Name of the pod
            namespace: Namespace of the pod
            
        Returns:
            str: Node ID for the pod
        """
        # Try standard format 
        # Pod: "gnode:Pod:<namespace>/<name>" (example: "gnode:Pod:default/test-pod-1-0")
        pod_node_id = f"gnode:Pod:{namespace}/{pod_name}"
        if self.kg.graph.has_node(pod_node_id):
            return pod_node_id
            
        # Try alternative format
        pod_node_id = f"gnode:Pod:{pod_name}"
        if self.kg.graph.has_node(pod_node_id):
            return pod_node_id
            
        # Search by name and namespace attributes
        for node_id, attrs in self.kg.graph.nodes(data=True):
            if (attrs.get('gnode_subtype') == 'Pod' and 
                attrs.get('name') == pod_name and
                attrs.get('namespace') == namespace):
                return node_id
                
        # Return default if not found
        return f"gnode:Pod:{namespace}/{pod_name}"
    
    def _trace_volume_chain(self, pod_node_id: str, target_entities: Dict[str, str]) -> None:
        """
        Trace the volume chain from Pod -> PVC -> PV -> Drive -> Node
        
        Args:
            pod_node_id: Node ID for the pod
            target_entities: Dictionary to update with found entities
        """
        # Find connected PVCs
        for _, target, _ in self.kg.graph.out_edges(pod_node_id, data=True):
            target_attrs = self.kg.graph.nodes[target]
            if target_attrs.get('gnode_subtype') == 'PVC':
                target_entities["pvc"] = target
                self._trace_pvc_to_pv(target, target_entities)
                break
    
    def _trace_pvc_to_pv(self, pvc_node_id: str, target_entities: Dict[str, str]) -> None:
        """
        Trace from PVC to PV
        
        Args:
            pvc_node_id: Node ID for the PVC
            target_entities: Dictionary to update with found entities
        """
        # Find connected PV
        for _, pv_target, _ in self.kg.graph.out_edges(pvc_node_id, data=True):
            pv_attrs = self.kg.graph.nodes[pv_target]
            if pv_attrs.get('gnode_subtype') == 'PV':
                target_entities["pv"] = pv_target
                self._trace_pv_to_drive(pv_target, target_entities)
                break
    
    def _trace_pv_to_drive(self, pv_node_id: str, target_entities: Dict[str, str]) -> None:
        """
        Trace from PV to Drive
        
        Args:
            pv_node_id: Node ID for the PV
            target_entities: Dictionary to update with found entities
        """
        # Find connected Drive
        for _, drive_target, _ in self.kg.graph.out_edges(pv_node_id, data=True):
            drive_attrs = self.kg.graph.nodes[drive_target]
            if drive_attrs.get('gnode_subtype') == 'Drive':
                target_entities["drive"] = drive_target
                self._trace_drive_to_node(drive_target, target_entities)
                break
    
    def _trace_drive_to_node(self, drive_node_id: str, target_entities: Dict[str, str]) -> None:
        """
        Trace from Drive to Node
        
        Args:
            drive_node_id: Node ID for the Drive
            target_entities: Dictionary to update with found entities
        """
        # Find the Node hosting the drive
        for _, node_target, _ in self.kg.graph.out_edges(drive_node_id, data=True):
            node_attrs = self.kg.graph.nodes[node_target]
            if node_attrs.get('gnode_subtype') == 'Node':
                target_entities["node"] = node_target
                break
</file>

<file path="tools/diagnostics/disk_monitoring.py">
#!/usr/bin/env python3
"""
Disk monitoring tools for volume troubleshooting.

This module contains tools for monitoring disk status changes and detecting
jitter in disk hardware status.
"""

import time
from datetime import datetime
from typing import Dict, List, Optional
from langchain_core.tools import tool

@tool
def detect_disk_jitter(duration_minutes: int = 5, check_interval_seconds: int = 30, 
                      jitter_threshold: int = 3, node_name: str = None, 
                      drive_uuid: str = None) -> str:
    """
    Detect intermittent online/offline jitter in disk hardware status
    
    This tool monitors disk status changes by periodically running 'kubectl get drive -o wide'
    and detects if a disk frequently switches between online and offline states within
    the specified time window.
    
    Args:
        duration_minutes: Monitoring duration in minutes
        check_interval_seconds: Interval between status checks in seconds
        jitter_threshold: Number of status changes that constitute jitter
        node_name: Specific K8s node to check (optional)
        drive_uuid: Specific drive UUID to monitor (optional, monitors all if not specified)
        
    Returns:
        str: Jitter detection report with timestamps and frequency
    """
    try:
        from tools.kubernetes.csi_baremetal import kubectl_get_drive
        from tools.diagnostics.hardware import ssh_execute
        # todo
        duration_minutes = 1
        # Calculate iterations based on duration and interval
        iterations = int((duration_minutes * 60) / check_interval_seconds)
        
        # Initialize tracking dictionaries
        status_history = {}  # {drive_uuid: [(timestamp, status), ...]}
        status_changes = {}  # {drive_uuid: count}
        drive_info = {}     # {drive_uuid: {node, serial, etc}}
        
        print(f"Starting disk jitter detection for {duration_minutes} minutes "
              f"(checking every {check_interval_seconds} seconds)...")
        
        # Run monitoring loop
        for i in range(iterations):
            # Get current timestamp
            current_time = datetime.now()
            timestamp = current_time.strftime("%Y-%m-%d %H:%M:%S")
            
            # Get drive status
            drive_output = kubectl_get_drive.invoke({
                    'resource_type': 'drive',
                    'resource_name': drive_uuid,
                    'output_format': 'wide'
                })
            
            # Parse drive status output
            for line in drive_output.strip().split('\n')[1:]:  # Skip header
                parts = line.split()
                if len(parts) >= 5:  # Ensure we have enough parts
                    current_uuid = parts[0]
                    current_status = parts[4]  # STATUS column
                    
                    # Skip if we're monitoring a specific drive and this isn't it
                    if drive_uuid and current_uuid != drive_uuid:
                        continue
                        
                    # Skip if we're monitoring a specific node and this isn't it
                    if node_name:
                        drive_node = parts[9] if len(parts) > 9 else ""
                        if node_name not in drive_node:
                            continue
                    
                    # Store drive info if we don't have it yet
                    if current_uuid not in drive_info:
                        drive_info[current_uuid] = {
                            "serial": parts[8] if len(parts) > 8 else "Unknown",
                            "node": parts[9] if len(parts) > 9 else "Unknown",
                            "path": parts[7] if len(parts) > 7 else "Unknown"
                        }
                    
                    # Initialize history if needed
                    if current_uuid not in status_history:
                        status_history[current_uuid] = []
                        status_changes[current_uuid] = 0
                    
                    # Check for status change
                    if status_history[current_uuid] and status_history[current_uuid][-1][1] != current_status:
                        status_changes[current_uuid] += 1
                        print(f"[{timestamp}] Drive {current_uuid} changed from "
                              f"{status_history[current_uuid][-1][1]} to {current_status}")
                    
                    # Record current status
                    status_history[current_uuid].append((timestamp, current_status))
            
            # Sleep before next check (unless it's the last iteration)
            if i < iterations - 1:
                time.sleep(check_interval_seconds)
        
        # Generate report
        report_lines = [f"Disk Jitter Detection Report ({duration_minutes} minutes monitoring period):"]
        report_lines.append("-" * 80)
        
        jitter_detected = False
        for uuid, changes in status_changes.items():
            if changes >= jitter_threshold:
                jitter_detected = True
                info = drive_info.get(uuid, {})
                serial = info.get("serial", "Unknown")
                node = info.get("node", "Unknown")
                
                # Get status change details
                status_sequence = [f"{ts}: {status}" for ts, status in status_history[uuid]]
                
                report_lines.append(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Disk Jitter Detected: "
                                   f"Disk {uuid} (Serial: {serial}) switched status "
                                   f"{changes} times in {duration_minutes} minutes on node {node}.")
                report_lines.append(f"Status change sequence: {' -> '.join(s[1] for s in status_history[uuid])}")
                report_lines.append(f"Detailed timeline: {', '.join(status_sequence)}")
                report_lines.append("-" * 80)
        
        if not jitter_detected:
            report_lines.append("No disk jitter detected during the monitoring period.")
        
        return "\n".join(report_lines)
    
    except Exception as e:
        return f"Error during disk jitter detection: {str(e)}"
</file>

<file path="tools/kubernetes/core.py">
#!/usr/bin/env python3
"""
Core Kubernetes tools for volume troubleshooting.

This module contains basic kubectl operations and general Kubernetes
resource management tools.
"""

import subprocess
import shlex
from langchain_core.tools import tool

@tool
def kubectl_get(resource_type: str, resource_name: str = None, namespace: str = None, output_format: str = "yaml") -> str:
    """
    Execute kubectl get command
    
    Args:
        resource_type: Type of resource (pod, pvc, pv, node, etc.)
        resource_name: Name of resource (optional) (e.g. test-pod-1)
        namespace: Namespace (optional) (e.g. default)
        output_format: Output format (yaml, json, wide, etc.)
        
    Returns:
        str: Command output
    """
    cmd = ["kubectl", "get", resource_type]
    
    if resource_name:
        cmd.append(resource_name)
    
    if namespace:
        cmd.extend(["-n", namespace])
        
    if output_format:
        cmd.extend(["-o", output_format])
    else:
        cmd.append("-o=wide")

    # Execute command
    try:
        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        return f"Error: {e.stderr}"
    except Exception as e:
        return f"Error executing kubectl get: {str(e)}"

@tool
def kubectl_describe(resource_type: str, resource_name: str, namespace: str = None) -> str:
    """
    Execute kubectl describe command
    
    Args:
        resource_type: Type of resource (pod, pvc, pv, node, etc.)
        resource_name: Name of resource (e.g. test-pod-1)
        namespace: Namespace (optional)
        
    Returns:
        str: Command output
    """
    cmd = ["kubectl", "describe", resource_type, resource_name]
    
    if namespace:
        cmd.extend(["-n", namespace])
    
    # Execute command
    try:
        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        return f"Error: {e.stderr}"
    except Exception as e:
        return f"Error executing kubectl describe: {str(e)}"

@tool
def kubectl_apply(yaml_content: str, namespace: str = None) -> str:
    """
    Execute kubectl apply with provided YAML content
    
    Args:
        yaml_content: YAML content to apply
        namespace: Namespace (optional)
        
    Returns:
        str: Command output
    """
    cmd = ["kubectl", "apply", "-f", "-"]
    
    if namespace:
        cmd.extend(["-n", namespace])
    
    # Execute command
    try:
        result = subprocess.run(cmd, input=yaml_content, check=True, 
                               stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        return f"Error: {e.stderr}"
    except Exception as e:
        return f"Error executing kubectl apply: {str(e)}"

@tool
def kubectl_delete(resource_type: str, resource_name: str, namespace: str = None) -> str:
    """
    Execute kubectl delete command
    
    Args:
        resource_type: Type of resource (pod, pvc, pv, node, etc.)
        resource_name: Name of resource (e.g. test-pod-1)
        namespace: Namespace (optional)
        
    Returns:
        str: Command output
    """
    cmd = ["kubectl", "delete", resource_type, resource_name]
    
    if namespace:
        cmd.extend(["-n", namespace])
    
    # Execute command
    try:
        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        return f"Error: {e.stderr}"
    except Exception as e:
        return f"Error executing kubectl delete: {str(e)}"

@tool
def kubectl_exec(pod_name: str, command: str, namespace: str = None) -> str:
    """
    Execute command in a pod
    
    Args:
        pod_name: Pod name (e.g. test-pod-1)
        command: Command to execute
        namespace: Namespace (optional) (e.g. default)
        
    Returns:
        str: Command output
    """
    cmd = ["kubectl", "exec", pod_name]
    
    if namespace:
        cmd.extend(["-n", namespace])
    
    cmd.extend(["--", *command.split()])
    
    # Execute command
    try:
        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        return f"Error: {e.stderr}"
    except Exception as e:
        return f"Error executing kubectl exec: {str(e)}"

@tool
def kubectl_logs(pod_name: str, namespace: str = None, container: str = None, tail: int = 100) -> str:
    """
    Get logs from a pod
    
    Args:
        pod_name: Pod name (e.g. test-pod-1)
        namespace: Namespace (optional)
        container: Container name (optional)
        tail: Number of lines to show from the end (optional)
        
    Returns:
        str: Command output
    """
    cmd = ["kubectl", "logs", pod_name]
    
    if namespace:
        cmd.extend(["-n", namespace])
    
    if container:
        cmd.extend(["-c", container])
    
    if tail:
        cmd.extend(["--tail", str(tail)])
    
    # Execute command
    try:
        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        return f"Error: {e.stderr}"
    except Exception as e:
        return f"Error executing kubectl logs: {str(e)}"

@tool
def kubectl_ls_pod_volume(pod_name: str, volume_path: str, ls_options: str = "-la", namespace: str = None) -> str:
    """
    Execute 'ls' command on a pod's volume path
    
    Args:
        pod_name: Pod name (e.g. test-pod-1)
        volume_path: Path to the volume in the pod (e.g. /data)
        ls_options: Options for ls command (e.g. -la, -lh) (optional)
        namespace: Namespace (optional) (e.g. default)
        
    Returns:
        str: Command output
    """
    # Safely construct the ls command with proper escaping
    ls_command = f"ls {ls_options} {shlex.quote(volume_path)}"
    
    # Use the existing kubectl_exec function to run the command
    return kubectl_exec.invoke({"pod_name": pod_name, "command": ls_command, "namespace": namespace})
</file>

<file path="information_collector/tool_executors.py">
"""
Tool Executors

Contains methods for executing different categories of diagnostic tools.
"""

import yaml
import string
import logging
from typing import Dict, List, Any
from .base import InformationCollectorBase

# Import LangGraph tools
from tools import (
    kubectl_get, kubectl_describe, kubectl_logs,
    kubectl_get_drive, kubectl_get_csibmnode, kubectl_get_availablecapacity,
    kubectl_get_logicalvolumegroup, kubectl_get_storageclass,
    df_command, lsblk_command, dmesg_command, journalctl_command
)


class ToolExecutors(InformationCollectorBase):
    """Tool execution methods for different diagnostic categories"""
    
    async def _execute_pod_discovery_tools(self, target_pod: str, target_namespace: str):
        """Execute pod discovery tools"""
        logging.info(f"Executing pod discovery tools for {target_namespace}/{target_pod}")
        
        # Get pod information
        pod_output = self._execute_tool_with_validation(
            kubectl_get, {
                'resource_type': 'pod',
                'resource_name': target_pod,
                'namespace': target_namespace,
                'output_format': 'yaml'
            },
            'kubectl_get_pod', 'Get target pod details'
        )
        self.collected_data['kubernetes']['target_pod'] = pod_output
        
        # Describe pod
        pod_describe = self._execute_tool_with_validation(
            kubectl_describe, {
                'resource_type': 'pod',
                'resource_name': target_pod,
                'namespace': target_namespace
            },
            'kubectl_describe_pod', 'Get detailed pod configuration and events'
        )
        self.collected_data['kubernetes']['target_pod_describe'] = pod_describe
        
        # Get pod logs
        pod_logs = self._execute_tool_with_validation(
            kubectl_logs, {
                'pod_name': target_pod,
                'namespace': target_namespace,
                'tail': 100
            },
            'kubectl_logs', 'Collect pod logs for error analysis'
        )
        self.collected_data['logs']['target_pod_logs'] = pod_logs
    
    async def _execute_volume_chain_tools(self, volume_chain: Dict[str, List[str]], target_volume_path: str = 'default'):
        """Execute volume chain discovery tools"""
        logging.info("Executing volume chain discovery tools")

        # Initialize describe data container if not exists
        if 'describe' not in self.collected_data:
            self.collected_data['describe'] = {}

        if volume_chain.get('pods', []):
            pod_namespace, pod_name = volume_chain.get('pods', [])[0].split('/', 1)
            # Get pod information with kubectl get
            pods_output = self._execute_tool_with_validation(
                kubectl_get, {
                    'resource_type': 'pod',
                    'resource_name': pod_name,
                    'namespace': pod_namespace,
                    'output_format': 'yaml'
                },
                'kubectl_get_pods', 'Get pod information'
            )
            self.collected_data['kubernetes']['pods'] = pods_output
            
            # Get detailed pod information with kubectl describe
            pod_describe = self._execute_tool_with_validation(
                kubectl_describe, {
                    'resource_type': 'pod',
                    'resource_name': pod_name,
                    'namespace': pod_namespace
                },
                'kubectl_describe_pod', 'Get detailed pod configuration and events'
            )
            self.collected_data['describe']['pods'] = pod_describe

        # Get all PVCs
        if volume_chain.get('pvcs', []):
            pvc_namespace, pvc_name = volume_chain.get('pvcs', [])[0].split('/', 1)
            # Get PVC information with kubectl get
            pvcs_output = self._execute_tool_with_validation(
                kubectl_get, {
                    'resource_type': 'pvc',
                    'resource_name': pvc_name,
                    'namespace': pvc_namespace,
                    'output_format': 'yaml'
                },
                'kubectl_get_pvcs', 'Get PVC information'
            )
            self.collected_data['kubernetes']['pvcs'] = pvcs_output
            
            # Get detailed PVC information with kubectl describe
            pvc_describe = self._execute_tool_with_validation(
                kubectl_describe, {
                    'resource_type': 'pvc',
                    'resource_name': pvc_name,
                    'namespace': pvc_namespace
                },
                'kubectl_describe_pvc', 'Get detailed PVC configuration and events'
            )
            self.collected_data['describe']['pvcs'] = pvc_describe
        
        # Get all PVs
        if volume_chain.get('pvs', []):
            pv_name = volume_chain.get('pvs', [])[0]
            # Get PV information with kubectl get
            pvs_output = self._execute_tool_with_validation(
                kubectl_get, {
                    'resource_type': 'pv',
                    'resource_name': pv_name,
                    'namespace': target_volume_path,
                    'output_format': 'yaml'
                },
                'kubectl_get_pvs', 'Get PV information'
            )
            self.collected_data['kubernetes']['pvs'] = pvs_output
            
            # Get detailed PV information with kubectl describe
            pv_describe = self._execute_tool_with_validation(
                kubectl_describe, {
                    'resource_type': 'pv',
                    'resource_name': pv_name
                },
                'kubectl_describe_pv', 'Get detailed PV configuration and events'
            )
            self.collected_data['describe']['pvs'] = pv_describe
        
        # Get volume information
        if volume_chain.get('volumes', []):
            volume_name = volume_chain.get('volumes', [])[0]
            # Get volume information with kubectl get
            vol_output = self._execute_tool_with_validation(
                kubectl_get, {
                    'resource_type': 'volume',
                    'resource_name': volume_name,
                    'namespace': target_volume_path,
                    'output_format': 'yaml'
                },
                'kubectl_get_volume', 'Get volume information'
            )
            self.collected_data['kubernetes']['volumes'] = vol_output
            
            # Get detailed volume information with kubectl describe
            vol_describe = self._execute_tool_with_validation(
                kubectl_describe, {
                    'resource_type': 'volume',
                    'resource_name': volume_name,
                    'namespace': target_volume_path
                },
                'kubectl_describe_volume', 'Get detailed volume configuration and events'
            )
            self.collected_data['describe']['volumes'] = vol_describe

        # Get LVG information
        if volume_chain.get('lvg', []):
            lvg_name = volume_chain.get('lvg', [])[0]
            # Get LVG information with kubectl get
            lvg_output = self._execute_tool_with_validation(
                kubectl_get, {
                    'resource_type': 'lvg',
                    'resource_name': lvg_name,
                    'namespace': target_volume_path,
                    'output_format': 'yaml'
                },
                'kubectl_get_lvg', 'Get LVG information'
            )
            self.collected_data['kubernetes']['lvg'] = lvg_output
            
            # Get detailed LVG information with kubectl describe
            lvg_describe = self._execute_tool_with_validation(
                kubectl_describe, {
                    'resource_type': 'lvg',
                    'resource_name': lvg_name,
                    'namespace': target_volume_path
                },
                'kubectl_describe_lvg', 'Get detailed LVG configuration and events'
            )
            self.collected_data['describe']['lvg'] = lvg_describe

        # Get drive information
        if volume_chain.get('drives', []):
            drive_name = volume_chain.get('drives', [])[0]
            # Get drive information with kubectl get
            drv_output = self._execute_tool_with_validation(
                kubectl_get, {
                    'resource_type': 'drive',
                    'resource_name': drive_name,
                    'output_format': 'yaml'
                },
                'kubectl_get_drive', 'Get drive information'
            )
            self.collected_data['kubernetes']['drives'] = drv_output
            
            # Get detailed drive information with kubectl describe
            drv_describe = self._execute_tool_with_validation(
                kubectl_describe, {
                    'resource_type': 'drive',
                    'resource_name': drive_name
                },
                'kubectl_describe_drive', 'Get detailed drive configuration and events'
            )
            self.collected_data['describe']['drives'] = drv_describe

        # Get node information
        if volume_chain.get('nodes', []):
            node_name = volume_chain.get('nodes', [])[0]
            # Get node information with kubectl get
            node_output = self._execute_tool_with_validation(
                kubectl_get, {
                    'resource_type': 'node',
                    'resource_name': node_name,
                    'output_format': 'yaml'
                },
                'kubectl_get_node', 'Get node information'
            )
            self.collected_data['kubernetes']['nodes'] = node_output
            
            # Get detailed node information with kubectl describe
            node_describe = self._execute_tool_with_validation(
                kubectl_describe, {
                    'resource_type': 'node',
                    'resource_name': node_name
                },
                'kubectl_describe_node', 'Get detailed node configuration and events'
            )
            self.collected_data['describe']['nodes'] = node_describe

        # Get StorageClass information
        if volume_chain.get('StorageClass', []):
            sc_name = volume_chain.get('StorageClass', [])[0]
            # Get StorageClass information with kubectl get
            sc_output = self._execute_tool_with_validation(
                kubectl_get, {
                    'resource_type': 'StorageClass',
                    'resource_name': sc_name,
                    'output_format': 'yaml'
                },
                'kubectl_get_StorageClass', 'Get StorageClass information'
            )
            self.collected_data['kubernetes']['storage_classes'] = sc_output
            
            # Get detailed StorageClass information with kubectl describe
            sc_describe = self._execute_tool_with_validation(
                kubectl_describe, {
                    'resource_type': 'StorageClass',
                    'resource_name': sc_name
                },
                'kubectl_describe_StorageClass', 'Get detailed StorageClass configuration'
            )
            self.collected_data['describe']['storage_classes'] = sc_describe
    
    async def _execute_csi_baremetal_tools(self, drives: List[str]):
        """Execute CSI Baremetal discovery tools"""
        logging.info("Executing CSI Baremetal discovery tools")
        
        # Get drives
        drives_output = self._execute_tool_with_validation(
            kubectl_get_drive, {
                'output_format': 'yaml'
            },
            'kubectl_get_drive', 'Get CSI Baremetal drive status and health'
        )
        self.collected_data['csi_baremetal']['drives'] = drives_output
        
        # Get CSI Baremetal nodes
        csibm_nodes_output = self._execute_tool_with_validation(
            kubectl_get_csibmnode, {
                'output_format': 'yaml'
            },
            'kubectl_get_csibmnode', 'Get CSI Baremetal node mapping'
        )
        self.collected_data['csi_baremetal']['nodes'] = csibm_nodes_output
        
        # Get available capacity
        ac_output = self._execute_tool_with_validation(
            kubectl_get_availablecapacity, {
                'output_format': 'yaml'
            },
            'kubectl_get_availablecapacity', 'Get available capacity information'
        )
        self.collected_data['csi_baremetal']['available_capacity'] = ac_output
        
        # Get logical volume groups
        lvg_output = self._execute_tool_with_validation(
            kubectl_get_logicalvolumegroup, {
                'output_format': 'yaml'
            },
            'kubectl_get_logicalvolumegroup', 'Get LVG health and drive associations'
        )
        self.collected_data['csi_baremetal']['lvgs'] = lvg_output
        
        # Get CSI Baremetal volumes
        volumes_output = self._execute_tool_with_validation(
            kubectl_get, {
                'resource_type': 'volume',
                'output_format': 'yaml'
            },
            'kubectl_get_volumes', 'Get CSI Baremetal volume information with location mapping'
        )
        self.collected_data['csi_baremetal']['volumes'] = volumes_output
    
    async def _execute_node_system_tools(self, nodes: List[str]):
        """Execute node and system discovery tools"""
        logging.info("Executing node and system discovery tools")
        
        # Get nodes
        nodes_output = self._execute_tool_with_validation(
            kubectl_get, {
                'resource_type': 'node',
                'output_format': 'yaml'
            },
            'kubectl_get_nodes', 'Get node status and health'
        )
        self.collected_data['kubernetes']['nodes'] = nodes_output
        
        # Execute system commands on each node in the list
        for node_name in nodes:
            logging.info(f"Executing system tools on node: {node_name}")
            node_key = node_name.replace('.', '_').replace('-', '_')
            
            # Initialize node-specific data structure if not exists
            if 'system' not in self.collected_data:
                self.collected_data['system'] = {}
            if node_key not in self.collected_data['system']:
                self.collected_data['system'][node_key] = {}
            
            # Get disk usage
            df_output = self._execute_tool_with_validation(
                df_command, {
                    'node_name': node_name,
                    'options': '-h'
                },
                f'df_command_{node_key}', f'Check disk space usage on {node_name}'
            )
            self.collected_data['system'][node_key]['disk_usage'] = df_output
            
            # Get block devices
            lsblk_output = self._execute_tool_with_validation(
                lsblk_command, {
                    'node_name': node_name,
                    'options': ''
                },
                f'lsblk_command_{node_key}', f'List block devices and mount points on {node_name}'
            )
            self.collected_data['system'][node_key]['block_devices'] = lsblk_output
            
            # Enhanced kernel logs with comprehensive storage keywords
            storage_keywords = "disk|drive|nvme|ssd|hdd|scsi|sata|xfs|ext4|mount|error|fail|i/o|io|sector|slot|bay|controller|csi|volume"
            dmesg_output = self._execute_tool_with_validation(
                dmesg_command, {
                    'node_name': node_name,
                    'options': f'| grep -iE "({storage_keywords})" | tail -50'
                },
                f'dmesg_command_{node_key}', f'Check kernel logs for storage-related issues on {node_name}'
            )
            self.collected_data['system'][node_key]['kernel_logs'] = dmesg_output
            
            # Get systemd journal logs for storage services
            journal_storage_output = self._execute_tool_with_validation(
                journalctl_command, {
                    'node_name': node_name,
                    'options': f'-n 100 --no-pager | grep -iE "({storage_keywords})"'
                },
                f'journalctl_storage_{node_key}', f'Collect storage-related journal logs on {node_name}'
            )
            self.collected_data['system'][node_key]['journal_storage_logs'] = journal_storage_output
            
            # Get kubelet service logs for volume issues
            journal_kubelet_output = self._execute_tool_with_validation(
                journalctl_command, {
                    'node_name': node_name,
                    'options': '-u kubelet -n 50 --no-pager'
                },
                f'journalctl_kubelet_{node_key}', f'Collect kubelet logs on {node_name}'
            )
            self.collected_data['system'][node_key]['journal_kubelet_logs'] = journal_kubelet_output
            
            # Get recent boot logs for hardware detection issues
            journal_boot_output = self._execute_tool_with_validation(
                journalctl_command, {
                    'node_name': node_name,
                    'options': f'-b --no-pager | grep -iE "({storage_keywords})" | tail -30'
                },
                f'journalctl_boot_{node_key}', f'Collect boot-time logs on {node_name}'
            )
            self.collected_data['system'][node_key]['journal_boot_logs'] = journal_boot_output
    
    async def _execute_smart_data_tools(self, drives: List[str]):
        """Execute SMART data collection tools for drive health monitoring"""
        logging.info("Executing SMART data collection tools")
        
        # Get SMART data for all drives
        for drive_uuid in drives:
            # Get drive path and node info from CSI Baremetal drive info
            drive_info = self._get_drive_info_from_uuid(drive_uuid)
            if drive_info and drive_info.get('path'):
                node_name = drive_info.get('node')
                drive_path = drive_info.get('path')
                
                smart_output = self._execute_tool_with_validation(
                    self._execute_smartctl_command, {
                        'device_path': drive_path,
                        'options': '-a',
                        'node_name': node_name
                    },
                    f'smartctl_{drive_uuid}', f'Collect SMART data for drive {drive_uuid} on node {node_name}'
                )
                if 'smart_data' not in self.collected_data:
                    self.collected_data['smart_data'] = {}
                self.collected_data['smart_data'][drive_uuid] = smart_output
    
    def _get_drive_info_from_uuid(self, drive_uuid: str) -> Dict[str, str]:
        """Extract drive information from CSI Baremetal drive information
        
        Args:
            drive_uuid: Drive UUID to look up
            
        Returns:
            Dict with drive info including path and node
        """
        drive_info = {'path': None, 'node': None, 'serial': None}
        drives_output = self.collected_data.get('csi_baremetal', {}).get('drives', '')
        
        if drives_output and drive_uuid in drives_output:
            try:
                # Parse the YAML output
                drives_data = yaml.safe_load(drives_output)
                
                # Find the drive with matching UUID
                target_drive = None
                if isinstance(drives_data, dict) and 'items' in drives_data and isinstance(drives_data['items'], list):
                    # List of drives case
                    for drive in drives_data['items']:
                        if drive.get('metadata', {}).get('name') == drive_uuid:
                            target_drive = drive
                            break
                elif isinstance(drives_data, list):
                    # Direct list of drives
                    for drive in drives_data:
                        if drive.get('metadata', {}).get('name') == drive_uuid:
                            target_drive = drive
                            break
                
                if target_drive:
                    # Extract drive information from the spec
                    spec = target_drive.get('spec', {})
                    drive_info['path'] = spec.get('path')
                    drive_info['node'] = spec.get('node')
                    drive_info['serial'] = spec.get('serialNumber')
                    return drive_info
                
            except Exception as e:
                logging.warning(f"Error parsing drive YAML with yaml package: {e}")
                # Fallback to the old method in case of parsing errors
                lines = drives_output.split('\n')
                in_drive_section = False
                
                for line in lines:
                    if f'name: {drive_uuid}' in line:
                        in_drive_section = True
                    elif in_drive_section:
                        # Extract path information
                        if 'path:' in line:
                            drive_info['path'] = line.split('path:')[-1].strip()
                        # Extract node information
                        elif 'node:' in line:
                            drive_info['node'] = line.split('node:')[-1].strip()
                        # Extract serial information
                        elif 'serial:' in line:
                            drive_info['serial'] = line.split('serial:')[-1].strip()
                        # Break if we've moved to another drive section
                        elif line.strip() and 'name:' in line and drive_uuid not in line:
                            break
        
        return drive_info
    
    def _get_drive_path_from_uuid(self, drive_uuid: str) -> str:
        """Extract drive path from CSI Baremetal drive information (legacy support)"""
        drive_info = self._get_drive_info_from_uuid(drive_uuid)
        return drive_info.get('path')
    
    def _execute_smartctl_command(self, device_path: str, options: str = '-a', node_name: str = None) -> str:
        """Execute smartctl command to get SMART data
        
        Args:
            device_path: Path to the device
            options: Command options
            node_name: Node hostname or IP (if None, runs locally)
            
        Returns:
            str: Command output
        """
        try:
            cmd_str = f"smartctl {options} {device_path}"
            
            # If node_name is provided, use SSH to execute on the remote node
            if node_name:
                from tools.diagnostics.hardware import ssh_execute
                return ssh_execute.invoke({"node_name": node_name, "command": cmd_str})
            else:
                # Fall back to local execution
                import subprocess
                cmd = ['smartctl', options, device_path]
                result = subprocess.run(cmd, check=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
                return result.stdout
        except Exception as e:
            return f"Error executing smartctl: {str(e)}"
    
    async def _execute_enhanced_log_analysis_tools(self, nodes: List[str]):
        """Execute enhanced log analysis tools for comprehensive storage issue detection"""
        logging.info("Executing enhanced log analysis tools")
        
        # Enhanced dmesg analysis with more specific patterns
        enhanced_dmesg_patterns = [
            "nvme.*error",
            "ssd.*fail",
            "disk.*timeout",
            "scsi.*error",
            "ata.*error",
            "bad.*sector",
            "i/o.*error",
            "filesystem.*error",
            "mount.*fail",
            "csi.*error"
        ]
        
        # Execute enhanced log analysis on each node
        for node_name in nodes:
            logging.info(f"Executing enhanced log analysis on node: {node_name}")
            node_key = node_name.replace('.', '_').replace('-', '_')
            
            # Initialize node-specific data structure if not exists
            if 'enhanced_logs' not in self.collected_data:
                self.collected_data['enhanced_logs'] = {}
            if node_key not in self.collected_data['enhanced_logs']:
                self.collected_data['enhanced_logs'][node_key] = {}
            
            # Process each dmesg pattern
            for pattern in enhanced_dmesg_patterns:
                pattern_key = pattern.replace(".*", "_")
                dmesg_output = self._execute_tool_with_validation(
                    dmesg_command, {
                        'node_name': node_name,
                        'options': f'| grep -iE "{pattern}" | tail -20'
                    },
                    f'dmesg_{pattern_key}_{node_key}', f'Check kernel logs for {pattern} issues on {node_name}'
                )
                self.collected_data['enhanced_logs'][node_key][f'dmesg_{pattern_key}'] = dmesg_output
            
            # Enhanced journal analysis for CSI and storage services
            csi_services = ['csi-baremetal-node', 'csi-baremetal-controller', 'kubelet']
            
            # Initialize service_logs if not exists
            if 'service_logs' not in self.collected_data:
                self.collected_data['service_logs'] = {}
            if node_key not in self.collected_data['service_logs']:
                self.collected_data['service_logs'][node_key] = {}
            
            for service in csi_services:
                journal_output = self._execute_tool_with_validation(
                    journalctl_command, {
                        'node_name': node_name,
                        'options': f'-u {service} -n 50 --no-pager'
                    },
                    f'journalctl_{service}_{node_key}', f'Collect {service} service logs on {node_name}'
                )
                self.collected_data['service_logs'][node_key][service] = journal_output
</file>

<file path="tools/core/mcp_adapter.py">
#!/usr/bin/env python3
"""
MCP Adapter for Kubernetes Volume I/O Error Troubleshooting

This module provides integration with MCP (Multi-Cloud Platform) tools
using the langchain_mcp_adapters package. It handles initialization
of MCP servers and routing of tool calls based on configuration.
"""

from typing import Dict, List, Any, Optional, Union
import logging
import asyncio
from langchain_mcp_adapters.client import MultiServerMCPClient

logger = logging.getLogger(__name__)

# Global MCP adapter instance
_mcp_adapter = None

class MCPAdapter:
    """
    MCP Adapter for integrating MCP tools into the troubleshooting system
    
    Handles initialization of MCP servers and routing of tool calls
    based on configuration.
    """
    
    def __init__(self, config_data: Dict[str, Any]):
        """
        Initialize MCP Adapter with configuration
        
        Args:
            config_data: Configuration data from config.yaml
        """
        self.config_data = config_data
        self.mcp_enabled = config_data.get('mcp_enabled', False)
        self.mcp_servers = config_data.get('mcp_servers', {})
        self.mcp_clients = {}
        self.mcp_tools = {}
        self.mcp_tools_by_phase = {
            'plan_phase': [],
            'phase1': [],
            'phase2': []
        }
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
    
    async def initialize_servers(self):
        """
        Initialize MCP servers based on configuration
        """
        if not self.mcp_enabled:
            self.logger.info("MCP integration is disabled")
            return
        
        for server_name, server_config in self.mcp_servers.items():
            # Check if server is enabled (default to True if not specified)
            if not server_config.get('enable', True):
                self.logger.info(f"Skipping disabled MCP server: {server_name}")
                continue
                
            try:
                # Extract server configuration
                server_type = server_config.get('type', 'sse')
                server_url = server_config.get('url', '')
                command = server_config.get('command', None)
                args = server_config.get('args', [])
                env = server_config.get('env', {})
                
                # Validate configuration
                if server_type == 'sse' and not server_url:
                    self.logger.error(f"Missing URL for SSE server: {server_name}")
                    continue
                
                if server_type == 'stdio' and not command:
                    self.logger.error(f"Missing command for stdio server: {server_name}")
                    continue
                
                # Configure server based on type
                server_config_dict = {}
                if server_type == 'sse':
                    server_config_dict = {
                        "url": server_url,
                        "transport": "sse"
                    }
                else:  # stdio
                    server_config_dict = {
                        "command": command,
                        "args": args,
                        "env": env,
                        "transport": "stdio"
                    }
                
                # Create MCP client for this server
                self.logger.info(f"Initializing MCP server: {server_name} ({server_type})")
                server_config_client = {server_name: server_config_dict}
                self.mcp_clients[server_name] = MultiServerMCPClient(server_config_client)
                
                # Get tools from this server 
                tools = await self.mcp_clients[server_name].get_tools()
                if tools:
                    self.mcp_tools[server_name] = tools
                    self.logger.info(f"Loaded {len(tools)} tools from MCP server: {server_name}")
                    
                    # Organize tools by phase
                    phase_config = server_config.get('tools', {})
                    if phase_config.get('plan_phase', False):
                        self.mcp_tools_by_phase['plan_phase'].extend(tools)
                    if phase_config.get('phase1', False):
                        self.mcp_tools_by_phase['phase1'].extend(tools)
                    if phase_config.get('phase2', False):
                        self.mcp_tools_by_phase['phase2'].extend(tools)
                else:
                    self.logger.warning(f"No tools loaded from MCP server: {server_name}")
                    
            except Exception as e:
                self.logger.error(f"Failed to initialize MCP server {server_name}: {str(e)}")
    
    def get_tools_for_phase(self, phase: str) -> List[Any]:
        """
        Get MCP tools for a specific phase
        
        Args:
            phase: Phase name ('plan_phase', 'phase1', or 'phase2')
            
        Returns:
            List[Any]: List of MCP tools for the specified phase
        """
        if not self.mcp_enabled:
            return []
            
        return self.mcp_tools_by_phase.get(phase, [])
    
    def get_all_tools(self) -> List[Any]:
        """
        Get all MCP tools from all servers
        
        Returns:
            List[Any]: List of all MCP tools
        """
        if not self.mcp_enabled:
            return []
            
        all_tools = []
        for server_name, tools in self.mcp_tools.items():
            all_tools.extend(tools)
        
        return all_tools
    
    async def call_tool(self, tool_name: str, **kwargs) -> Any:
        """
        Call an MCP tool by name
        
        Args:
            tool_name: Name of the tool to call
            **kwargs: Arguments to pass to the tool
            
        Returns:
            Any: Result of the tool call
        """
        if not self.mcp_enabled:
            raise ValueError("MCP integration is disabled")
            
        # Find the server that has this tool
        for server_name, tools in self.mcp_tools.items():
            for tool in tools:
                if tool.name == tool_name:
                    self.logger.info(f"Calling MCP tool: {tool_name} on server: {server_name}")
                    return await tool.invoke(kwargs)
                    
        raise ValueError(f"MCP tool not found: {tool_name}")
    
    async def close(self):
        """
        Close all MCP clients
        """
        if not self.mcp_enabled:
            return
            
        for server_name, client in self.mcp_clients.items():
            try:
                #await client.aclose()
                self.logger.info(f"Closed MCP client: {server_name}")
            except Exception as e:
                self.logger.error(f"Error closing MCP client {server_name}: {e}")


async def initialize_mcp_adapter(config_data: Dict[str, Any]) -> MCPAdapter:
    """
    Initialize the global MCP adapter
    
    Args:
        config_data: Configuration data from config.yaml
        
    Returns:
        MCPAdapter: Initialized MCP adapter
    """
    global _mcp_adapter
    
    if _mcp_adapter is None:
        _mcp_adapter = MCPAdapter(config_data)
        
        # Initialize MCP servers asynchronously
        await _mcp_adapter.initialize_servers()
    
    return _mcp_adapter


def get_mcp_adapter() -> Optional[MCPAdapter]:
    """
    Get the global MCP adapter instance
    
    Returns:
        Optional[MCPAdapter]: MCP adapter instance or None if not initialized
    """
    global _mcp_adapter
    return _mcp_adapter
</file>

<file path="tools/testing/volume_testing_basic.py">
#!/usr/bin/env python3
"""
Basic volume testing tools for validating volume functionality.

This module provides tools for running I/O tests, validating mounts,
and testing volume permissions during troubleshooting.
"""

import json
import time
import re
from typing import Dict, Any
from langchain_core.tools import tool
from tools.core.config import validate_command, execute_command

@tool
def run_volume_io_test(pod_name: str, namespace: str = "default", 
                      mount_path: str = "/test-volume", 
                      test_size: str = "10M") -> str:
    """
    Run I/O tests on a volume mounted in a pod
    
    Args:
        pod_name: Name of the pod with mounted volume
        namespace: Kubernetes namespace
        mount_path: Path where volume is mounted, must have a valid path
        test_size: Size of test file (e.g., 10M, 100M)
        
    Returns:
        str: Results of I/O tests
    """
    results = []
    
    try:
        # Test 1: Write test
        write_cmd = f"dd if=/dev/zero of={mount_path}/AI_test_write.dat bs=1M count=10 2>&1"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", write_cmd]
        write_result = execute_command(cmd)
        results.append(f"Write Test:\n{write_result}")
        
        # Test 2: Read test
        read_cmd = f"dd if={mount_path}/AI_test_write.dat of=/dev/null bs=1M 2>&1"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", read_cmd]
        read_result = execute_command(cmd)
        results.append(f"Read Test:\n{read_result}")
        
        # Test 3: Random I/O test using dd
        random_cmd = f"dd if=/dev/urandom of={mount_path}/AI_test_random.dat bs=4k count=100 2>&1"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", random_cmd]
        random_result = execute_command(cmd)
        results.append(f"Random Write Test:\n{random_result}")
        
        # Test 4: File operations test
        file_ops_cmd = f"""
        echo 'Testing file operations...' > {mount_path}/AI_test_file.txt &&
        cat {mount_path}/AI_test_file.txt &&
        ls -la {mount_path}/ &&
        df -h {mount_path}
        """
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", file_ops_cmd]
        file_ops_result = execute_command(cmd)
        results.append(f"File Operations Test:\n{file_ops_result}")
        
        # Test 5: Cleanup test files
        cleanup_cmd = f"rm -f {mount_path}/AI_test_*.dat {mount_path}/AI_test_file.txt"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", cleanup_cmd]
        cleanup_result = execute_command(cmd)
        results.append(f"Cleanup:\n{cleanup_result}")
        
        return "\n" + "="*50 + "\n".join(results)
        
    except Exception as e:
        return f"Error running volume I/O tests: {str(e)}"

@tool
def validate_volume_mount(pod_name: str, namespace: str = "default", 
                         mount_path: str = "/test-volume") -> str:
    """
    Validate that a volume is properly mounted in a pod
    
    Args:
        pod_name: Name of the pod
        namespace: Kubernetes namespace
        mount_path: Expected mount path, must be specified
        
    Returns:
        str: Volume mount validation results
    """
    results = []
    
    try:
        # Check if mount path exists
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "ls", "-la", mount_path]
        ls_result = execute_command(cmd)
        results.append(f"Mount Path Check ({mount_path}):\n{ls_result}")
        
        # Check mount information
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "mount"]
        mount_result = execute_command(cmd)
        results.append(f"Mount Information:\n{mount_result}")
        
        # Check disk space
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "df", "-h", mount_path]
        df_result = execute_command(cmd)
        results.append(f"Disk Space:\n{df_result}")
        
        # Check filesystem type
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "stat", "-f", mount_path]
        stat_result = execute_command(cmd)
        results.append(f"Filesystem Info:\n{stat_result}")
        
        # Check if volume is writable
        test_write_cmd = f"touch {mount_path}/write_test && rm {mount_path}/write_test && echo 'Volume is writable'"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", test_write_cmd]
        write_test_result = execute_command(cmd)
        results.append(f"Write Test:\n{write_test_result}")
        
        return "\n" + "="*50 + "\n".join(results)
        
    except Exception as e:
        return f"Error validating volume mount: {str(e)}"

@tool
def test_volume_permissions(pod_name: str, namespace: str = "default", 
                           mount_path: str = "/test-volume", 
                           test_user: str = None) -> str:
    """
    Test volume permissions and access rights
    
    Args:
        pod_name: Name of the pod
        namespace: Kubernetes namespace
        mount_path: Volume mount path, must be specified
        test_user: User to test permissions for (optional)
        
    Returns:
        str: Permission test results
    """
    results = []
    
    try:
        # Check current permissions
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "ls", "-la", mount_path]
        perm_result = execute_command(cmd)
        results.append(f"Current Permissions:\n{perm_result}")
        
        # Check who can access the mount
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "whoami"]
        user_result = execute_command(cmd)
        results.append(f"Current User:\n{user_result}")
        
        # Test read permissions
        read_test_cmd = f"ls {mount_path} && echo 'Read access: OK'"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", read_test_cmd]
        read_test_result = execute_command(cmd)
        results.append(f"Read Permission Test:\n{read_test_result}")
        
        # Test write permissions
        write_test_cmd = f"touch {mount_path}/perm_test_file && echo 'Write access: OK'"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", write_test_cmd]
        write_test_result = execute_command(cmd)
        results.append(f"Write Permission Test:\n{write_test_result}")
        
        # Test execute permissions (if applicable)
        exec_test_cmd = f"cd {mount_path} && echo 'Execute access: OK'"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", exec_test_cmd]
        exec_test_result = execute_command(cmd)
        results.append(f"Execute Permission Test:\n{exec_test_result}")
        
        # Check file ownership
        try:
            if execute_command(["kubectl", "exec", pod_name, "-n", namespace, "--", "ls", f"{mount_path}/perm_test_file"]):
                owner_cmd = f"ls -la {mount_path}/perm_test_file"
                cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", owner_cmd]
                owner_result = execute_command(cmd)
                results.append(f"File Ownership:\n{owner_result}")
                
                # Cleanup test file
                cleanup_cmd = f"rm -f {mount_path}/perm_test_file"
                cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", cleanup_cmd]
                execute_command(cmd)
        except Exception as e:
            results.append(f"Error checking file ownership: {str(e)}")
        
        return "\n" + "="*50 + "\n".join(results)
        
    except Exception as e:
        return f"Error testing volume permissions: {str(e)}"

@tool
def verify_volume_mount(pod_name: str, namespace: str = "default",
                       mount_path: str = "/test-volume") -> str:
    """
    Verify that a pod volume is correctly mounted and accessible
    
    Args:
        pod_name: Name of the pod
        namespace: Kubernetes namespace
        mount_path: Expected mount path, must be set by client
        
    Returns:
        str: Volume mount verification results
    """
    results = []
    
    try:
        # Check if mount path exists
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "ls", "-la", mount_path]
        ls_result = execute_command(cmd)
        results.append(f"Mount Path Existence Check ({mount_path}):\n{ls_result}")
        
        # Get mount details with grep
        mount_grep_cmd = f"mount | grep '{mount_path}'"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", mount_grep_cmd]
        mount_grep_result = execute_command(cmd)
        
        if mount_grep_result:
            results.append(f"Mount Entry Found:\n{mount_grep_result}")
        else:
            results.append(f"Warning: No mount entry found for {mount_path}")
        
        # Check mount options
        if mount_grep_result:
            # Extract mount options
            options_match = re.search(r'\((.*?)\)', mount_grep_result)
            if options_match:
                options = options_match.group(1)
                results.append(f"Mount Options: {options}")
                
                # Check for read-only mount
                if "ro" in options.split(","):
                    results.append("Warning: Volume is mounted read-only")
        
        # Check filesystem type
        fs_type_cmd = f"df -T {mount_path} | tail -n 1 | awk '{{print $2}}'"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", fs_type_cmd]
        fs_type_result = execute_command(cmd)
        results.append(f"Filesystem Type: {fs_type_result}")
        
        # Check available space
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "df", "-h", mount_path]
        df_result = execute_command(cmd)
        results.append(f"Space Information:\n{df_result}")
        
        # Check if volume is writable
        write_test_cmd = f"touch {mount_path}/mount_verify_test && echo 'Write test successful' && rm {mount_path}/mount_verify_test"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", write_test_cmd]
        write_test_result = execute_command(cmd)
        
        if "Write test successful" in write_test_result:
            results.append("Write Test: Passed - Volume is writable")
        else:
            results.append(f"Write Test: Failed - Volume may not be writable\n{write_test_result}")
        
        # Check inode usage
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "df", "-i", mount_path]
        inode_result = execute_command(cmd)
        results.append(f"Inode Usage:\n{inode_result}")
        
        return "\n" + "="*50 + "\n".join(results)
        
    except Exception as e:
        return f"Error verifying volume mount: {str(e)}"
</file>

<file path="tools/testing/volume_testing_performance.py">
#!/usr/bin/env python3
"""
Performance-related volume testing tools.

This module provides tools for performance testing, stress testing,
and latency monitoring of pod volumes.
"""

import time
from datetime import datetime
from typing import Dict, Any
from langchain_core.tools import tool
from tools.core.config import validate_command, execute_command

@tool
def run_volume_stress_test(pod_name: str, namespace: str = "default", 
                          mount_path: str = "/test-volume", 
                          duration: int = 60) -> str:
    """
    Run a stress test on the volume to check for I/O errors under load
    
    Args:
        pod_name: Name of the pod
        namespace: Kubernetes namespace
        mount_path: Volume mount path, must be specified
        duration: Test duration in seconds
        
    Returns:
        str: Stress test results
    """
    results = []
    
    try:
        # Check available space first
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "df", "-h", mount_path]
        space_result = execute_command(cmd)
        results.append(f"Available Space:\n{space_result}")
        
        # Run concurrent I/O operations
        stress_cmd = f"""
        echo 'Starting stress test for {duration} seconds...' &&
        for i in $(seq 1 5); do
            (dd if=/dev/zero of={mount_path}/stress_$i.dat bs=1M count=50 2>/dev/null; 
             dd if={mount_path}/stress_$i.dat of=/dev/null bs=1M 2>/dev/null;
             rm {mount_path}/stress_$i.dat) &
        done &&
        sleep {duration} &&
        wait &&
        echo 'Stress test completed'
        """
        
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", stress_cmd]
        stress_result = execute_command(cmd)
        results.append(f"Stress Test Results:\n{stress_result}")
        
        # Check for any errors in pod logs during the test
        cmd = ["kubectl", "logs", pod_name, "-n", namespace, "--tail=50"]
        log_result = execute_command(cmd)
        results.append(f"Pod Logs (last 50 lines):\n{log_result}")
        
        # Final space check
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "df", "-h", mount_path]
        final_space_result = execute_command(cmd)
        results.append(f"Final Space Check:\n{final_space_result}")
        
        return "\n" + "="*50 + "\n".join(results)
        
    except Exception as e:
        return f"Error running volume stress test: {str(e)}"

@tool
def test_volume_io_performance(pod_name: str, namespace: str = "default",
                              mount_path: str = "/test-volume",
                              test_size: str = "100M",
                              test_duration: int = 30) -> str:
    """
    Test I/O performance of a pod volume including read/write speeds and latency
    
    Args:
        pod_name: Name of the pod
        namespace: Kubernetes namespace
        mount_path: Volume mount path, must be specified
        test_size: Size of test data (e.g., 100M)
        test_duration: Test duration in seconds
        
    Returns:
        str: I/O performance test results with metrics
    """
    results = []
    
    try:
        # Check if test directory exists and is writable
        test_dir = f"{mount_path}/io_perf_test"
        setup_cmd = f"mkdir -p {test_dir} || echo 'Failed to create test directory'"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", setup_cmd]
        setup_result = execute_command(cmd)
        
        if "Failed" in setup_result:
            return f"Error: Unable to create test directory {test_dir}. Check permissions or if volume is mounted read-only."
        
        results.append(f"I/O Performance Test on {mount_path}")
        results.append(f"Test parameters: size={test_size}, duration={test_duration}s")
        
        # Check if fio is available (preferred tool)
        check_fio_cmd = "which fio || echo 'fio not found'"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", check_fio_cmd]
        fio_check_result = execute_command(cmd)
        
        if "not found" not in fio_check_result:
            # Use fio for testing
            results.append("Using fio for performance testing (more comprehensive results)")
            
            # Sequential read test
            seq_read_cmd = f"fio --name=seq_read --directory={test_dir} --rw=read --bs=4M --size={test_size} --numjobs=1 --time_based --runtime={test_duration} --group_reporting"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", seq_read_cmd]
            seq_read_result = execute_command(cmd)
            results.append(f"Sequential Read Test Results:\n{seq_read_result}")
            
            # Sequential write test
            seq_write_cmd = f"fio --name=seq_write --directory={test_dir} --rw=write --bs=4M --size={test_size} --numjobs=1 --time_based --runtime={test_duration} --group_reporting"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", seq_write_cmd]
            seq_write_result = execute_command(cmd)
            results.append(f"Sequential Write Test Results:\n{seq_write_result}")
            
            # Random read test
            rand_read_cmd = f"fio --name=rand_read --directory={test_dir} --rw=randread --bs=4k --size={test_size} --numjobs=4 --time_based --runtime={test_duration} --group_reporting"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", rand_read_cmd]
            rand_read_result = execute_command(cmd)
            results.append(f"Random Read Test Results:\n{rand_read_result}")
            
            # Random write test
            rand_write_cmd = f"fio --name=rand_write --directory={test_dir} --rw=randwrite --bs=4k --size={test_size} --numjobs=4 --time_based --runtime={test_duration} --group_reporting"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", rand_write_cmd]
            rand_write_result = execute_command(cmd)
            results.append(f"Random Write Test Results:\n{rand_write_result}")
        else:
            # Fallback to dd for basic testing
            results.append("Using dd for basic performance testing (fio not available)")
            
            # Sequential write test
            seq_write_cmd = f"dd if=/dev/zero of={test_dir}/write_test bs=1M count={test_size.replace('M', '')} oflag=direct 2>&1"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", seq_write_cmd]
            seq_write_result = execute_command(cmd)
            results.append(f"Sequential Write Test (dd):\n{seq_write_result}")
            
            # Sequential read test
            seq_read_cmd = f"dd if={test_dir}/write_test of=/dev/null bs=1M iflag=direct 2>&1"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", seq_read_cmd]
            seq_read_result = execute_command(cmd)
            results.append(f"Sequential Read Test (dd):\n{seq_read_result}")
            
            # Random I/O simulation with small blocks
            rand_io_cmd = f"dd if=/dev/urandom of={test_dir}/random_test bs=4k count=1000 oflag=direct 2>&1"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", rand_io_cmd]
            rand_io_result = execute_command(cmd)
            results.append(f"Random I/O Test (dd):\n{rand_io_result}")
        
        # Clean up test files
        cleanup_cmd = f"rm -rf {test_dir}"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", cleanup_cmd]
        execute_command(cmd)
        
        return "\n" + "="*50 + "\n".join(results)
        
    except Exception as e:
        # Attempt cleanup even if test fails
        try:
            cleanup_cmd = f"rm -rf {test_dir}"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", cleanup_cmd]
            execute_command(cmd)
        except:
            pass
            
        return f"Error testing volume I/O performance: {str(e)}"

@tool
def monitor_volume_latency(pod_name: str, namespace: str = "default",
                          mount_path: str = "/test-volume",
                          duration: int = 60,
                          operation_type: str = "all") -> str:
    """
    Monitor real-time latency of volume operations within a pod
    
    Args:
        pod_name: Name of the pod
        namespace: Kubernetes namespace
        mount_path: Volume mount path, must be specified
        duration: Monitoring duration in seconds
        operation_type: Operation type to monitor (read, write, all)
        
    Returns:
        str: Volume latency monitoring results
    """
    results = []
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    try:
        # Add timestamp to log
        results.append(f"[{timestamp}] Volume Latency Monitoring - Pod: {pod_name}, Path: {mount_path}")
        results.append(f"Monitoring duration: {duration} seconds")
        
        # Validate operation type
        valid_types = ["read", "write", "all"]
        if operation_type not in valid_types:
            return f"Error: Invalid operation type '{operation_type}'. Must be one of: {', '.join(valid_types)}"
        
        # Create monitoring directory
        test_dir = f"{mount_path}/latency_monitor"
        setup_cmd = f"mkdir -p {test_dir} || echo 'Failed to create test directory'"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", setup_cmd]
        setup_result = execute_command(cmd)
        
        if "Failed" in setup_result:
            return f"Error: Unable to create test directory {test_dir}. Check permissions or if volume is mounted read-only."
        
        # Check if required tools are available
        check_tools_cmd = "which time || which timeout || echo 'Missing required tools'"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", check_tools_cmd]
        tools_check_result = execute_command(cmd)
        
        if "Missing required tools" in tools_check_result:
            results.append("Warning: 'time' or 'timeout' command not found. Using fallback method.")
            
            # For Kubernetes pods, we need to handle the script differently
            # Let's create the script directly inside the pod
            import os
            
            # Read the content of the script
            script_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 
                                      "scripts", "volume_latency_monitor.sh")
            
            try:
                with open(script_path, 'r') as f:
                    script_content = f.read()
            except Exception as e:
                results.append(f"Error reading script file: {str(e)}")
                
                # Fallback to embedded script content if we can't read the file
                script_content = """#!/bin/bash
# volume_latency_monitor.sh - Script for monitoring volume operation latency

# Parse command line arguments
test_dir=$1
duration=$2
operation_type=$3

echo "Starting latency monitoring for $duration seconds..."
echo "Timestamp,Operation,Size,Duration(s)" > "$test_dir/results.csv"

start_time=$(date +%s)
end_time=$((start_time + duration))
current_time=$start_time
iteration=0

while [ $current_time -lt $end_time ]; do
  # Write test
  if [ "$operation_type" = "write" ] || [ "$operation_type" = "all" ]; then
    write_start=$(date +%s.%N)
    dd if=/dev/zero of="$test_dir/write_test_$iteration" bs=1M count=1 2>/dev/null
    write_end=$(date +%s.%N)
    write_duration=$(echo "$write_end - $write_start" | bc)
    echo "$(date +%Y-%m-%d_%H:%M:%S),write,1M,$write_duration" >> "$test_dir/results.csv"
  fi
  
  # Read test
  if [ "$operation_type" = "read" ] || [ "$operation_type" = "all" ]; then
    if [ -f "$test_dir/write_test_$iteration" ]; then
      read_start=$(date +%s.%N)
      dd if="$test_dir/write_test_$iteration" of=/dev/null bs=1M 2>/dev/null
      read_end=$(date +%s.%N)
      read_duration=$(echo "$read_end - $read_start" | bc)
      echo "$(date +%Y-%m-%d_%H:%M:%S),read,1M,$read_duration" >> "$test_dir/results.csv"
    fi
  fi
  
  # Random small writes
  if [ "$operation_type" = "write" ] || [ "$operation_type" = "all" ]; then
    write_small_start=$(date +%s.%N)
    dd if=/dev/urandom of="$test_dir/small_test_$iteration" bs=4k count=10 2>/dev/null
    write_small_end=$(date +%s.%N)
    write_small_duration=$(echo "$write_small_end - $write_small_start" | bc)
    echo "$(date +%Y-%m-%d_%H:%M:%S),write_small,40K,$write_small_duration" >> "$test_dir/results.csv"
  fi
  
  # Metadata operation test (touch file, list dir)
  meta_start=$(date +%s.%N)
  touch "$test_dir/meta_test_$iteration" && ls -la "$test_dir/" >/dev/null
  meta_end=$(date +%s.%N)
  meta_duration=$(echo "$meta_end - $meta_start" | bc)
  echo "$(date +%Y-%m-%d_%H:%M:%S),metadata,0,$meta_duration" >> "$test_dir/results.csv"
  
  # Sleep a bit to avoid overwhelming the system
  sleep 2
  current_time=$(date +%s)
  iteration=$((iteration + 1))
done

# Analyze results
echo "Latency Monitoring Completed"
echo "Summary:"

if [ "$operation_type" = "write" ] || [ "$operation_type" = "all" ]; then
  echo "Write operations (1MB):"
  awk -F, '$2=="write" {sum+=$4; count++} END {print "  Average latency: " (count > 0 ? sum/count : "N/A") " seconds over " count " operations"}' "$test_dir/results.csv"
fi

if [ "$operation_type" = "read" ] || [ "$operation_type" = "all" ]; then
  echo "Read operations (1MB):"
  awk -F, '$2=="read" {sum+=$4; count++} END {print "  Average latency: " (count > 0 ? sum/count : "N/A") " seconds over " count " operations"}' "$test_dir/results.csv"
fi

if [ "$operation_type" = "write" ] || [ "$operation_type" = "all" ]; then
  echo "Small write operations (40KB):"
  awk -F, '$2=="write_small" {sum+=$4; count++} END {print "  Average latency: " (count > 0 ? sum/count : "N/A") " seconds over " count " operations"}' "$test_dir/results.csv"
fi

echo "Metadata operations:"
awk -F, '$2=="metadata" {sum+=$4; count++} END {print "  Average latency: " (count > 0 ? sum/count : "N/A") " seconds over " count " operations"}' "$test_dir/results.csv"
"""
            
            # Create the script inside the pod
            pod_script_path = f"{test_dir}/volume_latency_monitor.sh"
            create_script_cmd = f"cat > {pod_script_path} << 'EOF'\n{script_content}\nEOF\nchmod +x {pod_script_path}"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", create_script_cmd]
            try:
                execute_command(cmd)
                results.append(f"Created latency monitoring script in pod")
            except Exception as e:
                results.append(f"Warning: Failed to create script in pod: {str(e)}")
                return "\n" + "="*50 + "\n".join(results)
            
            # Now run the script
            latency_script = f"{pod_script_path} {test_dir} {duration} {operation_type}"
            
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", latency_script]
            latency_result = execute_command(cmd)
            results.append(f"Latency Monitoring Results:\n{latency_result}")
            
            # Get the CSV results
            csv_cmd = f"cat {test_dir}/results.csv"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", csv_cmd]
            csv_result = execute_command(cmd)
            
            # Find slowest operations
            slowest_cmd = f"tail -n +2 {test_dir}/results.csv | sort -t, -k4 -nr | head -5"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", slowest_cmd]
            slowest_result = execute_command(cmd)
            
            if slowest_result and len(slowest_result.strip()) > 0:
                results.append(f"Slowest Operations:\n{slowest_result}")
                
                # Analyze for spikes - using string concatenation to avoid f-string issues
                analyze_cmd = "awk -F, '" + """
                BEGIN {max=0; min=999999; sum=0; count=0}
                NR>1 {
                  if($4 > max) max=$4;
                  if($4 < min) min=$4;
                  sum+=$4;
                  count++;
                }
                END {
                  avg=sum/count;
                  printf "Max: %.6f s, Min: %.6f s, Avg: %.6f s\\n", max, min, avg;
                  print "Detecting latency spikes (>2x average)";
                }""" + "' " + test_dir + "/results.csv"
                cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", analyze_cmd]
                analyze_result = execute_command(cmd)
                results.append(f"Statistical Analysis:\n{analyze_result}")
        else:
            # Advanced method with proper timing tools
            # First check if 'perf' or 'blktrace' are available for more detailed analysis
            check_advanced_cmd = "which perf || which blktrace || echo 'No advanced tools'"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", check_advanced_cmd]
            advanced_check_result = execute_command(cmd)
            
            if "No advanced tools" not in advanced_check_result:
                # Use advanced tools if available
                if "perf" in advanced_check_result:
                    # Using perf for IO latency monitoring
                    perf_cmd = f"""
                    cd {test_dir} &&
                    echo "Using perf for advanced IO latency monitoring" &&
                    perf record -e block:block_rq_issue -e block:block_rq_complete -a -o perf.data -- timeout {duration}s sh -c '
                      for i in $(seq 1 10); do
                        dd if=/dev/zero of=./perf_write_$i bs=1M count=10 oflag=direct 2>/dev/null;
                        dd if=./perf_write_$i of=/dev/null bs=1M iflag=direct 2>/dev/null;
                        rm ./perf_write_$i;
                      done
                    ' &&
                    perf report -i perf.data
                    """
                    cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", perf_cmd]
                    perf_result = execute_command(cmd)
                    results.append(f"Advanced IO Latency Monitoring (perf):\n{perf_result}")
                elif "blktrace" in advanced_check_result:
                    # Using blktrace for IO latency monitoring
                    blktrace_cmd = f"""
                    cd {test_dir} &&
                    echo "Using blktrace for advanced IO latency monitoring" &&
                    blktrace -d $(mount | grep {mount_path} | awk '{{print $1}}') -o trace &
                    BLKTRACE_PID=$! &&
                    for i in $(seq 1 5); do
                      dd if=/dev/zero of=./blk_write_$i bs=1M count=10 oflag=direct 2>/dev/null;
                      dd if=./blk_write_$i of=/dev/null bs=1M iflag=direct 2>/dev/null;
                      rm ./blk_write_$i;
                    done &&
                    sleep 2 &&
                    kill $BLKTRACE_PID &&
                    blkparse trace
                    """
                    cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", blktrace_cmd]
                    blktrace_result = execute_command(cmd)
                    results.append(f"Advanced IO Latency Monitoring (blktrace):\n{blktrace_result}")
            else:
                # Basic time-based measurements
                time_cmd = f"""
                cd {test_dir} &&
                echo "Operation,Size,Duration(s)" > time_results.csv &&
                
                # Sequential write test
                echo "Measuring sequential write latency..." &&
                TIMEFORMAT="%R" &&
                write_time=$({{ time dd if=/dev/zero of=./seq_write bs=1M count=100 conv=fdatasync 2>/dev/null; }} 2>&1) &&
                echo "write,100MB,$write_time" >> time_results.csv &&
                
                # Sequential read test
                echo "Measuring sequential read latency..." &&
                read_time=$({{ time dd if=./seq_write of=/dev/null bs=1M 2>/dev/null; }} 2>&1) &&
                echo "read,100MB,$read_time" >> time_results.csv &&
                
                # Random write test (smaller blocks)
                echo "Measuring random write latency..." &&
                rand_write_time=$({{ time dd if=/dev/urandom of=./rand_write bs=4k count=1000 conv=fdatasync 2>/dev/null; }} 2>&1) &&
                echo "rand_write,4MB,$rand_write_time" >> time_results.csv &&
                
                # Random read test
                echo "Measuring random read latency..." &&
                rand_read_cmd="for i in \$(seq 1 100); do dd if=./rand_write of=/dev/null bs=4k count=10 skip=\$((RANDOM % 100)) 2>/dev/null; done"
                rand_read_time=$({{ time sh -c "$rand_read_cmd"; }} 2>&1) &&
                echo "rand_read,4MB,$rand_read_time" >> time_results.csv &&
                
                # Metadata operations
                echo "Measuring metadata operations latency..." &&
                meta_cmd="for i in \$(seq 1 100); do touch ./meta_\$i; done && ls -la ./ >/dev/null && for i in \$(seq 1 100); do rm ./meta_\$i; done"
                meta_time=$({{ time sh -c "$meta_cmd"; }} 2>&1) &&
                echo "metadata,N/A,$meta_time" >> time_results.csv &&
                
                # Display results
                echo "Latency Measurements Complete" &&
                cat time_results.csv
                """
                cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", time_cmd]
                time_result = execute_command(cmd)
                results.append(f"Basic Latency Measurements:\n{time_result}")
        
        cleanup_cmd = f"rm -rf {test_dir}"
        cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", cleanup_cmd]
        execute_command(cmd)
        
        return "\n" + "="*50 + "\n".join(results)
        
    except Exception as e:
        # Attempt cleanup even if test fails
        try:
            cleanup_cmd = f"rm -rf {test_dir}"
            cmd = ["kubectl", "exec", pod_name, "-n", namespace, "--", "sh", "-c", cleanup_cmd]
            execute_command(cmd)
        except:
            pass
        
        return f"Error monitoring volume latency: {str(e)}"
</file>

<file path="troubleshooting/execute_tool_node.py">
"""
Refactored Execute Tool Node for Kubernetes Volume I/O Error Troubleshooting

This module contains the ExecuteToolNode class which handles tool execution in the
LangGraph-based troubleshooting system. It uses the Strategy pattern for tool execution
and delegates hook management to a dedicated HookManager.
"""

import asyncio
import inspect
from copy import copy
from dataclasses import replace
from typing import (
    Any,
    Callable,
    Dict,
    List,
    Literal,
    Optional,
    Sequence,
    Set,
    Tuple,
    Union,
    cast,
)
import logging

from langchain_core.messages import (
    AIMessage,
    AnyMessage,
    ToolCall,
    ToolMessage,
)
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import BaseTool
from langchain_core.tools import tool as create_tool
from pydantic import BaseModel

from langgraph.errors import GraphBubbleUp
from langgraph.store.base import BaseStore
from langgraph.types import Command, Send
from langgraph.utils.runnable import RunnableCallable
from langgraph.prebuilt.tool_node import (
    _handle_tool_error,
    _infer_handled_types,
    _get_state_args,
    _get_store_arg,
    INVALID_TOOL_NAME_ERROR_TEMPLATE,
)

# Import our new strategy and hook manager classes
from troubleshooting.strategies import (
    ExecutionType, 
    ToolExecutionStrategy,
    StrategyFactory
)
from troubleshooting.hook_manager import HookManager

# Configure logging
logger = logging.getLogger('execute_tool_node')
logger.setLevel(logging.INFO)

class ExecuteToolNode(RunnableCallable):
    """A node that runs tools based on their configuration (parallel or serial).
    
    Uses the Strategy pattern to separate execution logic for parallel and serial tools.
    Uses a HookManager to handle before/after tool execution hooks.
    
    It can be used either in StateGraph with a "messages" state key (or a custom key 
    passed via ExecuteToolNode's 'messages_key'). The output will be a list of 
    ToolMessages, one for each tool call, in the same order as the tools were called.

    Tool calls can also be passed directly as a list of `ToolCall` dicts.

    Args:
        tools: A sequence of tools that can be invoked by the ExecuteToolNode.
        parallel_tools: A set of tool names that should be executed in parallel.
        serial_tools: A set of tool names that should be executed serially.
        name: The name of the ExecuteToolNode in the graph. Defaults to "execute_tools".
        max_workers: Maximum number of worker threads to use for parallel execution.
            Defaults to None (uses ThreadPoolExecutor default).
        tags: Optional tags to associate with the node. Defaults to None.
        handle_tool_errors: How to handle tool errors raised by tools inside the node. Defaults to True.
            Must be one of the following:
            - True: all errors will be caught and
                a ToolMessage with a default error message (TOOL_CALL_ERROR_TEMPLATE) will be returned.
            - str: all errors will be caught and
                a ToolMessage with the string value of 'handle_tool_errors' will be returned.
            - tuple[type[Exception], ...]: exceptions in the tuple will be caught and
                a ToolMessage with a default error message (TOOL_CALL_ERROR_TEMPLATE) will be returned.
            - Callable[..., str]: exceptions from the signature of the callable will be caught and
                a ToolMessage with the string value of the result of the 'handle_tool_errors' callable will be returned.
            - False: none of the errors raised by the tools will be caught
        messages_key: The state key in the input that contains the list of messages.
            The same key will be used for the output from the ExecuteToolNode.
            Defaults to "messages".
    """

    name: str = "ExecuteToolNode"

    def __init__(
        self,
        tools: Sequence[Union[BaseTool, Callable]],
        parallel_tools: Set[str],
        serial_tools: Set[str],
        *,
        name: str = "execute_tools",
        max_workers: Optional[int] = None,
        tags: Optional[list[str]] = None,
        handle_tool_errors: Union[
            bool, str, Callable[..., str], tuple[type[Exception], ...]
        ] = True,
        messages_key: str = "messages",
    ) -> None:
        super().__init__(self._func, self._afunc, name=name, tags=tags, trace=False)
        # Tool management
        self.tools_by_name: dict[str, BaseTool] = {}
        self.tool_to_state_args: dict[str, dict[str, Optional[str]]] = {}
        self.tool_to_store_arg: dict[str, Optional[str]] = {}
        
        # Configuration
        self.handle_tool_errors = handle_tool_errors
        self.messages_key = messages_key
        self.parallel_tools = parallel_tools
        self.serial_tools = serial_tools
        self.max_workers = max_workers
        
        # Initialize hook manager
        self.hook_manager = HookManager()
        
        # Create execution strategies
        self.parallel_strategy = StrategyFactory.create_strategy(ExecutionType.PARALLEL, max_workers)
        self.serial_strategy = StrategyFactory.create_strategy(ExecutionType.SERIAL)
        
        # Process tools
        for tool_ in tools:
            if not isinstance(tool_, BaseTool):
                tool_ = create_tool(tool_)
            self.tools_by_name[tool_.name] = tool_
            self.tool_to_state_args[tool_.name] = _get_state_args(tool_)
            self.tool_to_store_arg[tool_.name] = _get_store_arg(tool_)
            
    def register_before_call_hook(self, hook: Callable) -> None:
        """Register a hook function to be called before tool execution.
        
        Args:
            hook: A callable that takes tool name and arguments as parameters
        """
        self.hook_manager.register_before_call_hook(hook)
        
    def register_after_call_hook(self, hook: Callable) -> None:
        """Register a hook function to be called after tool execution.
        
        Args:
            hook: A callable that takes tool name, arguments, and result as parameters
        """
        self.hook_manager.register_after_call_hook(hook)

    def _func(
        self,
        input: Union[
            list[AnyMessage],
            dict[str, Any],
            BaseModel,
        ],
        config: RunnableConfig,
        *,
        store: Optional[BaseStore],
    ) -> Any:
        tool_calls, input_type = self._parse_input(input, store)
        
        if not tool_calls:
            # If no tools to execute, return empty list
            return {"messages": []} if input_type == "dict" else []
        
        # Filter tool calls into parallel and serial groups
        parallel_tool_calls = self._filter_parallel_tools(tool_calls)
        serial_tool_calls = self._filter_serial_tools(tool_calls)
        
        outputs = []
        
        # First, process parallel tools concurrently if any exist
        if parallel_tool_calls:
            parallel_outputs = self.parallel_strategy.execute(
                parallel_tool_calls, 
                input_type, 
                config,
                self._run_one
            )
            outputs.extend(parallel_outputs)
        
        # Then, process serial tools sequentially if any exist
        if serial_tool_calls:
            serial_outputs = self.serial_strategy.execute(
                serial_tool_calls, 
                input_type, 
                config,
                self._run_one
            )
            outputs.extend(serial_outputs)

        return self._combine_tool_outputs(outputs, input_type)

    async def _afunc(
        self,
        input: Union[
            list[AnyMessage],
            dict[str, Any],
            BaseModel,
        ],
        config: RunnableConfig,
        *,
        store: Optional[BaseStore],
    ) -> Any:
        tool_calls, input_type = self._parse_input(input, store)
        
        if not tool_calls:
            # If no tools to execute, return empty list
            return {"messages": []} if input_type == "dict" else []
        
        # Filter tool calls into parallel and serial groups
        parallel_tool_calls = self._filter_parallel_tools(tool_calls)
        serial_tool_calls = self._filter_serial_tools(tool_calls)
        
        outputs = []
        
        # First, process parallel tools concurrently if any exist
        if parallel_tool_calls:
            parallel_outputs = await self.parallel_strategy.execute_async(
                parallel_tool_calls, 
                input_type, 
                config,
                self._arun_one
            )
            outputs.extend(parallel_outputs)
        
        # Then, process serial tools sequentially if any exist
        if serial_tool_calls:
            serial_outputs = await self.serial_strategy.execute_async(
                serial_tool_calls, 
                input_type, 
                config,
                self._arun_one
            )
            outputs.extend(serial_outputs)

        return self._combine_tool_outputs(outputs, input_type)

    def _filter_parallel_tools(self, tool_calls: List[ToolCall]) -> List[ToolCall]:
        """Filter tool calls to only include those configured for parallel execution.
        
        Args:
            tool_calls: List of tool calls to filter
            
        Returns:
            List of tool calls for parallel execution
        """
        return [call for call in tool_calls if call["name"] in self.parallel_tools]
    
    def _filter_serial_tools(self, tool_calls: List[ToolCall]) -> List[ToolCall]:
        """Filter tool calls to only include those configured for serial execution.
        Also includes tools not explicitly categorized as parallel or serial (defaults to serial).
        
        Args:
            tool_calls: List of tool calls to filter
            
        Returns:
            List of tool calls for serial execution
        """
        return [call for call in tool_calls if call["name"] in self.serial_tools or 
                (call["name"] not in self.parallel_tools and call["name"] not in self.serial_tools)]

    def _combine_tool_outputs(
        self,
        outputs: list[ToolMessage],
        input_type: Literal["list", "dict", "tool_calls"],
    ) -> list[Union[Command, list[ToolMessage], dict[str, list[ToolMessage]]]]:
        """Combine tool outputs into the expected format based on input type.
        
        Args:
            outputs: List of tool messages from execution
            input_type: Type of input that generated these outputs
            
        Returns:
            Tool outputs in the appropriate format
        """
        # preserve existing behavior for non-command tool outputs for backwards
        # compatibility
        if not any(isinstance(output, Command) for output in outputs):
            # TypedDict, pydantic, dataclass, etc. should all be able to load from dict
            return outputs if input_type == "list" else {self.messages_key: outputs}

        # LangGraph will automatically handle list of Command and non-command node
        # updates
        combined_outputs: list[
            Command | list[ToolMessage] | dict[str, list[ToolMessage]]
        ] = []

        # combine all parent commands with goto into a single parent command
        parent_command: Optional[Command] = None
        for output in outputs:
            if isinstance(output, Command):
                if (
                    output.graph is Command.PARENT
                    and isinstance(output.goto, list)
                    and all(isinstance(send, Send) for send in output.goto)
                ):
                    if parent_command:
                        parent_command = replace(
                            parent_command,
                            goto=cast(list[Send], parent_command.goto) + output.goto,
                        )
                    else:
                        parent_command = Command(graph=Command.PARENT, goto=output.goto)
                else:
                    combined_outputs.append(output)
            else:
                combined_outputs.append(
                    [output] if input_type == "list" else {self.messages_key: [output]}
                )

        if parent_command:
            combined_outputs.append(parent_command)
        return combined_outputs

    def _is_async_only_tool(self, tool_name: str) -> bool:
        """Check if a tool only supports async invocation.
        
        Args:
            tool_name: Name of the tool to check
            
        Returns:
            True if the tool only supports async invocation, False otherwise
        """
        tool = self.tools_by_name[tool_name]
        
        # Check #1: Class-based detection for MCP tools and known async-only tool classes
        tool_class_str = str(tool.__class__).lower()
        if "mcp" in tool_class_str or "structuredtool" in tool_class_str:
            logger.info(f"Tool {tool_name} detected as async-only based on class: {tool.__class__}")
            return True
            
        # Check #2: Module-based detection
        if hasattr(tool, "__module__") and ("mcp" in tool.__module__.lower() or "langchain_mcp" in tool.__module__.lower()):
            logger.info(f"Tool {tool_name} detected as async-only based on module: {tool.__module__}")
            return True
        
        # Check #3: _run vs _arun methods presence
        if (not hasattr(tool, "_run") or not callable(getattr(tool, "_run", None))) and hasattr(tool, "_arun") and callable(getattr(tool, "_arun")):
            logger.info(f"Tool {tool_name} detected as async-only (has _arun but no _run method)")
            return True
            
        # Check #4: Try to see if it has custom methods or attributes that indicate async-only
        async_only_indicators = [
            hasattr(tool, "async_invoke_only") and tool.async_invoke_only is True,
            hasattr(tool, "sync_invoke_supported") and tool.sync_invoke_supported is False
        ]
        if any(async_only_indicators):
            logger.info(f"Tool {tool_name} has explicit async-only indicators")
            return True
            
        return False

    def _run_one(
        self,
        call: ToolCall,
        input_type: Literal["list", "dict", "tool_calls"],
        config: RunnableConfig,
        call_type: str = "Serial",
    ) -> ToolMessage:
        """Execute a single tool.
        
        Args:
            call: Tool call to execute
            input_type: Type of input (list, dict, or tool_calls)
            config: Runnable configuration
            call_type: Type of call execution ("Parallel" or "Serial")
            
        Returns:
            Result of tool execution as a ToolMessage
        """
        if invalid_tool_message := self._validate_tool_call(call):
            return invalid_tool_message

        # Extract tool name and arguments for hooks
        tool_name = call["name"]
        tool_args = call["args"] if "args" in call else {}
        
        # Call before hook
        self.hook_manager.run_before_hook(tool_name, tool_args, call_type)

        # Get the tool
        tool = self.tools_by_name[tool_name]
        input_data = {**call, **{"type": "tool_call"}}
        
        # First attempt: Try to determine if it's an async-only tool
        use_async = self._is_async_only_tool(tool_name)

        try:
            # If we already know it's async-only, use the async method directly
            if use_async:
                logger.info(f"Tool {tool_name} identified as async-only, using async execution")
                # Create a new event loop for this thread if needed
                try:
                    loop = asyncio.get_event_loop()
                except RuntimeError:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                
                # Run the async method in the event loop
                response = loop.run_until_complete(tool.ainvoke(input_data, config))
            else:
                # Try normal sync invocation first
                try:
                    response = tool.invoke(input_data, config)
                except NotImplementedError as e:
                    # If we get a NotImplementedError with the specific message about StructuredTool,
                    # fall back to async invocation
                    if "StructuredTool does not support sync invocation" in str(e):
                        logger.info(f"Tool {tool_name} raised NotImplementedError for sync invocation, falling back to async")
                        # Create a new event loop for this thread if needed
                        try:
                            loop = asyncio.get_event_loop()
                        except RuntimeError:
                            loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(loop)
                        
                        # Run the async method in the event loop
                        response = loop.run_until_complete(tool.ainvoke(input_data, config))
                    else:
                        # If it's a different NotImplementedError, re-raise it
                        raise

            # Call after hook
            self.hook_manager.run_after_hook(tool_name, tool_args, response, call_type)
            return response

        except GraphBubbleUp as e:
            # Special exception that will always be raised
            raise e
        except Exception as e:
            # Special handling for structured tools that only support async invocation
            if isinstance(e, NotImplementedError) and "StructuredTool does not support sync invocation" in str(e):
                logger.warning(f"Tool {tool_name} only supports async invocation but wasn't detected. Retrying with async")
                try:
                    # Create a new event loop for this thread if needed
                    try:
                        loop = asyncio.get_event_loop()
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                    
                    # Run the async method in the event loop
                    response = loop.run_until_complete(tool.ainvoke(input_data, config))
                    
                    # Call after hook
                    self.hook_manager.run_after_hook(tool_name, tool_args, response, call_type)
                    return response
                except Exception as async_e:
                    # If async execution also fails, handle that error instead
                    e = async_e
            
            # Standard error handling
            if isinstance(self.handle_tool_errors, tuple):
                handled_types: tuple = self.handle_tool_errors
            elif callable(self.handle_tool_errors):
                handled_types = _infer_handled_types(self.handle_tool_errors)
            else:
                # default behavior is catching all exceptions
                handled_types = (Exception,)

            # Unhandled
            if not self.handle_tool_errors or not isinstance(e, handled_types):
                raise e
            # Handled
            else:
                content = _handle_tool_error(e, flag=self.handle_tool_errors)
            
            error_message = ToolMessage(
                content=content,
                name=call["name"],
                tool_call_id=call["id"],
                status="error",
            )
            
            # Call after hook with error result
            self.hook_manager.run_after_hook(tool_name, tool_args, error_message, call_type)
            return error_message

    async def _arun_one(
        self,
        call: ToolCall,
        input_type: Literal["list", "dict", "tool_calls"],
        config: RunnableConfig,
        call_type: str = "Serial",
    ) -> ToolMessage:
        """Execute a single tool asynchronously.
        
        Args:
            call: Tool call to execute
            input_type: Type of input (list, dict, or tool_calls)
            config: Runnable configuration
            call_type: Type of call execution ("Parallel" or "Serial")
            
        Returns:
            Result of tool execution as a ToolMessage
        """
        if invalid_tool_message := self._validate_tool_call(call):
            return invalid_tool_message

        # Extract tool name and arguments for hooks
        tool_name = call["name"]
        tool_args = call["args"] if "args" in call else {}
        
        # Call before hook
        self.hook_manager.run_before_hook(tool_name, tool_args, call_type)

        try:
            input = {**call, **{"type": "tool_call"}}
            response = await self.tools_by_name[tool_name].ainvoke(input, config)

            # Call after hook
            self.hook_manager.run_after_hook(tool_name, tool_args, response, call_type)
            return response

        except GraphBubbleUp as e:
            # Special exception that will always be raised
            raise e
        except Exception as e:
            # Handle errors based on configuration
            if isinstance(self.handle_tool_errors, tuple):
                handled_types: tuple = self.handle_tool_errors
            elif callable(self.handle_tool_errors):
                handled_types = _infer_handled_types(self.handle_tool_errors)
            else:
                # default behavior is catching all exceptions
                handled_types = (Exception,)

            # Unhandled
            if not self.handle_tool_errors or not isinstance(e, handled_types):
                raise e
            # Handled
            else:
                content = _handle_tool_error(e, flag=self.handle_tool_errors)

            error_message = ToolMessage(
                content=content,
                name=call["name"],
                tool_call_id=call["id"],
                status="error",
            )
            
            # Call after hook with error result
            self.hook_manager.run_after_hook(tool_name, tool_args, error_message, call_type)
            return error_message

    def _parse_input(
        self,
        input: Union[
            list[AnyMessage],
            dict[str, Any],
            BaseModel,
        ],
        store: Optional[BaseStore],
    ) -> Tuple[list[ToolCall], Literal["list", "dict", "tool_calls"]]:
        """Parse input to extract tool calls.
        
        Args:
            input: Input to the node
            store: Optional store for state management
            
        Returns:
            Tuple of tool calls and input type
        """
        if isinstance(input, list):
            if isinstance(input[-1], dict) and input[-1].get("type") == "tool_call":
                input_type = "tool_calls"
                tool_calls = input
                return tool_calls, input_type
            else:
                input_type = "list"
                message: AnyMessage = input[-1]
        elif isinstance(input, dict) and (messages := input.get(self.messages_key, [])):
            input_type = "dict"
            message = messages[-1]
        elif messages := getattr(input, self.messages_key, None):
            # Assume dataclass-like state that can coerce from dict
            input_type = "dict"
            message = messages[-1]
        elif tool_calls := input.get("tool_calls", []):
            # Handle case where tool_calls are passed directly
            input_type = "dict"
            return [
                self.inject_tool_args(call, input, store) for call in tool_calls
            ], input_type
        else:
            raise ValueError("No message or tool_calls found in input")

        if not isinstance(message, AIMessage):
            raise ValueError("Last message is not an AIMessage")

        tool_calls = [
            self.inject_tool_args(call, input, store) for call in message.tool_calls
        ]
        return tool_calls, input_type

    def _validate_tool_call(self, call: ToolCall) -> Optional[ToolMessage]:
        """Validate that the requested tool exists.
        
        Args:
            call: The tool call to validate
            
        Returns:
            None if valid, ToolMessage with error if invalid
        """
        if (requested_tool := call["name"]) not in self.tools_by_name:
            content = INVALID_TOOL_NAME_ERROR_TEMPLATE.format(
                requested_tool=requested_tool,
                available_tools=", ".join(self.tools_by_name.keys()),
            )
            return ToolMessage(
                content, name=requested_tool, tool_call_id=call["id"], status="error"
            )
        else:
            return None

    def _inject_state(
        self,
        tool_call: ToolCall,
        input: Union[
            list[AnyMessage],
            dict[str, Any],
            BaseModel,
        ],
    ) -> ToolCall:
        """Inject state into the tool call.
        
        Args:
            tool_call: Tool call to inject state into
            input: Input containing state
            
        Returns:
            Tool call with injected state
        """
        state_args = self.tool_to_state_args[tool_call["name"]]
        if state_args and isinstance(input, list):
            required_fields = list(state_args.values())
            if (
                len(required_fields) == 1
                and required_fields[0] == self.messages_key
                or required_fields[0] is None
            ):
                input = {self.messages_key: input}
            else:
                err_msg = (
                    f"Invalid input to ExecuteToolNode. Tool {tool_call['name']} requires "
                    f"graph state dict as input."
                )
                if any(state_field for state_field in state_args.values()):
                    required_fields_str = ", ".join(f for f in required_fields if f)
                    err_msg += f" State should contain fields {required_fields_str}."
                raise ValueError(err_msg)
        if isinstance(input, dict):
            tool_state_args = {
                tool_arg: input[state_field] if state_field else input
                for tool_arg, state_field in state_args.items()
            }

        else:
            tool_state_args = {
                tool_arg: getattr(input, state_field) if state_field else input
                for tool_arg, state_field in state_args.items()
            }

        tool_call["args"] = {
            **tool_call["args"],
            **tool_state_args,
        }
        return tool_call

    def _inject_store(
        self, tool_call: ToolCall, store: Optional[BaseStore]
    ) -> ToolCall:
        """Inject store into the tool call.
        
        Args:
            tool_call: Tool call to inject store into
            store: Store to inject
            
        Returns:
            Tool call with injected store
        """
        store_arg = self.tool_to_store_arg[tool_call["name"]]
        if not store_arg:
            return tool_call

        if store is None:
            raise ValueError(
                "Cannot inject store into tools with InjectedStore annotations - "
                "please compile your graph with a store."
            )

        tool_call["args"] = {
            **tool_call["args"],
            store_arg: store,
        }
        return tool_call

    def inject_tool_args(
        self,
        tool_call: ToolCall,
        input: Union[
            list[AnyMessage],
            dict[str, Any],
            BaseModel,
        ],
        store: Optional[BaseStore],
    ) -> ToolCall:
        """Injects the state and store into the tool call.

        Tool arguments with types annotated as `InjectedState` and `InjectedStore` are
        ignored in tool schemas for generation purposes. This method injects them into
        tool calls for tool invocation.

        Args:
            tool_call: The tool call to inject state and store into
            input: The input state to inject
            store: The store to inject

        Returns:
            The tool call with injected state and store
        """
        if tool_call["name"] not in self.tools_by_name:
            return tool_call

        tool_call_copy: ToolCall = copy(tool_call)
        tool_call_with_state = self._inject_state(tool_call_copy, input)
        tool_call_with_store = self._inject_store(tool_call_with_state, store)
        return tool_call_with_store
</file>

<file path="knowledge_graph/knowledge_graph.py">
#!/usr/bin/env python3
"""
Knowledge Graph Implementation for Kubernetes Volume Troubleshooting

This module provides NetworkX-based Knowledge Graph functionality to organize
diagnostic data, entities, and relationships for comprehensive root cause analysis
and fix plan generation in the CSI Baremetal driver troubleshooting system.
"""

import logging
import networkx as nx
from typing import Dict, List, Any, Optional, Tuple
import json

# Configure logger for knowledge graph operations
kg_logger = logging.getLogger('knowledge_graph')
kg_logger.setLevel(logging.INFO)
# Don't propagate to root logger to avoid console output
kg_logger.propagate = False


class KnowledgeGraph:
    """
    Knowledge Graph for organizing diagnostic data and relationships
    """
    
    def __init__(self):
        """Initialize the Knowledge Graph"""
        self.graph = nx.DiGraph()
        self.entities = {
            'gnodes': {
                'pods': {},
                'pvcs': {},
                'pvs': {},
                'drives': {},
                'nodes': {},
                'storage_classes': {},
                'lvgs': {},
                'acs': {},
                'volumes': {},
                'system_entities': {},
                'cluster_nodes': {},
                'historical_experiences': {}
            }
        }
        self.issues = []
        kg_logger.info("Knowledge Graph initialized")
    
    def add_gnode_pod(self, name: str, namespace: str, **attributes) -> str:
        """
        Add a Pod node to the knowledge graph
        
        Args:
            name: Pod name
            namespace: Pod namespace
            **attributes: Additional pod attributes (errors, SecurityContext, etc.)
            
        Returns:
            str: Node ID
        """
        node_id = f"gnode:Pod:{namespace}/{name}"
        self.graph.add_node(node_id, 
                           entity_type="gnode",
                           gnode_subtype="Pod",
                           name=name,
                           namespace=namespace,
                           **attributes)
        self.entities['gnodes']['pods'][node_id] = {
            'name': name,
            'namespace': namespace,
            **attributes
        }
        kg_logger.debug(f"Added Pod node: {node_id}")
        return node_id
    
    def add_gnode_pvc(self, name: str, namespace: str, **attributes) -> str:
        """
        Add a PVC node to the knowledge graph
        
        Args:
            name: PVC name
            namespace: PVC namespace
            **attributes: Additional PVC attributes (storageClass, bound PV, etc.)
            
        Returns:
            str: Node ID
        """
        node_id = f"gnode:PVC:{namespace}/{name}"
        self.graph.add_node(node_id,
                           entity_type="gnode",
                           gnode_subtype="PVC",
                           name=name,
                           namespace=namespace,
                           **attributes)
        self.entities['gnodes']['pvcs'][node_id] = {
            'name': name,
            'namespace': namespace,
            **attributes
        }
        kg_logger.debug(f"Added PVC node: {node_id}")
        return node_id
    
    def add_gnode_pv(self, name: str, **attributes) -> str:
        """
        Add a PV node to the knowledge graph
        
        Args:
            name: PV name
            **attributes: Additional PV attributes (diskPath, nodeAffinity, etc.)
            
        Returns:
            str: Node ID
        """
        node_id = f"gnode:PV:{name}"
        self.graph.add_node(node_id,
                           entity_type="gnode",
                           gnode_subtype="PV",
                           name=name,
                           **attributes)
        self.entities['gnodes']['pvs'][node_id] = {
            'name': name,
            **attributes
        }
        kg_logger.debug(f"Added PV node: {node_id}")
        return node_id
    
    def add_gnode_drive(self, uuid: str, **attributes) -> str:
        """
        Add a Drive node to the knowledge graph
        
        Args:
            uuid: Drive UUID
            **attributes: Additional drive attributes (Health, Status, Path, etc.)
            
        Returns:
            str: Node ID
        """
        node_id = f"gnode:Drive:{uuid}"
        self.graph.add_node(node_id,
                           entity_type="gnode",
                           gnode_subtype="Drive",
                           uuid=uuid,
                           **attributes)
        self.entities['gnodes']['drives'][node_id] = {
            'uuid': uuid,
            **attributes
        }
        kg_logger.debug(f"Added Drive node: {node_id}")
        return node_id
    
    def add_gnode_node(self, name: str, **attributes) -> str:
        """
        Add a Node node to the knowledge graph
        
        Args:
            name: Node name
            **attributes: Additional node attributes (Ready, DiskPressure, etc.)
            
        Returns:
            str: Node ID
        """
        node_id = f"gnode:Node:{name}"
        self.graph.add_node(node_id,
                           entity_type="gnode",
                           gnode_subtype="Node",
                           name=name,
                           **attributes)
        self.entities['gnodes']['nodes'][node_id] = {
            'name': name,
            **attributes
        }
        kg_logger.debug(f"Added Node node: {node_id}")
        return node_id
    
    def add_gnode_storage_class(self, name: str, **attributes) -> str:
        """
        Add a StorageClass node to the knowledge graph
        
        Args:
            name: StorageClass name
            **attributes: Additional storage class attributes (provisioner, etc.)
            
        Returns:
            str: Node ID
        """
        node_id = f"gnode:StorageClass:{name}"
        self.graph.add_node(node_id,
                           entity_type="gnode",
                           gnode_subtype="StorageClass",
                           name=name,
                           **attributes)
        self.entities['gnodes']['storage_classes'][node_id] = {
            'name': name,
            **attributes
        }
        kg_logger.debug(f"Added StorageClass node: {node_id}")
        return node_id
    
    def add_gnode_lvg(self, name: str, **attributes) -> str:
        """
        Add a LogicalVolumeGroup node to the knowledge graph
        
        Args:
            name: LVG name
            **attributes: Additional LVG attributes (Health, drive UUIDs, etc.)
            
        Returns:
            str: Node ID
        """
        node_id = f"gnode:LVG:{name}"
        self.graph.add_node(node_id,
                           entity_type="gnode",
                           gnode_subtype="LVG",
                           name=name,
                           **attributes)
        self.entities['gnodes']['lvgs'][node_id] = {
            'name': name,
            **attributes
        }
        kg_logger.debug(f"Added LVG node: {node_id}")
        return node_id
    
    def add_gnode_ac(self, name: str, **attributes) -> str:
        """
        Add an AvailableCapacity node to the knowledge graph
        
        Args:
            name: AC name
            **attributes: Additional AC attributes (size, storage class, location, etc.)
            
        Returns:
            str: Node ID
        """
        node_id = f"gnode:AC:{name}"
        self.graph.add_node(node_id,
                           entity_type="gnode",
                           gnode_subtype="AC",
                           name=name,
                           **attributes)
        self.entities['gnodes']['acs'][node_id] = {
            'name': name,
            **attributes
        }
        kg_logger.debug(f"Added AC node: {node_id}")
        return node_id
    
    def add_gnode_volume(self, name: str, namespace: str, **attributes) -> str:
        """
        Add a Volume node to the knowledge graph
        
        Args:
            name: Volume name
            namespace: Volume namespace
            **attributes: Additional volume attributes (Health, LocationType, size, storage class, location, Usage, etc.)
            
        Returns:
            str: Node ID
        """
        node_id = f"gnode:Volume:{namespace}/{name}"
        self.graph.add_node(node_id,
                           entity_type="gnode",
                           gnode_subtype="Volume",
                           name=name,
                           namespace=namespace,
                           **attributes)
        self.entities['gnodes']['volumes'][node_id] = {
            'name': name,
            'namespace': namespace,
            **attributes
        }
        kg_logger.debug(f"Added Volume node: {node_id}")
        return node_id
    
    def add_gnode_system_entity(self, entity_name: str, entity_subtype: str, **attributes) -> str:
        """
        Add a System entity node to the knowledge graph (for logs, kernel, services, etc.)
        
        Args:
            entity_name: System entity name (e.g., "kernel", "kubelet", "boot")
            entity_subtype: System entity subtype (e.g., "logs", "service", "hardware")
            **attributes: Additional system entity attributes
            
        Returns:
            str: Node ID
        """
        node_id = f"gnode:System:{entity_name}"
        self.graph.add_node(node_id,
                           entity_type="gnode",
                           gnode_subtype="System",
                           name=entity_name,
                           subtype=entity_subtype,
                           **attributes)
        self.entities['gnodes']['system_entities'][node_id] = {
            'name': entity_name,
            'subtype': entity_subtype,
            **attributes
        }
        kg_logger.debug(f"Added System entity node: {node_id}")
        return node_id

    def add_gnode_cluster_node(self, name: str, **attributes) -> str:
        """
        Add a ClusterNode node to the knowledge graph
        
        Args:
            name: ClusterNode name
            **attributes: Additional cluster node attributes
            
        Returns:
            str: Node ID
        """
        node_id = f"gnode:ClusterNode:{name}"
        self.graph.add_node(node_id,
                           entity_type="gnode",
                           gnode_subtype="ClusterNode",
                           name=name,
                           **attributes)
        self.entities['gnodes']['cluster_nodes'][node_id] = {
            'name': name,
            **attributes
        }
        kg_logger.debug(f"Added ClusterNode node: {node_id}")
        return node_id
        
    def add_gnode_historical_experience(self, experience_id: str, **attributes) -> str:
        """
        Add a Historical Experience node to the knowledge graph
        
        Args:
            experience_id: Unique identifier for the historical experience
            **attributes: Additional attributes including:
                - observation/phenomenon: Description of the observed issue
                - thinking: List of reasoning steps (new format)
                - investigation/localization_method: Steps or tools to diagnose the issue
                - diagnosis/root_cause: Analysis of the underlying cause
                - resolution/resolution_method: Steps or actions to resolve the issue
            
        Returns:
            str: Node ID
        """
        node_id = f"gnode:HistoricalExperience:{experience_id}"
        
        # Handle both old and new field names
        # Map new field names to old field names for backward compatibility
        field_mapping = {
            'observation': 'phenomenon',
            'diagnosis': 'root_cause',
            'investigation': 'localization_method',
            'resolution': 'resolution_method'
        }
        
        # Create a copy of attributes to avoid modifying the original
        node_attrs = attributes.copy()
        
        # For each new field name, check if it exists and map to old field name if not present
        for new_field, old_field in field_mapping.items():
            # If new field exists but old field doesn't, copy value to old field
            if new_field in node_attrs and old_field not in node_attrs:
                node_attrs[old_field] = node_attrs[new_field]
            # If old field exists but new field doesn't, copy value to new field
            elif old_field in node_attrs and new_field not in node_attrs:
                node_attrs[new_field] = node_attrs[old_field]
        
        # Add required fields with default values if missing
        required_fields = ['phenomenon', 'root_cause', 'localization_method', 'resolution_method',
                          'observation', 'diagnosis', 'investigation', 'resolution']
        
        missing_fields = [field for field in required_fields if field not in node_attrs]
        if missing_fields:
            kg_logger.debug(f"Historical experience {experience_id} is missing fields: {missing_fields}")
            
            # Set default values for missing fields
            defaults = {
                'phenomenon': node_attrs.get('observation', 'No phenomenon provided'),
                'root_cause': node_attrs.get('diagnosis', 'No root cause provided'),
                'localization_method': node_attrs.get('investigation', 'No localization method provided'),
                'resolution_method': node_attrs.get('resolution', 'No resolution method provided'),
                'observation': node_attrs.get('phenomenon', 'No observation provided'),
                'diagnosis': node_attrs.get('root_cause', 'No diagnosis provided'),
                'investigation': node_attrs.get('localization_method', 'No investigation provided'),
                'resolution': node_attrs.get('resolution_method', 'No resolution provided')
            }
            
            for field in missing_fields:
                node_attrs[field] = defaults.get(field, 'Not provided')
        
        # Add node to graph with all attributes
        node_attrs.update({
            'entity_type': 'gnode',
            'gnode_subtype': 'HistoricalExperience',
            'experience_id': experience_id
        })
        
        self.graph.add_node(node_id, **node_attrs)
        
        # Add to entities dictionary
        self.entities['gnodes']['historical_experiences'][node_id] = {
            'experience_id': experience_id,
            **node_attrs
        }
        
        kg_logger.debug(f"Added HistoricalExperience node: {node_id}")
        return node_id
    
    def add_relationship(self, source_id: str, target_id: str, relationship: str, **attributes):
        """
        Add a relationship edge between two nodes
        
        Args:
            source_id: Source node ID
            target_id: Target node ID
            relationship: Type of relationship
            **attributes: Additional edge attributes
        """
        self.graph.add_edge(source_id, target_id,
                           relationship=relationship,
                           **attributes)
        kg_logger.debug(f"Added relationship: {source_id} --{relationship}--> {target_id}")
    
    def add_issue(self, node_id: str, issue_type: str, description: str, severity: str = "medium"):
        """
        Add an issue to a node and the issues list
        
        Args:
            node_id: Node ID where the issue was found
            issue_type: Type of issue (e.g., "permission", "disk_health", "configuration")
            description: Description of the issue
            severity: Issue severity (low, medium, high, critical)
        """
        issue = {
            'node_id': node_id,
            'type': issue_type,
            'description': description,
            'severity': severity
        }
        
        # Add to issues list
        self.issues.append(issue)
        
        # Add to node attributes
        if self.graph.has_node(node_id):
            current_issues = self.graph.nodes[node_id].get('issues', [])
            current_issues.append(issue)
            self.graph.nodes[node_id]['issues'] = current_issues
        
        kg_logger.info(f"Added {severity} severity issue to {node_id}: {description}")
    
    def get_issues_by_severity(self, severity: str) -> List[Dict]:
        """
        Get all issues of a specific severity
        
        Args:
            severity: Issue severity to filter by
            
        Returns:
            List[Dict]: List of issues
        """
        return [issue for issue in self.issues if issue['severity'] == severity]
    
    def get_all_issues(self) -> List[Dict]:
        """
        Get all issues sorted by severity
        
        Returns:
            List[Dict]: List of all issues sorted by severity
        """
        severity_order = {'critical': 0, 'high': 1, 'medium': 2, 'low': 3}
        return sorted(self.issues, key=lambda x: severity_order.get(x['severity'], 4))
    
    def find_nodes_by_type(self, gnode_subtype: str) -> List[str]:
        """
        Find all nodes of a specific gnode subtype
        
        Args:
            gnode_subtype: Subtype of gnode to find
            
        Returns:
            List[str]: List of node IDs
        """
        return [node_id for node_id, attrs in self.graph.nodes(data=True) 
                if attrs.get('entity_type') == 'gnode' and attrs.get('gnode_subtype') == gnode_subtype]
    
    def find_connected_nodes(self, node_id: str, relationship: str = None) -> List[str]:
        """
        Find nodes connected to a given node
        
        Args:
            node_id: Source node ID
            relationship: Optional relationship type to filter by
            
        Returns:
            List[str]: List of connected node IDs
        """
        connected = []
        if self.graph.has_node(node_id):
            for target in self.graph.successors(node_id):
                edge_data = self.graph.edges[node_id, target]
                if relationship is None or edge_data.get('relationship') == relationship:
                    connected.append(target)
        return connected
    
    def find_path(self, source_id: str, target_id: str) -> Optional[List[str]]:
        """
        Find the shortest path between two nodes
        
        Args:
            source_id: Source node ID
            target_id: Target node ID
            
        Returns:
            Optional[List[str]]: Path as list of node IDs, or None if no path exists
        """
        try:
            return nx.shortest_path(self.graph, source_id, target_id)
        except nx.NetworkXNoPath:
            return None
    
    def analyze_issues(self) -> Dict[str, Any]:
        """
        Analyze issues in the knowledge graph to identify patterns and root causes
        
        Returns:
            Dict[str, Any]: Analysis results
        """
        analysis = {
            'total_issues': len(self.issues),
            'issues_by_severity': {},
            'issues_by_type': {},
            'affected_entities': {},
            'potential_root_causes': [],
            'issue_patterns': []
        }
        
        # Count issues by severity
        for severity in ['critical', 'high', 'medium', 'low']:
            analysis['issues_by_severity'][severity] = len(self.get_issues_by_severity(severity))
        
        # Count issues by type
        for issue in self.issues:
            issue_type = issue['type']
            analysis['issues_by_type'][issue_type] = analysis['issues_by_type'].get(issue_type, 0) + 1
        
        # Count affected entities by type
        for issue in self.issues:
            node_id = issue['node_id']
            if self.graph.has_node(node_id):
                entity_type = self.graph.nodes[node_id].get('entity_type', 'unknown')
                analysis['affected_entities'][entity_type] = analysis['affected_entities'].get(entity_type, 0) + 1
        
        # Identify potential root causes
        analysis['potential_root_causes'] = self._identify_root_causes()
        
        # Identify issue patterns
        analysis['issue_patterns'] = self._identify_patterns()
        
        kg_logger.info(f"Knowledge Graph analysis completed: {analysis['total_issues']} issues found")
        return analysis
    
    def _identify_root_causes(self) -> List[Dict]:
        """
        Identify potential root causes based on graph topology and issue patterns
        
        Returns:
            List[Dict]: List of potential root causes
        """
        root_causes = []
        
        # Check for drive health issues
        for drive_id in self.find_nodes_by_type('Drive'):
            drive_attrs = self.graph.nodes[drive_id]
            if drive_attrs.get('Health') in ['SUSPECT', 'BAD']:
                # Find all affected pods through the chain: Drive -> PV -> PVC -> Pod
                affected_pods = self._trace_drive_to_pods(drive_id)
                root_causes.append({
                    'type': 'disk_health',
                    'severity': 'high',
                    'source': drive_id,
                    'description': f"Drive {drive_attrs.get('uuid', 'unknown')} has health status: {drive_attrs.get('Health')}",
                    'affected_pods': affected_pods
                })
        
        # Check for node issues
        for node_id in self.find_nodes_by_type('Node'):
            node_attrs = self.graph.nodes[node_id]
            if not node_attrs.get('Ready', True) or node_attrs.get('DiskPressure', False):
                affected_pods = self._trace_node_to_pods(node_id)
                root_causes.append({
                    'type': 'node_health',
                    'severity': 'high',
                    'source': node_id,
                    'description': f"Node {node_attrs.get('name')} has issues: Ready={node_attrs.get('Ready')}, DiskPressure={node_attrs.get('DiskPressure')}",
                    'affected_pods': affected_pods
                })
        
        # Check for permission issues
        permission_issues = [issue for issue in self.issues if issue['type'] == 'permission']
        if permission_issues:
            root_causes.append({
                'type': 'permission',
                'severity': 'medium',
                'source': 'multiple',
                'description': f"Found {len(permission_issues)} permission-related issues",
                'issues': permission_issues
            })
        
        return root_causes
    
    def _identify_patterns(self) -> List[Dict]:
        """
        Identify patterns in issues across the graph
        
        Returns:
            List[Dict]: List of identified patterns
        """
        patterns = []
        
        # Pattern: Multiple pods affected by same drive
        drive_to_pods = {}
        for pod_id in self.find_nodes_by_type('Pod'):
            drives = self._trace_pod_to_drives(pod_id)
            for drive_id in drives:
                if drive_id not in drive_to_pods:
                    drive_to_pods[drive_id] = []
                drive_to_pods[drive_id].append(pod_id)
        
        for drive_id, pod_ids in drive_to_pods.items():
            if len(pod_ids) > 1:
                patterns.append({
                    'type': 'multiple_pods_same_drive',
                    'description': f"Multiple pods ({len(pod_ids)}) using the same drive",
                    'drive': drive_id,
                    'pods': pod_ids
                })
        
        # Pattern: Same error across multiple pods
        error_to_pods = {}
        for issue in self.issues:
            if issue['type'] == 'pod_error':
                error_desc = issue['description']
                if error_desc not in error_to_pods:
                    error_to_pods[error_desc] = []
                error_to_pods[error_desc].append(issue['node_id'])
        
        for error_desc, pod_ids in error_to_pods.items():
            if len(pod_ids) > 1:
                patterns.append({
                    'type': 'same_error_multiple_pods',
                    'description': f"Same error across {len(pod_ids)} pods: {error_desc}",
                    'pods': pod_ids
                })
        
        return patterns
    
    def _trace_drive_to_pods(self, drive_id: str) -> List[str]:
        """
        Trace from a drive to all pods that use it
        
        Args:
            drive_id: Drive node ID
            
        Returns:
            List[str]: List of pod node IDs
        """
        pods = []
        
        # Find PVs that map to this drive
        for pv_id in self.find_nodes_by_type('PV'):
            if drive_id in self.find_connected_nodes(pv_id, 'maps_to'):
                # Find PVCs bound to this PV
                for pvc_id in self.find_nodes_by_type('PVC'):
                    if pv_id in self.find_connected_nodes(pvc_id, 'bound_to'):
                        # Find pods that use this PVC
                        for pod_id in self.find_nodes_by_type('Pod'):
                            if pvc_id in self.find_connected_nodes(pod_id, 'uses'):
                                pods.append(pod_id)
        
        return pods
    
    def _trace_pod_to_drives(self, pod_id: str) -> List[str]:
        """
        Trace from a pod to all drives it uses
        
        Args:
            pod_id: Pod node ID
            
        Returns:
            List[str]: List of drive node IDs
        """
        drives = []
        
        # Find PVCs used by this pod
        pvc_ids = self.find_connected_nodes(pod_id, 'uses')
        for pvc_id in pvc_ids:
            # Find PVs bound to these PVCs
            pv_ids = self.find_connected_nodes(pvc_id, 'bound_to')
            for pv_id in pv_ids:
                # Find drives mapped to these PVs
                drive_ids = self.find_connected_nodes(pv_id, 'maps_to')
                drives.extend(drive_ids)
        
        return drives
    
    def _trace_node_to_pods(self, node_id: str) -> List[str]:
        """
        Trace from a node to all pods scheduled on it
        
        Args:
            node_id: Node node ID
            
        Returns:
            List[str]: List of pod node IDs
        """
        pods = []
        
        # Find pods with affinity to this node
        for pod_id in self.find_nodes_by_type('Pod'):
            pod_attrs = self.graph.nodes[pod_id]
            if pod_attrs.get('node_name') == self.graph.nodes[node_id].get('name'):
                pods.append(pod_id)
        
        return pods
    
    def generate_fix_plan(self, analysis: Dict[str, Any]) -> List[Dict]:
        """
        Generate a prioritized fix plan based on graph analysis
        
        Args:
            analysis: Analysis results from analyze_issues()
            
        Returns:
            List[Dict]: Prioritized list of fix actions
        """
        fix_plan = []
        
        # Process root causes by severity
        root_causes = sorted(analysis['potential_root_causes'], 
                           key=lambda x: {'critical': 0, 'high': 1, 'medium': 2, 'low': 3}.get(x['severity'], 4))
        
        for root_cause in root_causes:
            if root_cause['type'] == 'disk_health':
                fix_plan.append({
                    'step': len(fix_plan) + 1,
                    'type': 'disk_replacement',
                    'priority': 'high',
                    'description': f"Replace unhealthy drive: {root_cause['description']}",
                    'actions': [
                        'Backup data from affected volumes',
                        'Drain affected pods',
                        'Replace the faulty drive',
                        'Restore data and reschedule pods'
                    ],
                    'affected_entities': root_cause.get('affected_pods', [])
                })
            
            elif root_cause['type'] == 'node_health':
                fix_plan.append({
                    'step': len(fix_plan) + 1,
                    'type': 'node_remediation',
                    'priority': 'high',
                    'description': f"Fix node issues: {root_cause['description']}",
                    'actions': [
                        'Investigate node health issues',
                        'Clear disk pressure if present',
                        'Restart node services if needed',
                        'Verify node readiness'
                    ],
                    'affected_entities': root_cause.get('affected_pods', [])
                })
            
            elif root_cause['type'] == 'permission':
                fix_plan.append({
                    'step': len(fix_plan) + 1,
                    'type': 'permission_fix',
                    'priority': 'medium',
                    'description': f"Fix permission issues: {root_cause['description']}",
                    'actions': [
                        'Update Pod SecurityContext',
                        'Verify filesystem permissions',
                        'Restart affected pods'
                    ],
                    'affected_entities': [issue['node_id'] for issue in root_cause.get('issues', [])]
                })
        
        # Add pattern-based fixes
        for pattern in analysis['issue_patterns']:
            if pattern['type'] == 'multiple_pods_same_drive':
                fix_plan.append({
                    'step': len(fix_plan) + 1,
                    'type': 'resource_optimization',
                    'priority': 'low',
                    'description': f"Optimize resource usage: {pattern['description']}",
                    'actions': [
                        'Consider distributing pods across multiple drives',
                        'Monitor drive performance and capacity'
                    ],
                    'affected_entities': pattern['pods']
                })
        
        kg_logger.info(f"Generated fix plan with {len(fix_plan)} steps")
        return fix_plan
    
    def get_summary(self) -> Dict[str, Any]:
        """
        Get a summary of the knowledge graph
        
        Returns:
            Dict[str, Any]: Summary information
        """
        summary = {
            'total_nodes': self.graph.number_of_nodes(),
            'total_edges': self.graph.number_of_edges(),
            'entity_counts': {},
            'total_issues': len(self.issues),
            'critical_issues': len(self.get_issues_by_severity('critical')),
            'high_issues': len(self.get_issues_by_severity('high')),
            'medium_issues': len(self.get_issues_by_severity('medium')),
            'low_issues': len(self.get_issues_by_severity('low'))
        }
        
        # Count entities by type
        for entity_type in ['Pod', 'PVC', 'PV', 'Drive', 'Node', 'StorageClass', 'LVG', 'AC', 'Volume', 'System', 'ClusterNode', 'HistoricalExperience']:
            summary['entity_counts'][entity_type] = len(self.find_nodes_by_type(entity_type))
        
        # Add historical experience count specifically 
        summary['historical_experience_count'] = len(self.find_nodes_by_type('HistoricalExperience'))
        
        return summary
    
    def print_graph(self, include_detailed_entities: bool = True, include_relationships: bool = True, 
                   include_issues: bool = True, include_analysis: bool = True,
                   use_rich: bool = True) -> str:
        """
        Print the knowledge graph in a nice formatted way
        
        Args:
            include_detailed_entities: Whether to include detailed entity information
            include_relationships: Whether to include relationship details
            include_issues: Whether to include issues breakdown
            include_analysis: Whether to include analysis and patterns
            use_rich: Whether to use rich formatting for enhanced visual output
            
        Returns:
            str: Formatted graph representation
        """
        # Import rich components if available
        try:
            from rich.console import Console
            from rich.panel import Panel
            from rich.table import Table
            from rich.tree import Tree
            from rich import print as rprint
            rich_available = True
        except ImportError:
            rich_available = False
            use_rich = False
            
        # Create console for rich output
        console = Console(record=True)
        file_console = Console(file=open('troubleshoot.log', 'a'))

        output = []
        
        # Header
        console.print(Panel(
            "[bold cyan]KUBERNETES STORAGE KNOWLEDGE GRAPH[/bold cyan]",
            border_style="blue",
            width=80
        ))

        # Summary Statistics
        summary = self.get_summary()

        # Create summary table
        summary_table = Table(
            title="[bold] GRAPH SUMMARY",
            #show_header=True,
            header_style="bold cyan",
            #box=True,
            border_style="blue"
        )
        
        summary_table.add_column("Metric", style="dim")
        summary_table.add_column("Value", justify="right")
        
        def safe_format(value: Any) -> str:
            """Safely convert any value to a string for rich formatting"""
            try:
                # Explicitly handle boolean values first
                if isinstance(value, bool):
                    return "True" if value else "False"
                # For all other types, convert to string
                return str(value)
            except Exception:
                return "N/A"

        # Ensure all values are explicitly converted to strings
        summary_table.add_row("Total Nodes", f"[blue]{str(summary['total_nodes'])}[/blue]")
        summary_table.add_row("Total Edges", f"[blue]{str(summary['total_edges'])}[/blue]")
        summary_table.add_row("Total Issues", f"[yellow]{str(summary['total_issues'])}[/yellow]")
        summary_table.add_row("Critical Issues", f"[red]{str(summary['critical_issues'])}[/red]")
        summary_table.add_row("High Issues", f"[orange3]{str(summary['high_issues'])}[/orange3]")
        summary_table.add_row("Medium Issues", f"[yellow]{str(summary['medium_issues'])}[/yellow]")
        summary_table.add_row("Low Issues", f"[green]{str(summary['low_issues'])}[/green]")
        
        try:
            console.print(Panel(
                summary_table,
                safe_box=True  # Explicitly set safe_box to True
            ))
        except Exception as e:
            kg_logger.error(f"Error printing rich summary table: {e}")
            # Fallback to plain text
            output.append("\n GRAPH SUMMARY:")
            output.append("-" * 40)
            output.append(f" Total Nodes: {summary['total_nodes']}")
            output.append(f" Total Edges: {summary['total_edges']}")
            output.append(f" Total Issues: {summary['total_issues']}")
            output.append(f" Critical Issues: {summary['critical_issues']}")
            output.append(f" High Issues: {summary['high_issues']}")
            output.append(f" Medium Issues: {summary['medium_issues']}")
            output.append(f" Low Issues: {summary['low_issues']}")

        # Entity Breakdown
        output.append("\n ENTITY BREAKDOWN:")
        output.append("-" * 40)
        entity_icons = {
            'Pod': '',
            'PVC': '',
            'PV': '',
            'Drive': '',
            'Node': '',
            'StorageClass': '',
            'LVG': '',
            'AC': '',
            'Volume': '',
            'System': '',
            'ClusterNode': ''
        }
        
        for entity_type, count in summary['entity_counts'].items():
            if count > 0:
                icon = entity_icons.get(entity_type, '')
                output.append(f"{icon} {entity_type}: {count}")
        
        # Detailed Entity Information
        if include_detailed_entities and summary['total_nodes'] > 0:
            output.append("\n DETAILED ENTITIES:")
            output.append("-" * 40)
            
            for entity_type in ['Pod', 'PVC', 'PV', 'Drive', 'Node', 'StorageClass', 'LVG', 'AC', 'Volume', 'System', 'ClusterNode']:
                nodes = self.find_nodes_by_type(entity_type)
                if nodes:
                    icon = entity_icons.get(entity_type, '')
                    output.append(f"\n{icon} {entity_type}s:")
                    for node_id in nodes[:5]:  # Limit to first 5 to avoid too much output
                        node_attrs = self.graph.nodes[node_id]
                        name = node_attrs.get('name', node_attrs.get('uuid', 'unknown'))
                        
                        # Add status indicators
                        status_indicators = []
                        if entity_type == 'Drive':
                            health = node_attrs.get('Health', 'UNKNOWN')
                            if health == 'GOOD':
                                status_indicators.append(' Healthy')
                            elif health in ['SUSPECT', 'BAD']:
                                status_indicators.append(f' {health}')
                            else:
                                status_indicators.append(f' {health}')
                        elif entity_type == 'Node':
                            if node_attrs.get('Ready', True):
                                status_indicators.append(' Ready')
                            else:
                                status_indicators.append(' Not Ready')
                            if node_attrs.get('DiskPressure', False):
                                status_indicators.append(' Disk Pressure')
                        elif entity_type == 'Pod':
                            if 'issues' in node_attrs and node_attrs['issues']:
                                status_indicators.append(f" {len(node_attrs['issues'])} issues")
                            if node_attrs.get('Phase') == 'Running':
                                status_indicators.append(' Running')
                            elif node_attrs.get('Phase') == 'Pending':
                                status_indicators.append(' Pending')
                            elif node_attrs.get('Phase') == 'Failed':
                                status_indicators.append(' Failed')
                        elif entity_type == 'PVC':
                            status = node_attrs.get('Phase', 'UNKNOWN')
                            if status == 'Bound':
                                status_indicators.append(' Bound')
                            elif status == 'Pending':
                                status_indicators.append(' Pending')
                            else:
                                status_indicators.append(f' {status}')
                        elif entity_type == 'PV':
                            status = node_attrs.get('Phase', 'UNKNOWN')
                            if status == 'Available':
                                status_indicators.append(' Available')
                            elif status == 'Bound':
                                status_indicators.append(' Bound')
                            else:
                                status_indicators.append(f' {status}')
                        elif entity_type == 'StorageClass':
                            provisioner = node_attrs.get('provisioner', 'unknown')
                            reclaim_policy = node_attrs.get('reclaimPolicy', 'unknown')
                            status_indicators.append(f' {provisioner} | {reclaim_policy}')
                        elif entity_type == 'Volume':
                            health = node_attrs.get('Health', 'UNKNOWN')
                            if health == 'GOOD':
                                status_indicators.append(' Healthy')
                            elif health in ['SUSPECT', 'BAD']:
                                status_indicators.append(f' {health}')
                            usage = node_attrs.get('Usage', 'UNKNOWN')
                            if usage:
                                status_indicators.append(f' {usage}')
                        elif entity_type == 'System':
                            subtype = node_attrs.get('subtype', 'unknown')
                            status_indicators.append(f' {subtype}')
                            if 'issues' in node_attrs and node_attrs['issues']:
                                status_indicators.append(f" {len(node_attrs['issues'])} issues")
                        
                        status_str = ' | '.join(status_indicators) if status_indicators else ' No status'
                        output.append(f"   {name} - {status_str}")
                    
                    if len(nodes) > 5:
                        output.append(f"  ... and {len(nodes) - 5} more")
        
        # Relationships
        if include_relationships and self.graph.number_of_edges() > 0:
            output.append("\n KEY RELATIONSHIPS:")
            output.append("-" * 40)
            
            relationship_counts = {}
            for u, v, data in self.graph.edges(data=True):
                rel_type = data.get('relationship', 'unknown')
                relationship_counts[rel_type] = relationship_counts.get(rel_type, 0) + 1
            
            for rel_type, count in sorted(relationship_counts.items()):
                rel_icon = {
                    'uses': '',
                    'bound_to': '',
                    'maps_to': '',
                    'affinity_to': '',
                    'uses_storage_class': '',
                    'contains': '',
                    'located_on': '',
                    'available_on': '',
                    'monitors': ''
                }.get(rel_type, '')
                output.append(f"{rel_icon} {rel_type}: {count} connections")
            
            # Show VolumeStorage relationships specifically
            volume_relationships = []
            for volume_id in self.find_nodes_by_type('Volume'):
                volume_name = self.graph.nodes[volume_id].get('name', volume_id.split(':')[-1])
                
                # Find direct Drive connections
                for drive_id in self.find_connected_nodes(volume_id, 'bound_to'):
                    if drive_id.startswith('Drive:'):
                        drive_uuid = drive_id.split(':')[-1][:8] + "..."  # Truncate UUID for display
                        volume_relationships.append(f" {volume_name}   {drive_uuid}")
                
                # Find LVG connections
                for lvg_id in self.find_connected_nodes(volume_id, 'bound_to'):
                    if lvg_id.startswith('LVG:'):
                        lvg_name = lvg_id.split(':')[-1][:8] + "..."  # Truncate UUID for display
                        volume_relationships.append(f" {volume_name}   {lvg_name}")
            
            if volume_relationships:
                output.append("\n VolumeStorage Relationships:")
                for rel in volume_relationships[:5]:  # Show first 5
                    output.append(f"   {rel}")
                if len(volume_relationships) > 5:
                    output.append(f"  ... and {len(volume_relationships) - 5} more")
            
            # Show some example relationships
            output.append("\n Example Relationships:")
            shown_relationships = 0
            for u, v, data in self.graph.edges(data=True):
                #if shown_relationships >= 5:  # Limit examples
                #    break
                rel_type = data.get('relationship', 'unknown')
                u_name = self.graph.nodes[u].get('name', u.split(':')[-1])
                u_type = self.graph.nodes[u].get('gnode_subtype', "unknown")

                v_name = self.graph.nodes[v].get('name', v.split(':')[-1])
                v_type = self.graph.nodes[v].get('gnode_subtype', "unknown")

                output.append(f"   ({u_type}){u_name} --{rel_type}--> ({v_type}){v_name}")
                shown_relationships += 1
        
        # Issues Breakdown
        if include_issues and self.issues:
            output.append("\n  ISSUES BREAKDOWN:")
            output.append("-" * 40)
            
            issues_by_severity = {
                'critical': [i for i in self.issues if i['severity'] == 'critical'],
                'high': [i for i in self.issues if i['severity'] == 'high'],
                'medium': [i for i in self.issues if i['severity'] == 'medium'],
                'low': [i for i in self.issues if i['severity'] == 'low']
            }
            
            severity_icons = {
                'critical': '',
                'high': '',
                'medium': '',
                'low': ''
            }
            
            for severity, issues_list in issues_by_severity.items():
                if issues_list:
                    icon = severity_icons[severity]
                    output.append(f"\n{icon} {severity.upper()} Issues ({len(issues_list)}):")
                    for issue in issues_list[:50]:  # Show first 3 issues per severity
                        node_name = self.graph.nodes[issue['node_id']].get('name', issue['node_id'].split(':')[-1])
                        output.append(f"   {node_name}: {issue['description']}")
                    #if len(issues_list) > 3:
                    #    output.append(f"  ... and {len(issues_list) - 3} more")
        
        # Analysis and Patterns
        if include_analysis:
            try:
                analysis = self.analyze_issues()
                
                if analysis['potential_root_causes']:
                    output.append("\n ROOT CAUSE ANALYSIS:")
                    output.append("-" * 40)
                    for i, cause in enumerate(analysis['potential_root_causes'][:50], 1):
                        severity_icon = {'critical': '', 'high': '', 'medium': '', 'low': ''}.get(cause['severity'], '')
                        output.append(f"{i}. {severity_icon} {cause['type'].upper()}")
                        output.append(f"    {cause['description']}")
                        if 'affected_pods' in cause and cause['affected_pods']:
                            output.append(f"    Affects {len(cause['affected_pods'])} pod(s)")
                
                if analysis['issue_patterns']:
                    output.append("\n DETECTED PATTERNS:")
                    output.append("-" * 40)
                    for pattern in analysis['issue_patterns'][:50]:
                        pattern_icon = {
                            'multiple_pods_same_drive': '',
                            'same_error_multiple_pods': ''
                        }.get(pattern['type'], '')
                        output.append(f"{pattern_icon} {pattern['description']}")
                
            except Exception as e:
                output.append(f"\n  Analysis Error: {str(e)}")
        
        # Footer
        output.append("\n" + "=" * 80)
        
        formatted_output = '\n'.join(output)
        
        # Also log the formatted output
        kg_logger.info("Knowledge Graph formatted output generated")
        
        return formatted_output
    
    def export_graph(self, format: str = 'json') -> str:
        """
        Export the knowledge graph in the specified format
        
        Args:
            format: Export format ('json', 'yaml', etc.)
            
        Returns:
            str: Serialized graph data
        """
        if format == 'json':
            graph_data = {
                'nodes': dict(self.graph.nodes(data=True)),
                'edges': [{'source': u, 'target': v, 'attributes': d} 
                         for u, v, d in self.graph.edges(data=True)],
                'issues': self.issues,
                'summary': self.get_summary()
            }
            return json.dumps(graph_data, indent=2, default=str)
        else:
            raise ValueError(f"Unsupported export format: {format}")
</file>

<file path="phases/investigation_planner.py">
#!/usr/bin/env python3
"""
Investigation Planner for Kubernetes Volume Troubleshooting

This module contains the InvestigationPlanner class that generates structured
Investigation Plans based on Knowledge Graph data and issue context.
"""

import logging
from typing import Dict, List, Any, Optional, Tuple
from knowledge_graph import KnowledgeGraph

# Import modules for plan generation
from phases.kg_context_builder import KGContextBuilder
from phases.tool_registry_builder import ToolRegistryBuilder
from phases.llm_plan_generator import LLMPlanGenerator
from phases.rule_based_plan_generator import RuleBasedPlanGenerator
from phases.static_plan_step_reader import StaticPlanStepReader
from phases.utils import validate_knowledge_graph, generate_basic_fallback_plan, handle_exception

logger = logging.getLogger(__name__)

class InvestigationPlanner:
    """
    Generates Investigation Plans based on Knowledge Graph analysis
    
    The Investigation Planner follows a three-step process to create a structured
    step-by-step investigation plan that Phase 1 can follow to efficiently
    diagnose volume I/O issues:
    1. Rule-based preliminary steps - Generate critical initial investigation steps
    2. Static plan steps integration - Add mandatory steps from static_plan_step.json
    3. LLM refinement - Refine and supplement the plan using LLM without tool invocation
    """
    
    def __init__(self, knowledge_graph, config_data: Dict[str, Any] = None):
        """
        Initialize the Investigation Planner
        
        Args:
            knowledge_graph: KnowledgeGraph instance from Phase 0
            config_data: Configuration data for the system (optional)
        """
        self.kg = knowledge_graph
        self.config_data = config_data or {}
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # Validate knowledge_graph is a KnowledgeGraph instance
        validate_knowledge_graph(self.kg, self.__class__.__name__)
        
        # Initialize components
        self.kg_context_builder = KGContextBuilder(knowledge_graph)
        self.tool_registry_builder = ToolRegistryBuilder()
        self.llm_plan_generator = LLMPlanGenerator(config_data)
        self.rule_based_plan_generator = RuleBasedPlanGenerator(knowledge_graph)
        self.static_plan_step_reader = StaticPlanStepReader(config_data)
    
    async def generate_investigation_plan(self, pod_name: str, namespace: str, volume_path: str, 
                                  message_list: List[Dict[str, str]] = None,
                                  use_react: bool = True) -> Tuple[str, List[Dict[str, str]]]:
        """
        Generate a comprehensive Investigation Plan using the three-step process
        
        Args:
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod  
            volume_path: Path of the volume with I/O error
            message_list: Optional message list for chat mode
            use_react: Whether to use React mode (default: True)
            
        Returns:
            Tuple[str, List[Dict[str, str]]]: (Formatted Investigation Plan with step-by-step actions, Updated message list)
        """
        self.logger.info(f"Generating investigation plan for {namespace} {pod_name} volume {volume_path} using {'React' if use_react else 'Legacy'} mode")
        
        try:
            # Generate the plan using the three-step process
            formatted_plan, updated_message_list = await self._generate_plan_with_three_step_process(
                pod_name, namespace, volume_path, message_list, use_react
            )
            return formatted_plan, updated_message_list
            
        except Exception as e:
            error_msg = handle_exception("generate_investigation_plan", e, self.logger)
            fallback_plan = generate_basic_fallback_plan(pod_name, namespace, volume_path)
            
            # Update message list with fallback plan
            updated_message_list = self._update_message_list(message_list, fallback_plan)
            
            return fallback_plan, updated_message_list
    
    async def _generate_plan_with_three_step_process(self, pod_name: str, namespace: str, volume_path: str, 
                                             message_list: List[Dict[str, str]] = None,
                                             use_react: bool = True) -> Tuple[str, List[Dict[str, str]]]:
        """
        Generate a plan using the three-step process
        
        Args:
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod  
            volume_path: Path of the volume with I/O error
            message_list: Optional message list for chat mode
            use_react: Whether to use React mode (default: True)
            
        Returns:
            Tuple[str, List[Dict[str, str]]]: (Formatted Investigation Plan, Updated message list)
        """
        # Prepare Knowledge Graph context and extract necessary data
        kg_context = self.kg_context_builder.prepare_kg_context(pod_name, namespace, volume_path)
        issues_analysis = self.kg_context_builder.analyze_existing_issues()
        target_entities = self.kg_context_builder.identify_target_entities(pod_name, namespace)
        historical_experience = kg_context.get('historical_experiences', [])
        
        # Step 1: Generate preliminary steps using rule-based approach
        self.logger.info("Step 1: Generating rule-based preliminary steps")
        preliminary_steps = self.rule_based_plan_generator.generate_preliminary_steps(
            pod_name, namespace, volume_path, target_entities, issues_analysis, historical_experience
        )
        
        # Step 2: Add static plan steps
        self.logger.info("Step 2: Adding static plan steps")
        draft_plan = self.static_plan_step_reader.add_static_steps(preliminary_steps)
        
        # Step 3: Refine plan using LLM if enabled
        use_llm = self.config_data.get('plan_phase', {}).get('use_llm', True)
        
        if use_llm and self.llm_plan_generator.llm is not None:
            # Refine with LLM
            return await self._refine_plan_with_llm(draft_plan, pod_name, namespace, volume_path, 
                                                 kg_context, message_list, use_react)
        else:
            # Format draft plan directly
            return self._format_draft_plan_with_message_list(draft_plan, pod_name, namespace, 
                                                          volume_path, message_list)
    
    async def _refine_plan_with_llm(self, draft_plan: List[Dict[str, Any]], pod_name: str, namespace: str, 
                                  volume_path: str, kg_context: Dict[str, Any], 
                                  message_list: List[Dict[str, str]] = None,
                                  use_react: bool = True) -> Tuple[str, List[Dict[str, str]]]:
        """
        Refine the plan using LLM
        
        Args:
            draft_plan: Draft plan from rule-based generator and static steps
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod
            volume_path: Path of the volume with I/O error
            kg_context: Knowledge Graph context
            message_list: Optional message list for chat mode
            use_react: Whether to use React mode (default: True)
            
        Returns:
            Tuple[str, List[Dict[str, str]]]: (Refined Investigation Plan, Updated message list)
        """
        self.logger.info("Step 3: Refining plan using LLM")
        
        # Prepare Phase1 tool registry
        phase1_tools = self.tool_registry_builder.prepare_tool_registry()
        
        # Generate final plan using LLM refinement
        return await self.llm_plan_generator.refine_plan(
            draft_plan, pod_name, namespace, volume_path, kg_context, phase1_tools, message_list, use_react
        )
    
    def _format_draft_plan_with_message_list(self, draft_plan: List[Dict[str, Any]], pod_name: str, 
                                           namespace: str, volume_path: str,
                                           message_list: List[Dict[str, str]] = None) -> Tuple[str, List[Dict[str, str]]]:
        """
        Format the draft plan and update message list
        
        Args:
            draft_plan: Draft plan from rule-based generator and static steps
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod
            volume_path: Path of the volume with I/O error
            message_list: Optional message list for chat mode
            
        Returns:
            Tuple[str, List[Dict[str, str]]]: (Formatted Investigation Plan, Updated message list)
        """
        self.logger.info("LLM refinement disabled or unavailable, using draft plan")
        formatted_plan = self._format_draft_plan(draft_plan, pod_name, namespace, volume_path)
        
        # Update message list with formatted plan
        updated_message_list = self._update_message_list(message_list, formatted_plan)
        
        return formatted_plan, updated_message_list
    
    def _update_message_list(self, message_list: List[Dict[str, str]], plan: str) -> List[Dict[str, str]]:
        """
        Update message list with a new plan
        
        Args:
            message_list: Message list to update
            plan: Plan to add to the message list
            
        Returns:
            List[Dict[str, str]]: Updated message list
        """
        if message_list is None:
            return None
            
        # If the last message is from the user, append the assistant response
        if message_list[-1]["role"] == "user":
            message_list.append({"role": "assistant", "content": plan})
        else:
            # Replace the last message if it's from the assistant
            message_list[-1] = {"role": "assistant", "content": plan}
        
        return message_list
    
    def _format_draft_plan(self, draft_plan: List[Dict[str, Any]], pod_name: str, namespace: str, volume_path: str) -> str:
        """
        Format the draft plan into a structured Investigation Plan string
        
        Args:
            draft_plan: Draft plan from rule-based generator and static steps
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod
            volume_path: Path of the volume with I/O error
            
        Returns:
            str: Formatted Investigation Plan
        """
        plan_lines = []
        plan_lines.append("Investigation Plan:")
        plan_lines.append(f"Target: Pod {namespace}/{pod_name}, Volume Path: {volume_path}")
        plan_lines.append(f"Generated Steps: {len(draft_plan)} main steps, 0 fallback steps")
        plan_lines.append("")
        
        # Format main investigation steps
        for step in draft_plan:
            step_line = self._format_step(step)
            plan_lines.append(step_line)
        
        return "\n".join(plan_lines)
    
    def _format_step(self, step: Dict[str, Any]) -> str:
        """
        Format a single investigation step
        
        Args:
            step: Step data
            
        Returns:
            str: Formatted step string
        """
        # Format arguments
        args_str = ', '.join(f'{k}={repr(v)}' for k, v in step.get('arguments', {}).items())
        
        # Format the step line
        return (
            f"Step {step['step']}: {step['description']} | "
            f"Tool: {step['tool']}({args_str}) | "
            f"Expected: {step['expected']}"
        )
</file>

<file path="phases/plan_phase_react.py">
#!/usr/bin/env python3
"""
ReAct Graph Implementation for Plan Phase in Kubernetes Volume Troubleshooting

This module implements a standalone ReAct (Reasoning and Acting) graph using LangGraph
for the plan phase of Kubernetes volume troubleshooting. The graph exclusively uses
MCP (Multi-Component Platform) tools for function calling to gather information
when the plan phase encounters knowledge gaps.
"""

import logging
import json
import asyncio
from typing import Dict, List, Any, TypedDict, Optional, Union, Tuple, Set
from enum import Enum
from rich.console import Console
from rich.panel import Panel

from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import tools_condition, ToolNode
from langchain_core.messages import BaseMessage, ToolMessage, HumanMessage, SystemMessage, AIMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import BaseTool

from phases.llm_factory import LLMFactory
from phases.utils import handle_exception, format_json_safely, generate_basic_fallback_plan
from tools.core.mcp_adapter import get_mcp_adapter
from knowledge_graph import KnowledgeGraph
from troubleshooting.execute_tool_node import ExecuteToolNode
from troubleshooting.strategies import ExecutionType

# Configure logging
logger = logging.getLogger(__name__)

# Define hook functions for PlanPhaseReActGraph
def before_call_tools_hook(tool_name: str, args: Dict[str, Any], call_type: str = "Parallel") -> None:
    """Hook function called before a tool is executed in Plan Phase ReAct.
    
    Args:
        tool_name: Name of the tool being called
        args: Arguments passed to the tool
        call_type: Type of call execution ("Parallel" or "Serial")
    """
    try:
        # Format arguments for better readability
        formatted_args = json.dumps(args, indent=2) if args else "None"
        
        # Format the tool usage in a nice way
        if formatted_args != "None":
            # Print to console
            tool_panel = Panel(
                f"[bold yellow]Tool:[/bold yellow] [green]{tool_name}[/green] [bold cyan]({call_type})[/bold cyan]\n\n"
                f"[bold yellow]Arguments:[/bold yellow]\n[blue]{formatted_args}[/blue]",
                title="[bold magenta]Plan Phase ReAct Tool",
                border_style="magenta",
                safe_box=True
            )
            console = Console()
            console.print(tool_panel)
        else:
            # Simple version for tools without arguments
            tool_panel = Panel(
                f"[bold yellow]Tool:[/bold yellow] [green]{tool_name}[/green] [bold cyan]({call_type})[/bold cyan]\n\n"
                f"[bold yellow]Arguments:[/bold yellow] None",
                title="[bold magenta]Plan Phase ReAct Tool",
                border_style="magenta",
                safe_box=True
            )
            console = Console()
            console.print(tool_panel)

        # Log to standard logger
        logger.info(f"Plan Phase ReAct executing tool: {tool_name} ({call_type})")
        logger.info(f"Parameters: {formatted_args}")
    except Exception as e:
        logger.error(f"Error in before_call_tools_hook: {e}")

def after_call_tools_hook(tool_name: str, args: Dict[str, Any], result: Any, call_type: str = "Parallel") -> None:
    """Hook function called after a tool is executed in Plan Phase ReAct.
    
    Args:
        tool_name: Name of the tool that was called
        args: Arguments that were passed to the tool
        result: Result returned by the tool
        call_type: Type of call execution ("Parallel" or "Serial")
    """
    try:
        # Format result for better readability
        if isinstance(result, ToolMessage):
            result_content = result.content
            result_status = result.status if hasattr(result, 'status') else 'success'
            formatted_result = f"Status: {result_status}\nContent: {result_content[:1000]}"
        else:
            formatted_result = str(result)[:1000]
        
        # Print tool result to console
        tool_panel = Panel(
            f"[bold cyan]Tool completed:[/bold cyan] [green]{tool_name}[/green] [bold cyan]({call_type})[/bold cyan]\n"
            f"[bold cyan]Result:[/bold cyan]\n[yellow]{formatted_result}[/yellow]",
            title="[bold magenta]Plan Phase ReAct Result",
            border_style="magenta",
            safe_box=True
        )
        console = Console()
        console.print(tool_panel)

        # Log to standard logger
        logger.info(f"Plan Phase ReAct tool completed: {tool_name} ({call_type})")
        logger.info(f"Result: {formatted_result}")
    except Exception as e:
        logger.error(f"Error in after_call_tools_hook: {e}")

class PlanPhaseState(TypedDict):
    """State for the Plan Phase ReAct graph"""
    messages: List[BaseMessage]  # Conversation history
    iteration_count: int  # Track iterations
    tool_call_count: int  # Track tool calls
    knowledge_gathered: Dict[str, Any]  # Knowledge gathered from tools
    plan_complete: bool  # Whether the plan is complete
    pod_name: str  # Pod name for context
    namespace: str  # Namespace for context
    volume_path: str  # Volume path for context
    knowledge_graph: Optional[KnowledgeGraph]  # Knowledge graph for context

class ReActStage(Enum):
    """Stages in the ReAct process"""
    REASONING = "reasoning"  # LLM analyzing and reasoning about the problem
    ACTING = "acting"  # Calling tools to gather information
    OBSERVING = "observing"  # Processing tool outputs
    PLANNING = "planning"  # Generating the final plan

class PlanPhaseReActGraph:
    """
    ReAct Graph for Plan Phase using LangGraph
    
    Implements a standalone ReAct (Reasoning and Acting) graph that exclusively
    uses MCP tools for function calling when the plan phase encounters knowledge gaps.
    """
    
    def __init__(self, config_data: Dict[str, Any] = None, messages: List[BaseMessage] = None):
        """
        Initialize the Plan Phase ReAct Graph
        
        Args:
            config_data: Configuration data for the system
        """
        self.config_data = config_data or {}
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # Initialize LLM
        self.llm = self._initialize_llm()
        
        # Get MCP tools
        self.mcp_tools = self._get_mcp_tools_for_plan_phase()
        
        # Maximum iterations to prevent infinite loops
        self.max_iterations = self.config_data.get("max_iterations", 15)

        self.init_message = messages
    
    def _initialize_llm(self):
        """
        Initialize the LLM for the ReAct graph
        
        Returns:
            BaseChatModel: Initialized LLM instance
        """
        try:
            # Create LLM using the factory
            llm_factory = LLMFactory(self.config_data)
            
            # Check if streaming is enabled in config
            streaming_enabled = self.config_data.get('llm', {}).get('streaming', False)
            
            # Create LLM with streaming if enabled
            return llm_factory.create_llm(
                streaming=streaming_enabled,
                phase_name="plan_phase"
            )
        except Exception as e:
            error_msg = handle_exception("_initialize_llm", e, self.logger)
            raise ValueError(f"Failed to initialize LLM: {error_msg}")
    
    def _get_mcp_tools_for_plan_phase(self) -> List[BaseTool]:
        """
        Get MCP tools for the plan phase
        
        Returns:
            List[BaseTool]: List of MCP tools for the plan phase
        
        Raises:
            ValueError: If MCP integration is not enabled or no tools are available
        """
        # Get MCP adapter
        mcp_adapter = get_mcp_adapter()
        
        if not mcp_adapter:
            self.logger.warning("MCP adapter not initialized, no MCP tools will be available")
            return []
        
        if not mcp_adapter.mcp_enabled:
            self.logger.warning("MCP integration is disabled, no MCP tools will be available")
            return []
        
        # Get MCP tools for plan phase
        mcp_tools = mcp_adapter.get_tools_for_phase('plan_phase')
        
        if not mcp_tools:
            self.logger.warning("No MCP tools available for plan phase")
            return []
        
        self.logger.info(f"Loaded {len(mcp_tools)} MCP tools for Plan Phase")
        return mcp_tools
    
    def build_graph(self) -> StateGraph:
        """
        Build the Plan Phase ReAct graph
        
        Returns:
            StateGraph: Compiled LangGraph StateGraph
        """
        # Build state graph
        self.logger.info("Building Plan Phase ReAct graph")
        builder = StateGraph(PlanPhaseState)
        
        # Add nodes
        self.logger.info("Adding node: call_model")
        builder.add_node("call_model", self.call_model)
        
        # Initialize ExecuteToolNode with all MCP tools set to parallel execution
        execute_tools_node = self._initialize_execute_tool_node()
        self.logger.info("Adding node: execute_tools")
        builder.add_node("execute_tools", execute_tools_node)
        
        self.logger.info("Adding node: check_end")
        builder.add_node("check_end", self.check_end_conditions)
        
        # Add edges
        self.logger.info("Adding edge: START -> call_model")
        builder.add_edge(START, "call_model")
        
        # Add conditional edges for tools
        self.logger.info("Adding conditional edges for tools")
        builder.add_conditional_edges(
            "call_model",
            tools_condition,
            {
                "tools": "execute_tools",   # Route to execute_tools node
                "none": "check_end",        # If no tools, go to check_end
                "__end__": "check_end"
            }
        )
        
        # Add edge from execute_tools to call_model
        self.logger.info("Adding edge: execute_tools -> call_model")
        builder.add_edge("execute_tools", "call_model")
        
        # Add conditional edges from check_end node
        self.logger.info("Adding conditional edges from check_end node")
        builder.add_conditional_edges(
            "check_end",
            lambda state: self.check_end_conditions(state)["result"],
            {
                "end": END,
                "__end__": END,
                "continue": "call_model"  # Loop back if conditions not met
            }
        )
        
        # Compile graph
        self.logger.info("Compiling graph")
        return builder.compile()
    
    def call_model(self, state: PlanPhaseState) -> PlanPhaseState:
        """
        LLM reasoning node that analyzes current state and decides next action
        
        Args:
            state: Current state of the graph
            
        Returns:
            PlanPhaseState: Updated state after LLM reasoning
        """
        # Increment iteration count
        state["iteration_count"] += 1
        
        # Check if we've reached max iterations
        if state["iteration_count"] > self.max_iterations:
            self.logger.info(f"Reached max iterations ({self.max_iterations}), marking plan as complete")
            state["plan_complete"] = True
            
            # Add a final message indicating max iterations reached
            final_message = AIMessage(content=f"[MAX_ITERATIONS_REACHED] Completed {self.max_iterations} iterations. Finalizing plan with current information.")
            state["messages"].append(final_message)
            
            return state
        
        self.logger.info(f"Calling model (iteration {state['iteration_count']})")
        
        try:
            # Prepare messages if this is the first iteration or if messages need to be initialized
            state = self._prepare_messages(state)
            
            # Call the model with tools
            response = None
            if len(self.mcp_tools) != 0:
                response = self.llm.bind_tools(self.mcp_tools).invoke(state["messages"])
            else:
                response = self.llm.invoke(state["messages"])
            
            # Add response to messages
            state["messages"].append(response)
            
            # Log the response
            self.logger.info(f"Model response: {response.content[:100]}...")
            #print (f"Model response: {response.content[:]}...")
            return state
        except Exception as e:
            error_msg = handle_exception("call_model", e, self.logger)
            
            # Add error message to state
            error_message = SystemMessage(content=f"Error calling model: {error_msg}")
            state["messages"].append(error_message)
            
            # Mark plan as complete to exit the loop
            state["plan_complete"] = True
            
            return state
    
    def _initialize_execute_tool_node(self) -> ExecuteToolNode:
        """
        Initialize ExecuteToolNode with all MCP tools set to parallel execution
        
        Returns:
            ExecuteToolNode: Configured ExecuteToolNode instance
        """
        if not self.mcp_tools:
            self.logger.warning("No MCP tools available for ExecuteToolNode")
            # Return empty ExecuteToolNode if no tools available
            return ExecuteToolNode(
                tools=[],
                parallel_tools=set(),
                serial_tools=set()
            )
        
        # Get all tool names
        tool_names = {tool.name for tool in self.mcp_tools}
        
        # Configure all MCP tools to run in parallel by default
        parallel_tools = tool_names
        serial_tools = set()
        
        self.logger.info(f"Configuring ExecuteToolNode with {len(parallel_tools)} parallel tools")
        
        # Create and return ExecuteToolNode with all tools set to parallel execution
        execute_tool_node = ExecuteToolNode(
            tools=self.mcp_tools,
            parallel_tools=parallel_tools,
            serial_tools=serial_tools,
            handle_tool_errors=True,
            messages_key="messages"
        )
        
        # Create a hook manager for console output
        from troubleshooting.hook_manager import HookManager
        
        console = Console()
        file_console = Console(file=open('plan_phase_react.log', 'a'))
        hook_manager = HookManager(console=console, file_console=file_console)
        
        # Register custom hook functions with the hook manager
        hook_manager.register_before_call_hook(before_call_tools_hook)
        hook_manager.register_after_call_hook(after_call_tools_hook)
        
        # Register hook manager with the ExecuteToolNode
        execute_tool_node.register_before_call_hook(hook_manager.run_before_hook)
        execute_tool_node.register_after_call_hook(hook_manager.run_after_hook)
        
        return execute_tool_node

    def _prepare_messages(self, state: PlanPhaseState) -> PlanPhaseState:
        """
        Prepare messages for the LLM with system prompt and context
        
        Similar to graph.py's _prepare_messages() but using LLMPlanGenerator's approach for
        preparing system prompt and human query.
        
        Args:
            state: Current state with messages
            
        Returns:
            PlanPhaseState: Updated state with prepared messages
        """
        
        user_messages = []
        if state["messages"]:
            if isinstance(state["messages"], list):
                for msg in state["messages"]:
                    if not isinstance(msg, SystemMessage) and not isinstance(msg, HumanMessage):
                        user_messages.append(msg)
                
                # Create new message list with system message, context message, and existing user messages
                state["messages"] = self.init_message + user_messages
            else:
                state["messages"] = [self.init_message, state["messages"]]
        else:
            state["messages"] = self.init_message

        self.logger.info("Prepared initial messages with system prompt and user query")
        return state
        
    def check_end_conditions(self, state: PlanPhaseState) -> Dict[str, str]:
        """
        Check if plan generation is complete
        
        Args:
            state: Current state of the graph
            
        Returns:
            Dict[str, str]: Result indicating whether to end or continue
        """
        self.logger.info("Checking end conditions for Plan Phase ReAct graph")

        # If plan is already marked as complete, end the graph
        if state["plan_complete"]:
            self.logger.info("Plan marked as complete, ending graph")
            return {"result": "end"}
        
        # Check if we've reached max iterations
        if state["iteration_count"] >= self.max_iterations:
            self.logger.info(f"Reached max iterations ({self.max_iterations}), ending graph")
            return {"result": "end"}
        
        # Get the last message
        messages = state["messages"]
        if not messages:
            return {"result": "continue"}
        
        last_message = messages[-1]
        
        # Skip content checks if the last message isn't from the AI
        if getattr(last_message, "type", "") != "ai":
            return {"result": "continue"}
        
        content = getattr(last_message, "content", "")
        if not content:
            return {"result": "continue"}
        
        llm_end_markers = self._check_explicit_end_markers(content)
        if llm_end_markers:
            self.logger.info("Detected end markers from LLM, ending graph")
            state["plan_complete"] = True
            return {"result": "end"}

        # Check for explicit end markers in the content
        end_markers = ["[END_GRAPH]", "[END]", "End of graph", "GRAPH END", "Investigation Plan:", "Fix Plan:", "Step by Step"]
        if any(marker in content for marker in end_markers):
            self.logger.info(f"Detected end marker in content, ending graph")
            state["plan_complete"] = True
            return {"result": "end"}
        
        # Check for completion indicators
        if "Summary of Findings:" in content and "Root Cause:" in content and "Fix Plan:" in content:
            self.logger.info("Detected completion indicators in content, ending graph")
            state["plan_complete"] = True
            return {"result": "end"}
        
        # Check for convergence (model repeating itself)
        ai_messages = [m for m in messages if getattr(m, "type", "") == "ai"]
        if len(ai_messages) > 3:
            # Compare the last message with the third-to-last message
            last_content = content
            third_to_last_content = getattr(ai_messages[-3], "content", "")
            
            # Simple similarity check - if they start with the same paragraph
            if last_content and third_to_last_content:
                # Get first 100 chars of each message
                last_start = last_content[:100] if len(last_content) > 100 else last_content
                third_start = third_to_last_content[:100] if len(third_to_last_content) > 100 else third_to_last_content
                
                if last_start == third_start:
                    self.logger.info("Detected convergence (model repeating itself), ending graph")
                    state["plan_complete"] = True
                    return {"result": "end"}
        
        # Default: continue execution
        return {"result": "continue"}

    def _check_explicit_end_markers(self, content: str) -> bool:
        """Use LLM to check if content contains explicit or implicit end markers.
        
        Args:
            content: The content to check for end markers
            
        Returns:
            bool: True if end markers detected, False otherwise
        """
        # Create a focused prompt for the LLM
        system_prompt = """
        You are an AI assistant tasked with determining if a text contains explicit or implicit markers 
        indicating the end of a process or conversation. Your task is to analyze the given text and 
        determine if it contains phrases or markers that suggest completion or termination.
        
        Examples of explicit end markers include:
        - "[END_GRAPH]", "[END]", "End of graph", "GRAPH END"
        - "This concludes the analysis"
        - "Final report"
        - "Step by Step"
        - "Step XXX: [Description and Reason] | Tool: [tool_name(parameters)] | Expected: [expected]"
        - "Investigation complete"
        - " Would you like to"
        - A question from AI that indicates the end of the process, such as " Would you like to proceed with planning the disk replacement or further investigate filesystem integrity?"
        - If just a call tools result, then return 'NO'

        Examples of implicit end markers include:
        - A summary followed by recommendations with no further questions
        - A conclusion paragraph that wraps up all findings
        - A complete analysis with all required sections present
        - A question from AI that indicates the end of the process, such as "Is there anything else I can help you with?" or "Do you have any further questions?"
        
        Respond with "YES" if you detect end markers, or "NO" if you don't.
        """
        
        user_prompt = f"""
        Analyze the following text and determine if it contains explicit or implicit end markers:
        
        {content}  # Limit content length to avoid token limits
        
        Does this text contain markers indicating it's the end of the process? Respond with only YES or NO.
        """
        
        try:
            # Create messages for the LLM
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            
            # Call the LLM
            response = self.llm.invoke(messages)
            
            # Check if the response indicates end markers
            response_text = response.content.strip().upper()
            
            # Log the LLM's response
            logger.info(f"LLM end marker detection response: {response_text}")
            
            # Return True if the LLM detected end markers
            return "YES" in response_text
        except Exception as e:
            # Log any errors and fall back to the original behavior
            logger.error(f"Error in LLM end marker detection: {e}")
            
            # Fall back to simple string matching
            return any(marker in content for marker in ["[END_GRAPH]", "[END]", "End of graph", "GRAPH END", "Fix Plan", "FIX PLAN"])
    
    def extract_plan_from_state(self, state: PlanPhaseState) -> str:
        """
        Extract the investigation plan from the final state
        
        Args:
            state: Final state of the graph
            
        Returns:
            str: Investigation plan as a formatted string
        """
        # Get the last AI message
        ai_messages = [m for m in state["messages"] if getattr(m, "type", "") == "ai"]
        
        if not ai_messages:
            self.logger.warning("No AI messages found in final state")
            return generate_basic_fallback_plan(
                state["pod_name"], state["namespace"], state["volume_path"]
            )
        
        # Get the content of the last AI message
        last_ai_message = ai_messages[-1]
        content = getattr(last_ai_message, "content", "")
        
        # Extract the investigation plan
        if "Investigation Plan:" in content:
            plan_index = content.find("Investigation Plan:")
            plan = content[plan_index:]
            
            # Remove end markers
            for marker in ["[END_GRAPH]", "[END]", "End of graph", "GRAPH END"]:
                plan = plan.replace(marker, "")
                
            return plan.strip()
        
        # If no plan found, return the entire content
        return content.strip()

async def run_plan_phase_react(pod_name: str, namespace: str, volume_path: str, 
                             messages: List[BaseMessage] = None, 
                             config_data: Dict[str, Any] = None) -> Tuple[str, List[Dict[str, str]]]:
    """
    Run the Plan Phase using ReAct graph
    
    Args:
        pod_name: Name of the pod with the error
        namespace: Namespace of the pod
        volume_path: Path of the volume with I/O error
        messages: List of messages (system prompt, user message) to use for the ReAct graph
        config_data: Configuration data for the system (optional)
        
    Returns:
        Tuple[str, List[Dict[str, str]]]: (Investigation Plan as a formatted string, Updated message list)
    """
    logger = logging.getLogger(__name__)
    logger.info(f"Running Plan Phase ReAct for {namespace}/{pod_name} volume {volume_path}")
    
    try:
        # Initialize and build the ReAct graph
        react_graph = PlanPhaseReActGraph(config_data, messages if messages else [])
        graph = react_graph.build_graph() 

        # Prepare initial state with provided messages
        initial_state = {
            "messages": messages if messages else [],
            "iteration_count": 0,
            "tool_call_count": 0,
            "knowledge_gathered": {},
            "plan_complete": False,
            "pod_name": pod_name,
            "namespace": namespace,
            "volume_path": volume_path,
            "knowledge_graph": None  # Knowledge graph information is now in messages
        }
        
        # Run the graph
        logger.info("Running Plan Phase ReAct graph")
        final_state = graph.invoke(initial_state)
        
        # Extract the investigation plan
        investigation_plan = react_graph.extract_plan_from_state(final_state)
        
        # Convert messages to message list format
        message_list = _convert_messages_to_message_list(final_state["messages"])
        
        # Log the results
        logger.info("Plan Phase ReAct completed successfully")
        
        return investigation_plan, message_list
        
    except Exception as e:
        error_msg = handle_exception("run_plan_phase_react", e, logger)
        fallback_plan = generate_basic_fallback_plan(pod_name, namespace, volume_path)
        return fallback_plan, []

def _convert_messages_to_message_list(messages: List[BaseMessage]) -> List[Dict[str, str]]:
    """
    Convert BaseMessage list to message list format
    
    Args:
        messages: List of BaseMessage objects
        
    Returns:
        List[Dict[str, str]]: Message list format
    """
    message_list = []
    
    for message in messages:
        role = "system"
        if isinstance(message, HumanMessage):
            role = "user"
        elif isinstance(message, AIMessage):
            role = "assistant"
        elif isinstance(message, ToolMessage):
            role = "tool"
        
        message_list.append({
            "role": role,
            "content": message.content
        })
    
    return message_list
</file>

<file path="tools/diagnostics/disk_analysis.py">
#!/usr/bin/env python3
"""
Disk analysis tools for volume troubleshooting.

This module contains tools for analyzing disk health, space usage,
and scanning system logs for disk-related errors.
"""

import re
import json
import subprocess
from datetime import datetime
from typing import Dict, List, Optional, Any
from langchain_core.tools import tool
from tools.diagnostics.system import journalctl_command, dmesg_command

@tool
def check_disk_health(node_name: str, device_path: str) -> str:
    """
    Query disk SMART data to assess overall disk health
    
    This tool provides a comprehensive health assessment including
    attributes like reallocated sectors, temperature, and wear leveling.
    
    Args:
        node_name: Node hostname or IP
        device_path: Device path (e.g., /dev/sda)
        
    Returns:
        str: Disk health assessment with key metrics and status
    """
    try:
        from tools.diagnostics.hardware import ssh_execute, smartctl_check
        
        # Get SMART data
        smart_output = smartctl_check.invoke({'node_name': node_name, 'device_path': device_path})

        # Parse SMART data for key health indicators
        health_status = "Unknown"
        temperature = "Unknown"
        power_on_hours = "Unknown"
        reallocated_sectors = "Unknown"
        pending_sectors = "Unknown"
        offline_uncorrectable = "Unknown"
        
        # Extract overall health status
        health_match = re.search(r"SMART overall-health self-assessment test result: (\w+)", smart_output)
        if health_match:
            health_status = health_match.group(1)
        
        # Extract key attributes
        attributes = {
            "Temperature": re.search(r"Temperature.*?(\d+)", smart_output),
            "Power_On_Hours": re.search(r"Power_On_Hours.*?(\d+)", smart_output),
            "Reallocated_Sector_Ct": re.search(r"Reallocated_Sector_Ct.*?(\d+)", smart_output),
            "Current_Pending_Sector": re.search(r"Current_Pending_Sector.*?(\d+)", smart_output),
            "Offline_Uncorrectable": re.search(r"Offline_Uncorrectable.*?(\d+)", smart_output),
            "Wear_Leveling_Count": re.search(r"Wear_Leveling_Count.*?(\d+)", smart_output)
        }
        
        # Extract values from matches
        for attr, match in attributes.items():
            if match:
                attributes[attr] = match.group(1)
        
        # Format health summary
        summary = [
            f"Disk Health Assessment for {device_path} on {node_name}:",
            f"Overall Health: {health_status}",
            f"Temperature: {attributes.get('Temperature', 'N/A')}C",
            f"Power On Hours: {attributes.get('Power_On_Hours', 'N/A')}",
            f"Reallocated Sectors: {attributes.get('Reallocated_Sector_Ct', 'N/A')}",
            f"Current Pending Sectors: {attributes.get('Current_Pending_Sector', 'N/A')}",
            f"Offline Uncorrectable Sectors: {attributes.get('Offline_Uncorrectable', 'N/A')}",
            f"Wear Leveling Count: {attributes.get('Wear_Leveling_Count', 'N/A')}\n"
            f"Original SMART Output:\n{smart_output}"
        ]
        
        # Add recommendations based on health indicators
        recommendations = []
        
        if health_status.lower() != "passed":
            recommendations.append("Disk has failed SMART health assessment - immediate replacement recommended")
        
        # Check for concerning values in key attributes
        if attributes.get('Reallocated_Sector_Ct', '0') != '0':
            recommendations.append("Disk has reallocated sectors - monitor closely for further deterioration")
            
        if attributes.get('Current_Pending_Sector', '0') != '0':
            recommendations.append("Disk has pending sectors - data backup recommended")
            
        if attributes.get('Offline_Uncorrectable', '0') != '0':
            recommendations.append("Disk has uncorrectable sectors - consider replacement")
        
        # Add recommendations to summary if any
        if recommendations:
            summary.append("\nRecommendations:")
            summary.extend(recommendations)
        
        return "\n".join(summary)
        
    except Exception as e:
        return f"Error checking disk health: {str(e)}"

@tool
def analyze_disk_space_usage(node_name: str, mount_path: str = "/", 
                            min_file_size_mb: int = 100, 
                            show_top_n: int = 10) -> str:
    """
    Analyze disk space usage to identify large files and potential space issues
    
    This tool identifies large files, unused files, or potential space leaks,
    with options to generate a detailed report.
    
    Args:
        node_name: Node hostname or IP
        mount_path: Path to analyze (default: root)
        min_file_size_mb: Minimum file size to report in MB
        show_top_n: Number of largest files/directories to show
        
    Returns:
        str: Disk space analysis report
    """
    try:
        from tools.diagnostics.hardware import ssh_execute
        
        # Get overall disk usage
        df_cmd = f"df -h {mount_path}"
        df_output = ssh_execute.invoke({'node_name': node_name, 'command': df_cmd})
        
        # Get directory usage breakdown
        du_cmd = f"du -h --max-depth=2 {mount_path} | sort -hr | head -n {show_top_n}"
        du_output = ssh_execute.invoke({'node_name': node_name, 'command': du_cmd})

        # Find large files
        find_cmd = f"find {mount_path} -type f -size +{min_file_size_mb}M -exec ls -lh {{}} \\; | sort -k5hr | head -n {show_top_n}"
        find_output = ssh_execute.invoke({'node_name': node_name, 'command': find_cmd})

        # Find old unused files (not accessed in 90+ days)
        old_files_cmd = f"find {mount_path} -type f -atime +90 -size +{min_file_size_mb}M -exec ls -lh {{}} \\; | sort -k5hr | head -n {show_top_n}"
        old_files_output = ssh_execute.invoke({'node_name': node_name, 'command': old_files_cmd})

        # Format analysis report
        report = [
            f"Disk Space Analysis for {mount_path} on {node_name}:",
            "\n=== Overall Disk Usage ===",
            df_output,
            "\n=== Largest Directories ===",
            du_output,
            "\n=== Largest Files ===",
            find_output,
            "\n=== Large Unused Files (not accessed in 90+ days) ===",
            old_files_output
        ]
        
        # Add recommendations based on findings
        recommendations = []
        
        # Check if disk usage is high (over 85%)
        if "85%" in df_output or "9%" in df_output:
            recommendations.append("High disk usage detected (>85%) - consider cleanup")
        
        # Check for log files in largest files
        if ".log" in find_output:
            recommendations.append("Large log files detected - consider log rotation or cleanup")
            
        # Check for old unused files
        if len(old_files_output.strip()) > 0:
            recommendations.append("Large unused files detected - consider archiving or removing")
        
        # Add recommendations to report if any
        if recommendations:
            report.append("\n=== Recommendations ===")
            report.extend(recommendations)
        
        return "\n".join(report)
        
    except Exception as e:
        return f"Error analyzing disk space usage: {str(e)}"

@tool
def scan_disk_error_logs(node_name: str, hours_back: int = 24, 
                         log_paths: List[str] = None) -> str:
    """
    Scan system logs for disk-related errors or warnings
    
    This tool scans system logs for disk-related errors or warnings,
    summarizing findings with actionable insights. It checks traditional
    log files, journalctl logs, and dmesg output.
    
    Args:
        node_name: Node hostname or IP
        hours_back: Hours of logs to scan
        log_paths: List of log paths to scan (default: common system logs)
        
    Returns:
        str: Summary of disk-related errors with insights
    """
    try:
        from tools.diagnostics.hardware import ssh_execute
        
        # Default log paths if not specified
        if log_paths is None:
            log_paths = [
                "/var/log/syslog",
                "/var/log/kern.log",
                "/var/log/dmesg",
                "/var/log/messages"
            ]
        
        # Keywords to search for
        disk_error_keywords = [
            "I/O error", "read error", "write error", "sector error",
            "disk failure", "drive failure", "bad sector", "failed command",
            "ata error", "scsi error", "medium error", "sense key",
            "timeout", "reset", "offline", "uncorrectable", "ECC"
        ]
        
        # Build grep pattern
        grep_pattern = "|".join(disk_error_keywords)
        
        # Results storage
        results = []
        error_count = 0
        
        # Process each log file
        for log_path in log_paths:
            # Check if log file exists
            check_cmd = f"test -f {log_path} && echo exists || echo not found"
            check_result = ssh_execute.invoke({'node_name': node_name, 'command': check_cmd}).strip()
            
            if check_result == "not found":
                results.append(f"Log file {log_path} not found")
                continue
            
            # Get timestamp for hours_back
            time_cmd = f"date -d '{hours_back} hours ago' +'%Y-%m-%d %H:%M:%S'"
            time_result = ssh_execute.invoke({'node_name': node_name, 'command': time_cmd}).strip()
            
            # Search log file for disk errors after the timestamp
            grep_cmd = f"grep -E '{grep_pattern}' {log_path} | grep -A 2 -B 2 '{grep_pattern}'"
            grep_result = ssh_execute.invoke({'node_name': node_name, 'command': grep_cmd}).strip()
            
            # Count errors
            error_lines = grep_result.strip().split('\n')
            if error_lines and error_lines[0]:
                file_error_count = len(error_lines)
                error_count += file_error_count
                results.append(f"\n=== {log_path} ({file_error_count} errors) ===")
                
                # Limit output to avoid overwhelming
                if file_error_count > 20:
                    results.append(f"First 20 of {file_error_count} errors:")
                    results.append(grep_result.split('\n')[:20])
                else:
                    results.append(grep_result)
            else:
                results.append(f"\n=== {log_path} (No errors) ===")
        
        # Get logs from journalctl
        journalctl_options = f"--since='{hours_back} hours ago' -p err"
        journalctl_result = journalctl_command.invoke({'node_name': node_name, 'options': journalctl_options})
        
        # Filter journalctl output for disk errors
        journalctl_errors = []
        for line in journalctl_result.split('\n'):
            if any(keyword.lower() in line.lower() for keyword in disk_error_keywords):
                journalctl_errors.append(line)
        
        # Add journalctl results
        if journalctl_errors:
            journalctl_error_count = len(journalctl_errors)
            error_count += journalctl_error_count
            results.append(f"\n=== journalctl ({journalctl_error_count} errors) ===")
            
            # Limit output to avoid overwhelming
            if journalctl_error_count > 20:
                results.append(f"First 20 of {journalctl_error_count} errors:")
                results.append('\n'.join(journalctl_errors[:20]))
            else:
                results.append('\n'.join(journalctl_errors))
        else:
            results.append("\n=== journalctl (No errors) ===")
        
        # Get logs from dmesg
        dmesg_options = f"--level=err,crit,alert,emerg --since='{hours_back} hours ago' -T"
        dmesg_result = dmesg_command.invoke({'node_name': node_name, 'options': dmesg_options})
        
        # Filter dmesg output for disk errors
        dmesg_errors = []
        for line in dmesg_result.split('\n'):
            if any(keyword.lower() in line.lower() for keyword in disk_error_keywords):
                dmesg_errors.append(line)
        
        # Add dmesg results
        if dmesg_errors:
            dmesg_error_count = len(dmesg_errors)
            error_count += dmesg_error_count
            results.append(f"\n=== dmesg ({dmesg_error_count} errors) ===")
            
            # Limit output to avoid overwhelming
            if dmesg_error_count > 20:
                results.append(f"First 20 of {dmesg_error_count} errors:")
                results.append('\n'.join(dmesg_errors[:20]))
            else:
                results.append('\n'.join(dmesg_errors))
        else:
            results.append("\n=== dmesg (No errors) ===")
        
        # Create summary
        summary = [
            f"Disk Error Log Scan for {node_name} (past {hours_back} hours):",
            f"Total errors found: {error_count}",
            f"Sources scanned: {', '.join(log_paths + ['journalctl', 'dmesg'])}"
        ]
        
        # Add recommendations based on findings
        recommendations = []
        
        if error_count > 0:
            recommendations.append("Disk errors detected - further investigation recommended")
            
            # Check for common patterns
            all_results = ' '.join(results)
            if "I/O error" in all_results:
                recommendations.append("I/O errors detected - possible hardware failure")
                
            if "timeout" in all_results:
                recommendations.append("Disk timeout errors detected - check disk connectivity")
                
            if "bad sector" in all_results or "uncorrectable" in all_results:
                recommendations.append("Bad sectors detected - backup data and consider replacement")
        
        # Add recommendations to summary if any
        if recommendations:
            summary.append("\n=== Recommendations ===")
            summary.extend(recommendations)
        
        # Combine summary and results
        return "\n".join(summary + ["\n=== Detailed Log Analysis ==="] + results)
        
    except Exception as e:
        return f"Error scanning disk error logs: {str(e)}"
</file>

<file path="tools/diagnostics/disk_performance.py">
#!/usr/bin/env python3
"""
Disk performance testing tools for volume troubleshooting.

This module contains tools for testing disk performance, including
read-only tests and I/O performance measurements.
"""

import time
import json
import re
import subprocess
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from langchain_core.tools import tool

@tool
def run_disk_readonly_test(node_name: str, device_path: str, 
                          duration_minutes: int = 10, 
                          block_size: str = "4M",
                          compare_with_healthy: bool = True) -> str:
    """
    Perform a read-only test on the disk to verify readability
    
    This tool reads disk data continuously for a configurable duration
    to verify readability and detect any read errors or timeouts.
    If compare_with_healthy is True, it will also find and test another
    disk of the same model for performance comparison.
    
    Args:
        node_name: Node hostname or IP
        device_path: Device path (e.g., /dev/sda)
        duration_minutes: Test duration in minutes
        block_size: Block size for reading (e.g., 4M, 8M)
        compare_with_healthy: Whether to compare with a healthy disk of same model
        
    Returns:
        str: Summary report with metrics and any errors encountered
    """
    try:
        from tools.diagnostics.hardware import ssh_execute
        
        # Calculate test parameters
        # duration_seconds = duration_minutes * 60
        # TODO Temporary fix for testing purposes
        duration_seconds = 1 * 60
        
        # First, get the model of the target disk
        disk_model = get_disk_model(node_name, device_path, ssh_execute)
        
        # Test the target disk
        print(f"Starting read-only test on {node_name}:{device_path} for {duration_minutes} minutes...")
        target_metrics = run_single_disk_test(node_name, device_path, duration_seconds, block_size, ssh_execute)
        
        # Initialize comparison metrics
        comparison_disk = None
        comparison_metrics = None
        
        # If comparison is requested and we found the disk model
        if compare_with_healthy and disk_model:
            # Find another disk of the same model
            comparison_disk = find_comparison_disk(node_name, device_path, disk_model, ssh_execute)
            
            if comparison_disk:
                print(f"Found comparison disk of same model: {comparison_disk}")
                print(f"Running comparison test on {comparison_disk}...")
                
                # Run the same test on the comparison disk
                comparison_metrics = run_single_disk_test(node_name, comparison_disk, duration_seconds, block_size, ssh_execute)
        
        # Generate report
        report = generate_report(node_name, device_path, target_metrics, comparison_disk, comparison_metrics)
        
        return report
        
    except Exception as e:
        return f"Error during disk read-only test: {str(e)}"

def get_disk_model(node_name: str, device_path: str, ssh_execute) -> str:
    """
    Get the model of a disk
    
    Args:
        node_name: Node hostname or IP
        device_path: Device path (e.g., /dev/sda)
        ssh_execute: Function to execute SSH commands
        
    Returns:
        str: Disk model or empty string if not found
    """
    # Try to get disk model using lsblk
    cmd = f"lsblk -o NAME,MODEL,SERIAL {device_path} -n"
    result = ssh_execute.invoke({'node_name': node_name, 'command': cmd})
    
    # Parse the output to extract the model
    model = ""
    for line in result.split('\n'):
        if line.strip():
            parts = line.strip().split()
            if len(parts) > 1:
                # The model is typically the second field
                model = parts[1]
                break
    
    # If lsblk didn't work, try smartctl
    if not model:
        cmd = f"smartctl -i {device_path} | grep 'Device Model'"
        result = ssh_execute.invoke({'node_name': node_name, 'command': cmd})
        
        # Parse the output
        match = re.search(r'Device Model:\s+(.+)', result)
        if match:
            model = match.group(1).strip()
    
    return model

def find_comparison_disk(node_name: str, target_disk: str, disk_model: str, ssh_execute) -> str:
    """
    Find another disk of the same model
    
    Args:
        node_name: Node hostname or IP
        target_disk: The disk being tested (to exclude)
        disk_model: Model to match
        ssh_execute: Function to execute SSH commands
        
    Returns:
        str: Path to comparison disk or None if not found
    """
    # List all disks
    cmd = "lsblk -d -o NAME,MODEL,SIZE -n"
    result = ssh_execute.invoke({'node_name': node_name, 'command': cmd})

    # Parse the output to find disks with the same model
    for line in result.split('\n'):
        if line.strip():
            parts = line.strip().split()
            if len(parts) >= 2:
                disk_name = parts[0]
                model = parts[1]
                
                # Check if this disk matches the model and is not the target disk
                if model == disk_model and f"/dev/{disk_name}" != target_disk:
                    return f"/dev/{disk_name}"
    
    return None

def run_single_disk_test(node_name: str, device_path: str, duration_seconds: int, block_size: str, ssh_execute) -> Dict[str, Any]:
    """
    Run a read-only test on a single disk
    
    Args:
        node_name: Node hostname or IP
        device_path: Device path (e.g., /dev/sda)
        duration_seconds: Test duration in seconds
        block_size: Block size for reading
        ssh_execute: Function to execute SSH commands
        
    Returns:
        Dict[str, Any]: Test metrics
    """
    # Start time
    start_time = datetime.now()
    
    # Command to read from disk continuously without writing
    cmd = (
        f"timeout {duration_seconds}s dd if={device_path} of=/dev/null bs={block_size} "
        f"iflag=direct status=progress 2>&1"
    )
    
    # Execute command
    result = ssh_execute.invoke({'node_name': node_name, 'command': cmd})
    
    # End time
    end_time = datetime.now()
    test_duration = (end_time - start_time).total_seconds()
    
    # Parse results to extract metrics
    read_bytes = 0
    read_speed = "0 MB/s"
    read_speed_value = 0
    
    # Look for lines like "1073741824 bytes (1.1 GB, 1.0 GiB) copied, 1.12345 s, 954 MB/s"
    for line in result.split('\n'):
        if "bytes" in line and "copied" in line:
            parts = line.split(',')
            if len(parts) >= 3:
                # Extract bytes read
                bytes_part = parts[0].strip()
                read_bytes = int(bytes_part.split()[0])
                
                # Extract speed
                speed_part = parts[2].strip()
                read_speed = speed_part
                
                # Try to extract the numeric value from the speed
                speed_match = re.search(r'(\d+\.?\d*)', speed_part)
                if speed_match:
                    read_speed_value = float(speed_match.group(1))
    
    # Check for read errors in the output
    read_errors = []
    error_keywords = ["error", "failed", "timeout", "i/o error", "cannot read"]
    for line in result.lower().split('\n'):
        if any(keyword in line for keyword in error_keywords):
            read_errors.append(line.strip())
    
    # Return metrics
    return {
        "device_path": device_path,
        "test_duration": test_duration,
        "read_bytes": read_bytes,
        "read_speed": read_speed,
        "read_speed_value": read_speed_value,
        "read_errors": read_errors
    }

def generate_report(node_name: str, device_path: str, target_metrics: Dict[str, Any], 
                   comparison_disk: str, comparison_metrics: Dict[str, Any]) -> str:
    """
    Generate a report comparing the test results
    
    Args:
        node_name: Node hostname or IP
        device_path: Target device path
        target_metrics: Metrics for the target disk
        comparison_disk: Path to comparison disk
        comparison_metrics: Metrics for the comparison disk
        
    Returns:
        str: Formatted report
    """
    # Generate summary report
    report = [
        f"Disk Read-Only Test Report for {node_name}:{device_path}",
        f"=" * 70,
        f"Test Duration: {target_metrics['test_duration']:.2f} seconds",
        f"Total Data Read: {target_metrics['read_bytes']} bytes ({target_metrics['read_bytes'] / (1024**3):.2f} GB)",
        f"Average Read Speed: {target_metrics['read_speed']}",
        f"Read Errors Detected: {len(target_metrics['read_errors'])}"
    ]
    
    # Add error details if any
    if target_metrics['read_errors']:
        report.append("\nError Details:")
        for i, error in enumerate(target_metrics['read_errors'], 1):
            report.append(f"{i}. {error}")
    else:
        report.append("\nNo read errors detected during the test.")
    
    # Add comparison results if available
    if comparison_disk and comparison_metrics:
        report.append("\nComparison with Healthy Disk:")
        report.append("-" * 50)
        report.append(f"Comparison Disk: {comparison_disk}")
        report.append(f"Comparison Disk Read Speed: {comparison_metrics['read_speed']}")
        
        # Calculate speed difference
        if target_metrics['read_speed_value'] > 0 and comparison_metrics['read_speed_value'] > 0:
            speed_ratio = target_metrics['read_speed_value'] / comparison_metrics['read_speed_value']
            percentage_diff = abs(1 - speed_ratio) * 100
            
            if speed_ratio < 0.8:  # Target disk is significantly slower
                report.append(f"\nWARNING: Target disk is {percentage_diff:.1f}% slower than the comparison disk of the same model.")
                report.append(f"Target: {target_metrics['read_speed']} vs Comparison: {comparison_metrics['read_speed']}")
                report.append("This significant performance difference may indicate hardware degradation.")
            elif speed_ratio > 1.2:  # Target disk is significantly faster
                report.append(f"\nNote: Target disk is {percentage_diff:.1f}% faster than the comparison disk of the same model.")
                report.append(f"Target: {target_metrics['read_speed']} vs Comparison: {comparison_metrics['read_speed']}")
            else:
                report.append(f"\nPerformance is within normal range compared to the same model disk.")
                report.append(f"Difference: {percentage_diff:.1f}%")
    
    # Add test result summary
    if len(target_metrics['read_errors']) == 0:
        if comparison_metrics and target_metrics['read_speed_value'] < 0.7 * comparison_metrics['read_speed_value']:
            report.append("\nTest Result: WARNING - Disk is readable but performance is significantly lower than expected")
        else:
            report.append("\nTest Result: PASSED - Disk is readable without errors")
    else:
        report.append("\nTest Result: FAILED - Disk has read errors")
    
    return "\n".join(report)

@tool
def test_disk_io_performance(node_name: str, device_path: str, 
                            test_types: List[str] = ["read", "write", "randread", "randwrite"],
                            duration_seconds: int = 30,
                            block_sizes: List[str] = ["4k", "128k", "1m"]) -> str:
    """
    Measure disk I/O performance under different workloads
    
    This tool tests disk I/O performance, including read/write speeds and IOPS,
    under different workloads (sequential and random access).
    
    Args:
        node_name: Node hostname or IP
        device_path: Device path (e.g., /dev/sda)
        test_types: List of test types to run (read, write, randread, randwrite)
        duration_seconds: Duration for each test in seconds
        block_sizes: List of block sizes to test with
        
    Returns:
        str: Performance test results showing IOPS and throughput
    """
    try:
        from tools.diagnostics.hardware import ssh_execute
        
        results = []
        results.append(f"Disk I/O Performance Test for {node_name}:{device_path}")
        results.append("=" * 70)
        
        # Ensure test_types contains valid values
        valid_test_types = ["read", "randread"]
        test_types = [t for t in test_types if t in valid_test_types]
        
        # If no valid test types, use defaults
        if not test_types:
            test_types = ["read", "randread"]
        
        # Run tests for each combination of test type and block size
        for test_type in test_types:
            results.append(f"\n{test_type.upper()} Tests:")
            results.append("-" * 50)
            
            for block_size in block_sizes:
                results.append(f"\nBlock Size: {block_size}")
                
                # Build fio command
                cmd = (
                    f"sudo fio --name=test --filename={device_path} --direct=1 "
                    f"--rw={test_type} --bs={block_size} --ioengine=libaio "
                    f"--iodepth=16 --runtime={duration_seconds} --numjobs=4 "
                    f"--time_based --group_reporting --size=1G "
                    f"--output-format=json"
                )
                
                # Execute command
                print(f"Running {test_type} test with {block_size} block size...")
                print(f"Command: {cmd}")
                output = ssh_execute.invoke({"node_name": node_name, "command": cmd})
                
                # Parse JSON output if available
                try:
                    # Extract JSON part from output (may have other text before/after)
                    json_start = output.find('{')
                    json_end = output.rfind('}') + 1
                    
                    if json_start >= 0 and json_end > json_start:
                        json_data = json.loads(output[json_start:json_end])
                        
                        # Extract key metrics
                        job_data = json_data.get("jobs", [{}])[0]
                        
                        # Read metrics
                        read_iops = job_data.get("read", {}).get("iops", 0)
                        read_bw = job_data.get("read", {}).get("bw", 0)  # KiB/s
                        read_bw_mb = read_bw / 1024  # Convert to MiB/s
                        
                        # Write metrics
                        write_iops = job_data.get("write", {}).get("iops", 0)
                        write_bw = job_data.get("write", {}).get("bw", 0)  # KiB/s
                        write_bw_mb = write_bw / 1024  # Convert to MiB/s
                        
                        # Latency metrics (in nanoseconds)
                        lat_ns = job_data.get(test_type, {}).get("lat_ns", {})
                        avg_lat_us = lat_ns.get("mean", 0) / 1000  # Convert to microseconds
                        max_lat_us = lat_ns.get("max", 0) / 1000  # Convert to microseconds
                        
                        # Add metrics to results
                        if test_type.startswith("read"):
                            results.append(f"  Read IOPS: {read_iops:.2f}")
                            results.append(f"  Read Bandwidth: {read_bw_mb:.2f} MiB/s")
                            results.append(f"  Avg Latency: {avg_lat_us:.2f} s")
                            results.append(f"  Max Latency: {max_lat_us:.2f} s")
                        else:
                            results.append(f"  Write IOPS: {write_iops:.2f}")
                            results.append(f"  Write Bandwidth: {write_bw_mb:.2f} MiB/s")
                            results.append(f"  Avg Latency: {avg_lat_us:.2f} s")
                            results.append(f"  Max Latency: {max_lat_us:.2f} s")
                    else:
                        # If JSON parsing fails, include raw output
                        results.append("  Failed to parse JSON output")
                        results.append(f"  Raw output: {output[:200]}...")
                except Exception as e:
                    results.append(f"  Error parsing results: {str(e)}")
                    results.append(f"  Raw output: {output[:200]}...")
        
        # Add summary
        results.append("\nPerformance Test Summary:")
        results.append("=" * 50)
        results.append("Tests completed successfully. Review the metrics above to evaluate disk performance.")
        results.append("Higher IOPS and bandwidth values indicate better performance.")
        results.append("Lower latency values indicate better responsiveness.")
        
        return "\n".join(results)
        
    except Exception as e:
        return f"Error during disk I/O performance test: {str(e)}"
</file>

<file path="tools/diagnostics/system.py">
#!/usr/bin/env python3
"""
System diagnostic tools for volume troubleshooting.

This module contains tools for system-level diagnostics including
disk space, mount points, kernel messages, and system logs.
"""

import subprocess
import json
from langchain_core.tools import tool
from tools.diagnostics.hardware import ssh_execute

@tool
def df_command(node_name: str, path: str = None, options: str = "-h") -> str:
    """
    Execute df command to show disk space usage
    
    Args:
        node_name: Node hostname or IP
        path: Path to check (optional)
        options: Command options (optional)
        
    Returns:
        str: Command output
    """
    cmd = ["df"]
    
    if options:
        cmd.extend(options.split())
    
    if path:
        cmd.append(path)
    
    # Build command string
    cmd_str = " ".join(cmd)
    
    # Execute command via SSH
    try:
        return ssh_execute.invoke({"node_name": node_name, "command": cmd_str})
    except Exception as e:
        return f"Error executing df: {str(e)}"

@tool
def lsblk_command(node_name: str, options: str = "") -> str:
    """
    Execute lsblk command to list block devices
    
    Args:
        node_name: Node hostname or IP
        options: Command options (optional)
        
    Returns:
        str: Command output
    """
    cmd = ["lsblk"]
    
    if options:
        cmd.extend(options.split())
    
    # Build command string
    cmd_str = " ".join(cmd)
    
    # Execute command via SSH
    try:
        return ssh_execute.invoke({"node_name": node_name, "command": cmd_str})
    except Exception as e:
        return f"Error executing lsblk: {str(e)}"

@tool
def mount_command(node_name: str, options: str = "") -> str:
    """
    Execute mount command to show mounted filesystems
    
    Args:
        node_name: Node hostname or IP
        options: Command options (optional)
        
    Returns:
        str: Command output
    """
    cmd = ["mount"]
    
    if options:
        cmd.extend(options.split())
    
    # Build command string
    cmd_str = " ".join(cmd)
    
    # Execute command via SSH
    try:
        return ssh_execute.invoke({"node_name": node_name, "command": cmd_str})
    except Exception as e:
        return f"Error executing mount: {str(e)}"

@tool
def dmesg_command(node_name: str, options: str = "--since='1 hours ago'") -> str:
    """
    Execute dmesg command to show kernel messages
    
    Args:
        node_name: Node hostname or IP
        options: Command options (default: show logs from last 5 minutes with timestamps)
        
    Returns:
        str: Command output
    """
    cmd = ["dmesg"]
    if "--since" not in options:
        cmd = ["dmesg", "--since='1 hours ago'", "-T"]
    
    if options:
        cmd.extend(options.split())
    
    # Build command string
    cmd_str = " ".join(cmd)
    
    # Execute command via SSH
    try:
        return ssh_execute.invoke({"node_name": node_name, "command": cmd_str})
    except Exception as e:
        return f"Error executing dmesg: {str(e)}"

@tool
def journalctl_command(node_name: str, options: str = "--since='1 hours ago'") -> str:
    """
    Execute journalctl command to show systemd journal logs from the last 5 minutes
    
    Args:
        node_name: Node hostname or IP
        options: Command options (default: show logs from last 5 minutes)
        
    Returns:
        str: Command output
    """
    cmd = ["journalctl"]
    if "--since" not in options:
        cmd = ["journalctl", "--since='1 hours ago'"]
    
    if options:
        cmd.extend(options.split())
    
    # Build command string
    cmd_str = " ".join(cmd)
    
    # Execute command via SSH
    try:
        return ssh_execute.invoke({"node_name": node_name, "command": cmd_str})
    except Exception as e:
        return f"Error executing journalctl: {str(e)}"

@tool
def get_system_hardware_info(node_name: str) -> str:
    """
    Get system manufacturer and product name information using dmidecode
    
    Args:
        node_name: Node hostname or IP
        
    Returns:
        str: System hardware information
    """
    try:
        # Execute dmidecode commands via SSH
        manufacturer_cmd = "dmidecode -s system-manufacturer"
        product_name_cmd = "dmidecode -s system-product-name"
        
        manufacturer = ssh_execute.invoke({"node_name": node_name, "command": manufacturer_cmd})
        product_name = ssh_execute.invoke({"node_name": node_name, "command": product_name_cmd})
        
        # Clean up the output
        manufacturer = manufacturer.strip() if isinstance(manufacturer, str) else "Unknown"
        product_name = product_name.strip() if isinstance(product_name, str) else "Unknown"
        
        # Format the result
        result = {
            "manufacturer": manufacturer,
            "product_name": product_name
        }
        
        return json.dumps(result, indent=2)
        
    except Exception as e:
        return f"Error getting system hardware info: {str(e)}"
</file>

<file path="phases/phase_plan_phase.py">
#!/usr/bin/env python3
"""
Plan Phase for Kubernetes Volume Troubleshooting

This module contains the PlanPhase class that orchestrates the planning phase
of the troubleshooting process, generating an Investigation Plan for Phase 1.
"""

import logging
import os
import json
from typing import Dict, List, Any, Optional, Tuple
from knowledge_graph import KnowledgeGraph
from phases.investigation_planner import InvestigationPlanner
from phases.utils import validate_knowledge_graph, generate_basic_fallback_plan, handle_exception

logger = logging.getLogger(__name__)

class PlanPhase:
    """
    Orchestrates the Plan Phase of the troubleshooting process
    
    The Plan Phase can use one of two approaches to generate an Investigation Plan:
    1. Traditional approach (default):
       - Rule-based preliminary steps - Generate critical initial investigation steps
       - Static plan steps integration - Add mandatory steps from static_plan_step.json
       - LLM refinement - Refine and supplement the plan using LLM without tool invocation
    
    2. ReAct graph approach (when use_react=True):
       - Implements a ReAct (Reasoning and Acting) graph using LangGraph
       - Uses the Strategy Pattern with PlanLLMGraph implementing LangGraphInterface
       - Exclusively uses MCP tools for function calling to gather information
       - Follows the standard ReAct pattern: reasoning, acting, observing in a loop
    
    This phase uses the Knowledge Graph from Phase 0, historical experience data, and
    the complete Phase1 tool registry to produce a comprehensive Investigation Plan.
    """
    
    def __init__(self, config_data: Dict[str, Any] = None):
        """
        Initialize the Plan Phase
        
        Args:
            config_data: Configuration data for the system (optional)
        """
        self.config_data = config_data or {}
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.investigation_planner = None
        
        # Check if ReAct graph should be used
        self.use_react = self.config_data.get('plan_phase', {}).get('use_react', False)
        self.logger.info(f"Plan Phase initialized with {'ReAct' if self.use_react else 'traditional'} approach")
    
    async def execute(self, knowledge_graph: KnowledgeGraph, pod_name: str, namespace: str, 
                    volume_path: str, message_list: List[Dict[str, str]] = None) -> Dict[str, Any]:
        """
        Execute the Plan Phase
        
        Args:
            knowledge_graph: KnowledgeGraph instance from Phase 0
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod
            volume_path: Path of the volume with I/O error
            message_list: Optional message list for chat mode
            
        Returns:
            Dict[str, Any]: Results of the Plan Phase, including the Investigation Plan and updated message list
        """
        self.logger.info(f"Executing Plan Phase for {namespace}/{pod_name} volume {volume_path}")
        
        try:
            # Generate investigation plan using the Investigation Planner
            # The use_react flag is passed to the planner to determine the approach
            return await self._generate_investigation_plan(
                knowledge_graph, pod_name, namespace, volume_path, message_list, self.use_react
            )
            
        except Exception as e:
            error_msg = handle_exception("execute", e, self.logger)
            return self._handle_plan_generation_error(
                error_msg, pod_name, namespace, volume_path, message_list
            )
    
    async def _generate_investigation_plan(self, knowledge_graph: KnowledgeGraph, pod_name: str, namespace: str, 
                                        volume_path: str, message_list: List[Dict[str, str]] = None,
                                        use_react: bool = False) -> Dict[str, Any]:
        """
        Generate an investigation plan using the Investigation Planner (Legacy mode)
        
        Args:
            knowledge_graph: KnowledgeGraph instance from Phase 0
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod
            volume_path: Path of the volume with I/O error
            message_list: Optional message list for chat mode
            use_react: Whether to use React mode (default: False)
            
        Returns:
            Dict[str, Any]: Results of the plan generation
        """
        # Initialize Investigation Planner
        self.investigation_planner = InvestigationPlanner(knowledge_graph, self.config_data)
        
        # Generate Investigation Plan with use_react flag
        investigation_plan, message_list = await self.investigation_planner.generate_investigation_plan(
            pod_name, namespace, volume_path, message_list, use_react
        )
        
        # Parse the plan into a structured format for Phase 1
        structured_plan = self._parse_investigation_plan(investigation_plan)
        
        # Return results
        return {
            "status": "success",
            "investigation_plan": investigation_plan,
            "structured_plan": structured_plan,
            "pod_name": pod_name,
            "namespace": namespace,
            "volume_path": volume_path,
            "message_list": message_list
        }
        
    # React mode is now handled by the PlanLLMGraph class in llm_graph/graphs/plan_llm_graph.py
    # and is integrated into the LLMPlanGenerator._call_llm_and_process_response() method
    
    def _handle_plan_generation_error(self, error_msg: str, pod_name: str, namespace: str, 
                                    volume_path: str, message_list: List[Dict[str, str]] = None) -> Dict[str, Any]:
        """
        Handle errors during plan generation
        
        Args:
            error_msg: Error message
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod
            volume_path: Path of the volume with I/O error
            message_list: Optional message list for chat mode
            
        Returns:
            Dict[str, Any]: Error results with fallback plan
        """
        # Generate fallback plan
        fallback_plan = generate_basic_fallback_plan(pod_name, namespace, volume_path)
        
        # Add fallback plan to message list if provided
        updated_message_list = self._update_message_list(message_list, fallback_plan)
        
        return {
            "status": "error",
            "error_message": error_msg,
            "investigation_plan": fallback_plan,
            "pod_name": pod_name,
            "namespace": namespace,
            "volume_path": volume_path,
            "message_list": updated_message_list
        }
    
    def _update_message_list(self, message_list: List[Dict[str, str]], content: str) -> List[Dict[str, str]]:
        """
        Update message list with new content
        
        Args:
            message_list: Message list to update
            content: Content to add to the message list
            
        Returns:
            List[Dict[str, str]]: Updated message list
        """
        if message_list is None:
            return None
            
        # If the last message is from the user, append the assistant response
        if message_list[-1]["role"] == "user":
            message_list.append({"role": "assistant", "content": content})
        else:
            # Replace the last message if it's from the assistant
            message_list[-1] = {"role": "assistant", "content": content}
        
        return message_list
    
    def _parse_investigation_plan(self, investigation_plan: str) -> Dict[str, Any]:
        """
        Parse the Investigation Plan into a structured format for Phase 1
        
        Args:
            investigation_plan: Formatted Investigation Plan
            
        Returns:
            Dict[str, Any]: Structured Investigation Plan
        """
        try:
            # Initialize structured plan
            structured_plan = {
                "steps": [],
                "fallback_steps": []
            }
            
            # Parse the plan
            lines = investigation_plan.strip().split('\n')
            in_fallback_section = False
            
            for line in lines:
                line = line.strip()
                
                # Skip empty lines and headers
                if self._is_header_line(line):
                    continue
                
                # Check if we're in the fallback section
                if line == "Fallback Steps (if main steps fail):":
                    in_fallback_section = True
                    continue
                
                # Parse step
                if line.startswith("Step "):
                    step = self._parse_step_line(line, in_fallback_section)
                    if step:
                        # Add to appropriate list
                        if in_fallback_section:
                            structured_plan["fallback_steps"].append(step)
                        else:
                            structured_plan["steps"].append(step)
            
            return structured_plan
            
        except Exception as e:
            error_msg = handle_exception("_parse_investigation_plan", e, self.logger)
            return {"steps": [], "fallback_steps": []}
    
    def _is_header_line(self, line: str) -> bool:
        """
        Check if a line is a header line that should be skipped
        
        Args:
            line: Line to check
            
        Returns:
            bool: True if the line is a header, False otherwise
        """
        return (not line or 
                line.startswith("Investigation Plan:") or 
                line.startswith("Target:") or 
                line.startswith("Generated Steps:"))
    
    def _parse_step_line(self, line: str, in_fallback_section: bool) -> Dict[str, Any]:
        """
        Parse a step line into a structured step
        
        Args:
            line: Step line to parse
            in_fallback_section: Whether we're in the fallback section
            
        Returns:
            Dict[str, Any]: Parsed step, or None if parsing failed
        """
        step_parts = line.split(" | ")
        
        if len(step_parts) < 3:
            return None
        
        # Extract step number and description
        step_info = step_parts[0].split(": ", 1)
        step_number = step_info[0].replace("Step ", "")
        description = step_info[1] if len(step_info) > 1 else ""
        
        # Extract tool and arguments
        tool_info = step_parts[1].replace("Tool: ", "")
        tool_name = tool_info.split("(")[0] if "(" in tool_info else tool_info
        
        # Extract arguments
        arguments = self._parse_tool_arguments(tool_info)
        
        # Extract expected outcome
        expected = step_parts[2].replace("Expected: ", "") if len(step_parts) > 2 else ""
        
        # Extract trigger for fallback steps
        trigger = step_parts[3].replace("Trigger: ", "") if len(step_parts) > 3 and in_fallback_section else None
        
        # Create step dictionary
        step = {
            "step": step_number,
            "description": description,
            "tool": tool_name,
            "arguments": arguments,
            "expected": expected
        }
        
        if trigger:
            step["trigger"] = trigger
        
        return step
    
    def _parse_tool_arguments(self, tool_info: str) -> Dict[str, Any]:
        """
        Parse tool arguments from tool info string
        
        Args:
            tool_info: Tool info string
            
        Returns:
            Dict[str, Any]: Parsed arguments
        """
        arguments = {}
        if "(" in tool_info and ")" in tool_info:
            args_str = tool_info.split("(", 1)[1].rsplit(")", 1)[0]
            if args_str:
                # Parse arguments
                for arg in args_str.split(", "):
                    if "=" in arg:
                        key, value = arg.split("=", 1)
                        # Convert string representations to actual values
                        arguments[key] = self._convert_argument_value(value)
        
        return arguments
    
    def _convert_argument_value(self, value: str) -> Any:
        """
        Convert a string argument value to its appropriate type
        
        Args:
            value: String value to convert
            
        Returns:
            Any: Converted value
        """
        if value.lower() == "true":
            return True
        elif value.lower() == "false":
            return False
        elif value.isdigit():
            return int(value)
        elif value.startswith("'") and value.endswith("'"):
            return value[1:-1]
        elif value.startswith('"') and value.endswith('"'):
            return value[1:-1]
        return value
    


async def run_plan_phase(pod_name, namespace, volume_path, collected_info, config_data=None, message_list=None):
    """
    Run the Plan Phase
    
    Args:
        pod_name: Name of the pod with the error
        namespace: Namespace of the pod
        volume_path: Path of the volume with I/O error
        collected_info: Dictionary containing collected information from Phase 0, including knowledge_graph
        config_data: Configuration data for the system (optional)
        message_list: Optional message list for chat mode
        
    Returns:
        Tuple[str, List[Dict[str, str]]]: (Investigation Plan as a formatted string, Updated message list)
    """
    logger = logging.getLogger(__name__)
    logger.info(f"Running Plan Phase for {namespace}/{pod_name} volume {volume_path}")
    
    try:
        # Extract and validate knowledge_graph
        knowledge_graph = _extract_and_validate_knowledge_graph(collected_info, logger)
        
        # Initialize and execute Plan Phase
        plan_phase = PlanPhase(config_data)
        
        # Execute the plan phase
        # The React/Legacy mode distinction is handled in the PlanPhase.execute method
        use_react = config_data.get('plan_phase', {}).get('use_react', False)
        logger.info(f"Using {'React' if use_react else 'traditional'} approach for plan generation")
        results = await plan_phase.execute(knowledge_graph, pod_name, namespace, volume_path, message_list)
        
        # Log the results
        logger.info(f"Plan Phase completed with status: {results['status']}")
        
        # Save the investigation plan to a file if configured
        _save_plan_to_file_if_configured(
            results['investigation_plan'], namespace, pod_name, config_data, logger
        )
        
        # Return the investigation plan as a string and the updated message list
        return results['investigation_plan'], results['message_list']
        
    except Exception as e:
        error_msg = handle_exception("run_plan_phase", e, logger)
        return error_msg, message_list


def _extract_and_validate_knowledge_graph(collected_info: Dict[str, Any], logger: logging.Logger) -> KnowledgeGraph:
    """
    Extract and validate the knowledge graph from collected information
    
    Args:
        collected_info: Dictionary containing collected information from Phase 0
        logger: Logger instance
        
    Returns:
        KnowledgeGraph: Validated knowledge graph
        
    Raises:
        ValueError: If the knowledge graph is invalid
    """
    # Extract knowledge_graph from collected_info
    knowledge_graph = collected_info.get('knowledge_graph')
    
    # Validate knowledge_graph is present
    if knowledge_graph is None:
        error_msg = "Knowledge Graph not found in collected_info"
        logger.error(error_msg)
        raise ValueError(error_msg)
    
    # Validate knowledge_graph is a KnowledgeGraph instance
    if not isinstance(knowledge_graph, KnowledgeGraph):
        error_msg = f"Invalid Knowledge Graph type: {type(knowledge_graph)}"
        logger.error(error_msg)
        raise ValueError(error_msg)
    
    return knowledge_graph


def _save_plan_to_file_if_configured(investigation_plan: str, namespace: str, pod_name: str, 
                                   config_data: Dict[str, Any], logger: logging.Logger) -> None:
    """
    Save the investigation plan to a file if configured
    
    Args:
        investigation_plan: Investigation plan to save
        namespace: Namespace of the pod
        pod_name: Name of the pod
        config_data: Configuration data
        logger: Logger instance
    """
    if not config_data or not config_data.get('plan_phase', {}).get('save_plan', False):
        return
        
    try:
        output_dir = config_data.get('output_dir', 'output')
        os.makedirs(output_dir, exist_ok=True)
        
        plan_file = os.path.join(output_dir, f"investigation_plan_{namespace}_{pod_name}.txt")
        with open(plan_file, 'w') as f:
            f.write(investigation_plan)
        
        logger.info(f"Investigation Plan saved to {plan_file}")
    except Exception as e:
        logger.warning(f"Failed to save investigation plan to file: {str(e)}")
</file>

<file path="tools/core/knowledge_graph.py">
#!/usr/bin/env python3
"""
Knowledge Graph tools for Kubernetes volume troubleshooting.

This module contains tools for interacting with the Knowledge Graph,
including entity queries, relationship analysis, and issue management.
"""

import json
import logging
from typing import Any
from langchain_core.tools import tool

# Configure logger for knowledge graph tools
kg_tools_logger = logging.getLogger('knowledge_graph.tools')
kg_tools_logger.setLevel(logging.INFO)
# Don't propagate to root logger to avoid console output
kg_tools_logger.propagate = False

# Import Knowledge Graph
from knowledge_graph import KnowledgeGraph

# Global Knowledge Graph instance
KNOWLEDGE_GRAPH = None

def initialize_knowledge_graph(kg_instance: 'KnowledgeGraph') -> 'KnowledgeGraph':
    """
    Initialize or set the global Knowledge Graph instance
    
    Args:
        kg_instance: Existing KnowledgeGraph instance from Phase0 (required)
        
    Returns:
        KnowledgeGraph: Global KnowledgeGraph instance
    """
    global KNOWLEDGE_GRAPH
    
    if kg_instance:
        KNOWLEDGE_GRAPH = kg_instance
        kg_tools_logger.info("Using Knowledge Graph instance from Phase0")
    else:
        error_msg = "Knowledge Graph instance must be provided from Phase0 (information collection)"
        kg_tools_logger.error(error_msg)
        raise ValueError(error_msg)
    
    return KNOWLEDGE_GRAPH

def get_knowledge_graph() -> 'KnowledgeGraph':
    """
    Get the global Knowledge Graph instance
    
    Returns:
        KnowledgeGraph: Global KnowledgeGraph instance from Phase0
        
    Raises:
        ValueError: If Knowledge Graph has not been initialized with a valid instance from Phase0
    """
    global KNOWLEDGE_GRAPH
    
    if KNOWLEDGE_GRAPH is None:
        error_msg = "Knowledge Graph has not been initialized. Use initialize_knowledge_graph() with the Knowledge Graph instance from Phase0 first."
        kg_tools_logger.error(error_msg)
        raise ValueError(error_msg)
    
    return KNOWLEDGE_GRAPH

@tool
def kg_get_entity_info(entity_type: str, id: str) -> str:
    """
    Get detailed information about an entity in the Knowledge Graph
    
    Args:
        entity_type: Type of entity (Pod, PVC, PV, Drive, Node, etc.)
        id: ID or name of the entity. Can be provided in two formats:
           Examples: "gnode:Pod:default/nginx-pod", "gnode:PV:pv-00001", "gnode:Drive:drive-sda"
           
           Entity ID formats:
           - Pod: "gnode:Pod:<namespace>/<name>" (example: "gnode:Pod:default/test-pod-1-0")
           - PVC: "gnode:PVC:<namespace>/<name>" (example: "gnode:PVC:default/test-pvc-1")
           - PV: "gnode:PV:<name>" (example: "gnode:PV:pv-test-123")
           - Drive: "gnode:Drive:<uuid>" (example: "gnode:Drive:a1b2c3d4-e5f6")
           - Node: "gnode:Node:<name>" (example: "gnode:Node:kind-control-plane")
           - StorageClass: "gnode:StorageClass:<name>" (example: "gnode:StorageClass:csi-baremetal-sc")
           - LVG: "gnode:LVG:<name>" (example: "gnode:LVG:lvg-1")
           - AC: "gnode:AC:<name>" (example: "gnode:AC:ac-node1-ssd")
           - Volume: "gnode:Volume:<namespace>/<name>" (example: "gnode:Volume:default/vol-1")
           - System: "gnode:System:<entity_name>" (example: "gnode:System:kernel")
           - ClusterNode: "gnode:ClusterNode:<name>" (example: "gnode:ClusterNode:worker-1")
           - HistoricalExperience: "gnode:HistoricalExperience:<experience_id>" (example: "gnode:HistoricalExperience:exp-001")
        
    Returns:
        str: JSON serialized entity details with attributes and relationships
    """
    kg = get_knowledge_graph()
    
    # Construct the full node_id if only name was provided
    if ':' not in id:
        node_id = f"gnode:{entity_type}:{id}"
    else:
        node_id = id
    
    # Check if node exists in graph
    if not kg.graph.has_node(node_id):
        # Try to find by name or uuid attribute
        found = False
        for n_id, attrs in kg.graph.nodes(data=True):
            if (attrs.get('entity_type') == entity_type and 
                (attrs.get('name') == id or attrs.get('uuid') == id)):
                node_id = n_id
                found = True
                break
        
        if not found:
            return json.dumps({"error": f"Entity not found: gnode:{entity_type}:{id}"})
    
    # Get node attributes
    node_attrs = dict(kg.graph.nodes[node_id])
    
    # Get incoming and outgoing relationships
    incoming_edges = []
    outgoing_edges = []
    
    for u, v, data in kg.graph.in_edges(node_id, data=True):
        source_type = kg.graph.nodes[u].get('entity_type', 'Unknown')
        source_name = kg.graph.nodes[u].get('name', u.split(':')[-1])
        incoming_edges.append({
            "source_id": u,
            "source_type": source_type,
            "source_name": source_name,
            "relationship": data.get('relationship', 'connected_to'),
            "attributes": {k: v for k, v in data.items() if k != 'relationship'}
        })
    
    for u, v, data in kg.graph.out_edges(node_id, data=True):
        target_type = kg.graph.nodes[v].get('entity_type', 'Unknown')
        target_name = kg.graph.nodes[v].get('name', v.split(':')[-1])
        outgoing_edges.append({
            "target_id": v,
            "target_type": target_type,
            "target_name": target_name,
            "relationship": data.get('relationship', 'connected_to'),
            "attributes": {k: v for k, v in data.items() if k != 'relationship'}
        })
    
    # Compile result
    result = {
        "node_id": node_id,
        "entity_type": entity_type,
        "attributes": node_attrs,
        "incoming_relationships": incoming_edges,
        "outgoing_relationships": outgoing_edges,
        "issues": node_attrs.get('issues', [])
    }
    
    return json.dumps(result, indent=2)

@tool
def kg_get_related_entities(entity_type: str, id: str, relationship_type: str = None, max_depth: int = 1) -> str:
    """
    Get entities related to a target entity in the Knowledge Graph
    
    Args:
        entity_type: Type of entity (Pod, PVC, PV, Drive, Node, etc.)
        id: ID or name of the entity. Can be provided in two formats:
           Examples: "gnode:Pod:default/nginx-pod", "gnode:PV:pv-00001", "gnode:Drive:drive-sda"
           
           Entity ID formats:
           - Pod: "gnode:Pod:<namespace>/<name>" (example: "gnode:Pod:default/test-pod-1-0")
           - PVC: "gnode:PVC:<namespace>/<name>" (example: "gnode:PVC:default/test-pvc-1")
           - PV: "gnode:PV:<name>" (example: "gnode:PV:pv-test-123")
           - Drive: "gnode:Drive:<uuid>" (example: "gnode:Drive:a1b2c3d4-e5f6")
           - Node: "gnode:Node:<name>" (example: "gnode:Node:kind-control-plane")
           - StorageClass: "gnode:StorageClass:<name>" (example: "gnode:StorageClass:csi-baremetal-sc")
           - LVG: "gnode:LVG:<name>" (example: "gnode:LVG:lvg-1")
           - AC: "gnode:AC:<name>" (example: "gnode:AC:ac-node1-ssd")
           - Volume: "gnode:Volume:<namespace>/<name>" (example: "gnode:Volume:default/vol-1")
           - System: "gnode:System:<entity_name>" (example: "gnode:System:kernel")
           - ClusterNode: "gnode:ClusterNode:<name>" (example: "gnode:ClusterNode:worker-1")
           - HistoricalExperience: "gnode:HistoricalExperience:<experience_id>" (example: "gnode:HistoricalExperience:exp-001")
        relationship_type: Optional relationship type to filter by (e.g., "uses", "bound_to", "runs_on")
        max_depth: Maximum traversal depth (1 = direct relationships only)
        
    Returns:
        str: JSON serialized list of related entities
    """
    kg = get_knowledge_graph()
    
    # Construct the full node_id if only name was provided
    if ':' not in id:
        node_id = f"gnode:{entity_type}:{id}"
    else:
        node_id = id
    
    # Check if node exists in graph
    if not kg.graph.has_node(node_id):
        # Try to find by name or uuid attribute
        found = False
        for n_id, attrs in kg.graph.nodes(data=True):
            if (attrs.get('entity_type') == entity_type and 
                (attrs.get('name') == id or attrs.get('uuid') == id)):
                node_id = n_id
                found = True
                break
        
        if not found:
            return json.dumps({"error": f"Entity not found: gnode:{entity_type}:{id}"})
    
    # Find related entities recursively up to max_depth
    related_entities = []
    visited = set([node_id])
    
    def explore_neighbors(current_node, current_depth):
        if current_depth > max_depth:
            return
        
        # Outgoing edges
        for _, target, edge_data in kg.graph.out_edges(current_node, data=True):
            if target in visited:
                continue
                
            edge_rel_type = edge_data.get('relationship', 'connected_to')
            if relationship_type is None or edge_rel_type == relationship_type:
                target_attrs = dict(kg.graph.nodes[target])
                entity = {
                    "node_id": target,
                    "entity_type": target_attrs.get('entity_type', 'Unknown'),
                    "name": target_attrs.get('name', target.split(':')[-1]),
                    "relationship": {
                        "type": edge_rel_type,
                        "direction": "outgoing",
                        "from": current_node
                    },
                    "attributes": {k: v for k, v in target_attrs.items() 
                                if k not in ['entity_type', 'name', 'issues']},
                    "issues": target_attrs.get('issues', [])
                }
                related_entities.append(entity)
                visited.add(target)
                
                # Recursive exploration
                if current_depth < max_depth:
                    explore_neighbors(target, current_depth + 1)
        
        # Incoming edges
        for source, _, edge_data in kg.graph.in_edges(current_node, data=True):
            if source in visited:
                continue
                
            edge_rel_type = edge_data.get('relationship', 'connected_to')
            if relationship_type is None or edge_rel_type == relationship_type:
                source_attrs = dict(kg.graph.nodes[source])
                entity = {
                    "node_id": source,
                    "entity_type": source_attrs.get('entity_type', 'Unknown'),
                    "name": source_attrs.get('name', source.split(':')[-1]),
                    "relationship": {
                        "type": edge_rel_type,
                        "direction": "incoming",
                        "to": current_node
                    },
                    "attributes": {k: v for k, v in source_attrs.items() 
                                if k not in ['entity_type', 'name', 'issues']},
                    "issues": source_attrs.get('issues', [])
                }
                related_entities.append(entity)
                visited.add(source)
                
                # Recursive exploration
                if current_depth < max_depth:
                    explore_neighbors(source, current_depth + 1)
    
    # Start exploration from the target node
    explore_neighbors(node_id, 1)
    
    return json.dumps({
        "source_entity": {
            "node_id": node_id,
            "entity_type": entity_type,
            "name": kg.graph.nodes[node_id].get('name', node_id.split(':')[-1])
        },
        "relationship_filter": relationship_type,
        "max_depth": max_depth,
        "related_entities": related_entities,
        "total_count": len(related_entities)
    }, indent=2)

@tool
def kg_get_all_issues(severity: str = None, issue_type: str = None) -> str:
    """
    Get all issues in the Knowledge Graph with optional filtering
    
    Args:
        severity: Optional severity filter (primary, critical, high, medium, low)
        issue_type: Optional issue type filter
        
    Returns:
        str: JSON serialized list of issues with entity information
    """
    kg = get_knowledge_graph()
    
    # Get all issues based on filters
    if severity and issue_type:
        issues = [issue for issue in kg.issues 
                 if issue['severity'] == severity and issue['type'] == issue_type]
    elif severity == 'primary':
        critical_issues = kg.get_issues_by_severity("critical")
        high_issues = kg.get_issues_by_severity("high")
        issues = critical_issues + high_issues
    elif severity:
        issues = kg.get_issues_by_severity(severity)
    elif issue_type:
        issues = [issue for issue in kg.issues if issue['type'] == issue_type]
    else:
        issues = kg.get_all_issues()
    
    # Enhance issues with entity information
    enhanced_issues = []
    for issue in issues:
        node_id = issue['node_id']
        entity_info = {}
        
        if kg.graph.has_node(node_id):
            node_attrs = kg.graph.nodes[node_id]
            entity_type = node_attrs.get('entity_type', 'Unknown')
            entity_name = node_attrs.get('name', node_id.split(':')[-1])
            entity_info = {
                "entity_type": entity_type,
                "name": entity_name,
                "attributes": {k: v for k, v in node_attrs.items() 
                             if k not in ['entity_type', 'name', 'issues']}
            }
        
        enhanced_issues.append({
            "issue": issue,
            "entity": entity_info
        })
    
    result = {
        "total_issues": len(enhanced_issues),
        "severity_filter": severity,
        "type_filter": issue_type,
        "issues": enhanced_issues
    }
    
    return json.dumps(result, indent=2)

@tool
def kg_find_path(source_entity_type: str, source_id: str, 
                target_entity_type: str, target_id: str) -> str:
    """
    Find the shortest path between two entities in the Knowledge Graph
    
    Args:
        source_entity_type: Type of source entity (Pod, PVC, PV, Drive, Node, etc.)
        source_id: ID or name of the source entity. Can be provided in two formats:
                  Examples: "gnode:Pod:default/nginx-pod", "gnode:PV:pv-00001", "gnode:Drive:drive-sda"
                  
                  Entity ID formats:
                  - Pod: "gnode:Pod:<namespace>/<name>" (example: "gnode:Pod:default/test-pod-1-0")
                  - PVC: "gnode:PVC:<namespace>/<name>" (example: "gnode:PVC:default/test-pvc-1")
                  - PV: "gnode:PV:<name>" (example: "gnode:PV:pv-test-123")
                  - Drive: "gnode:Drive:<uuid>" (example: "gnode:Drive:a1b2c3d4-e5f6")
                  - Node: "gnode:Node:<name>" (example: "gnode:Node:kind-control-plane")
                  - StorageClass: "gnode:StorageClass:<name>" (example: "gnode:StorageClass:csi-baremetal-sc")
                  - LVG: "gnode:LVG:<name>" (example: "gnode:LVG:lvg-1")
                  - AC: "gnode:AC:<name>" (example: "gnode:AC:ac-node1-ssd")
                  - Volume: "gnode:Volume:<namespace>/<name>" (example: "gnode:Volume:default/vol-1")
                  - System: "gnode:System:<entity_name>" (example: "gnode:System:kernel")
                  - ClusterNode: "gnode:ClusterNode:<name>" (example: "gnode:ClusterNode:worker-1")
                  - HistoricalExperience: "gnode:HistoricalExperience:<experience_id>" (example: "gnode:HistoricalExperience:exp-001")
        target_entity_type: Type of target entity (Pod, PVC, PV, Drive, Node, etc.)
        target_id: ID or name of the target entity. Format is the same as source_id.
                  The system will construct the full node_id as "gnode:{target_entity_type}:{target_id}"
                  Examples: "gnode:Pod:default/nginx-pod", "gnode:PV:pv-00001", "gnode:Drive:drive-sda"
        
    Returns:
        str: JSON serialized path between entities with relationship details
    """
    kg = get_knowledge_graph()
    
    # Construct node IDs
    source_node_id = f"gnode:{source_entity_type}:{source_id}"
    target_node_id = f"gnode:{target_entity_type}:{target_id}"
    
    # Check if nodes exist
    if not kg.graph.has_node(source_node_id):
        # Try to find by name or uuid
        for n_id, attrs in kg.graph.nodes(data=True):
            if (attrs.get('entity_type') == source_entity_type and 
                (attrs.get('name') == source_id or attrs.get('uuid') == source_id)):
                source_node_id = n_id
                break
    
    if not kg.graph.has_node(target_node_id):
        # Try to find by name or uuid
        for n_id, attrs in kg.graph.nodes(data=True):
            if (attrs.get('entity_type') == target_entity_type and 
                (attrs.get('name') == target_id or attrs.get('uuid') == target_id)):
                target_node_id = n_id
                break
    
    # Return error if either node is not found
    if not kg.graph.has_node(source_node_id):
        return json.dumps({"error": f"Source entity not found: gnode:{source_entity_type}:{source_id}"})
    
    if not kg.graph.has_node(target_node_id):
        return json.dumps({"error": f"Target entity not found: gnode:{target_entity_type}:{target_id}"})
    
    # Find shortest path
    path_nodes = kg.find_path(source_node_id, target_node_id)
    
    if not path_nodes:
        return json.dumps({
            "source_entity": {
                "node_id": source_node_id,
                "entity_type": source_entity_type,
                "name": kg.graph.nodes[source_node_id].get('name', source_node_id.split(':')[-1])
            },
            "target_entity": {
                "node_id": target_node_id,
                "entity_type": target_entity_type,
                "name": kg.graph.nodes[target_node_id].get('name', target_node_id.split(':')[-1])
            },
            "path_exists": False,
            "path": []
        })
    
    # Construct detailed path
    path_details = []
    for i in range(len(path_nodes) - 1):
        source = path_nodes[i]
        target = path_nodes[i + 1]
        
        # Get edge data
        edge_data = kg.graph.edges[source, target]
        relationship = edge_data.get('relationship', 'connected_to')
        
        # Get node data
        source_attrs = kg.graph.nodes[source]
        source_type = source_attrs.get('entity_type', 'Unknown')
        source_name = source_attrs.get('name', source.split(':')[-1])
        
        target_attrs = kg.graph.nodes[target]
        target_type = target_attrs.get('entity_type', 'Unknown')
        target_name = target_attrs.get('name', target.split(':')[-1])
        
        path_details.append({
            "source": {
                "node_id": source,
                "entity_type": source_type,
                "name": source_name
            },
            "target": {
                "node_id": target,
                "entity_type": target_type,
                "name": target_name
            },
            "relationship": relationship,
            "edge_attributes": {k: v for k, v in edge_data.items() if k != 'relationship'}
        })
    
    result = {
        "source_entity": {
            "node_id": source_node_id,
            "entity_type": source_entity_type,
            "name": kg.graph.nodes[source_node_id].get('name', source_node_id.split(':')[-1])
        },
        "target_entity": {
            "node_id": target_node_id,
            "entity_type": target_entity_type,
            "name": kg.graph.nodes[target_node_id].get('name', target_node_id.split(':')[-1])
        },
        "path_exists": True,
        "path_length": len(path_nodes) - 1,
        "path": path_details
    }
    
    return json.dumps(result, indent=2)

@tool
def kg_get_summary() -> str:
    """
    Get a summary of the Knowledge Graph including entity counts and issues
    
    Returns:
        str: JSON serialized summary of the Knowledge Graph
    """
    kg = get_knowledge_graph()
    summary = kg.get_summary()
    
    # Enhance with issue types distribution
    issue_types = {}
    for issue in kg.issues:
        issue_type = issue['type']
        issue_types[issue_type] = issue_types.get(issue_type, 0) + 1
    
    # Use a safer way to get current timestamp
    from datetime import datetime
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    result = {
        "graph_stats": summary,
        "issue_types": issue_types,
        "timestamp": current_time
    }
    
    return json.dumps(result, indent=2)

@tool
def kg_analyze_issues() -> str:
    """
    Analyze issues in the Knowledge Graph to identify patterns and root causes
    
    Returns:
        str: JSON serialized analysis results with potential root causes and fix plans
    """
    kg = get_knowledge_graph()
    
    # Run analysis
    analysis = kg.analyze_issues()
    
    # Generate fix plan based on analysis
    fix_plan = kg.generate_fix_plan(analysis)
    
    # Use a safer way to get current timestamp
    from datetime import datetime
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    result = {
        "analysis": analysis,
        "fix_plan": fix_plan,
        "timestamp": current_time
    }
    
    return json.dumps(result, indent=2)

@tool
def kg_print_graph(include_details: bool = True, include_issues: bool = True) -> str:
    """
    Get a human-friendly formatted representation of the Knowledge Graph
    
    Args:
        include_details: Whether to include detailed entity information
        include_issues: Whether to include issues in the output
        
    Returns:
        str: Formatted representation of the Knowledge Graph
    """
    kg = get_knowledge_graph()
    return kg.print_graph(
        include_detailed_entities=include_details,
        include_issues=include_issues,
        include_analysis=True,
        include_relationships=True
    )

@tool
def kg_list_entity_types() -> str:
    """
    Get a list of all entity types in the Knowledge Graph with their counts
    
    This tool helps discover what types of entities are available in the Knowledge Graph.
    Common entity types include: Pod, PVC, PV, Drive, Node, StorageClass, CSIDriver, etc.
    
    Returns:
        str: JSON serialized list of entity types and their counts
    """
    kg = get_knowledge_graph()
    
    # Count entities by type
    entity_types = {}
    for node_id, attrs in kg.graph.nodes(data=True):
        entity_type = attrs.get('entity_type', 'Unknown')
        entity_types[entity_type] = entity_types.get(entity_type, 0) + 1
    
    result = {
        "entity_types": [
            {"type": entity_type, "count": count}
            for entity_type, count in entity_types.items()
        ],
        "total_entity_types": len(entity_types),
        "total_entities": kg.graph.number_of_nodes()
    }
    
    return json.dumps(result, indent=2)

@tool
def kg_list_entities(entity_type: str = None) -> str:
    """
    Get a list of all entities of a specific type (or all entities if type is None)
    
    This tool helps discover what specific entities are available in the Knowledge Graph.
    Use this after kg_list_entity_types() to find entities of a particular type.
    
    Args:
        entity_type: Type of entity to list (Pod, PVC, PV, Drive, Node, etc.)
                    If None, lists all entities in the Knowledge Graph
    
    Returns:
        str: JSON serialized list of entities with their IDs, names, and key attributes
    """
    kg = get_knowledge_graph()
    
    entities = []
    for node_id, attrs in kg.graph.nodes(data=True):
        node_entity_type = attrs.get('entity_type', 'Unknown')
        
        # Filter by entity_type if specified
        if entity_type is not None and node_entity_type != entity_type:
            continue
            
        # Extract key information
        entity = {
            "node_id": node_id,
            "entity_type": node_entity_type,
            "name": attrs.get('name', node_id.split(':')[-1]),
            "has_issues": bool(attrs.get('issues', [])),
            "issue_count": len(attrs.get('issues', [])),
            "key_attributes": {k: v for k, v in attrs.items() 
                             if k not in ['entity_type', 'name', 'issues'] 
                             and not isinstance(v, (dict, list)) 
                             and len(str(v)) < 100}
        }
        entities.append(entity)
    
    result = {
        "filter_type": entity_type if entity_type else "All",
        "entities": entities,
        "total_count": len(entities)
    }
    
    return json.dumps(result, indent=2)

@tool
def kg_list_relationship_types() -> str:
    """
    Get a list of all relationship types in the Knowledge Graph with their counts
    
    This tool helps discover what types of relationships exist between entities.
    Common relationship types include: uses, bound_to, runs_on, located_on, maps_to, etc.
    
    Returns:
        str: JSON serialized list of relationship types and their counts
    """
    kg = get_knowledge_graph()
    
    # Count relationships by type
    relationship_types = {}
    for _, _, edge_data in kg.graph.edges(data=True):
        rel_type = edge_data.get('relationship', 'connected_to')
        relationship_types[rel_type] = relationship_types.get(rel_type, 0) + 1
    
    result = {
        "relationship_types": [
            {"type": rel_type, "count": count}
            for rel_type, count in relationship_types.items()
        ],
        "total_relationship_types": len(relationship_types),
        "total_relationships": kg.graph.number_of_edges()
    }
    
    return json.dumps(result, indent=2)

# Entity ID helper tools

@tool
def kg_get_entity_of_pod(namespace: str, name: str) -> str:
    """
    Get the entity ID for a Pod in the Knowledge Graph
    
    Args:
        namespace: Namespace of the Pod
        name: Name of the Pod
        
    Returns:
        str: Entity ID in the format 'gnode:Pod:<namespace>/<name>'
             Example: 'gnode:Pod:default/test-pod-1-0'
    """
    return f"gnode:Pod:{namespace}/{name}"

@tool
def kg_get_entity_of_pvc(namespace: str, name: str) -> str:
    """
    Get the entity ID for a PVC in the Knowledge Graph
    
    Args:
        namespace: Namespace of the PVC
        name: Name of the PVC
        
    Returns:
        str: Entity ID in the format 'gnode:PVC:<namespace>/<name>'
             Example: 'gnode:PVC:default/test-pvc-1'
    """
    return f"gnode:PVC:{namespace}/{name}"

@tool
def kg_get_entity_of_pv(name: str) -> str:
    """
    Get the entity ID for a PV in the Knowledge Graph
    
    Args:
        name: Name of the PV
        
    Returns:
        str: Entity ID in the format 'gnode:PV:<name>'
             Example: 'gnode:PV:pv-test-123'
    """
    return f"gnode:PV:{name}"

@tool
def kg_get_entity_of_drive(uuid: str) -> str:
    """
    Get the entity ID for a Drive in the Knowledge Graph
    
    Args:
        uuid: UUID of the Drive
        
    Returns:
        str: Entity ID in the format 'gnode:Drive:<uuid>'
             Example: 'gnode:Drive:a1b2c3d4-e5f6'
    """
    return f"gnode:Drive:{uuid}"

@tool
def kg_get_entity_of_node(name: str) -> str:
    """
    Get the entity ID for a Node in the Knowledge Graph
    
    Args:
        name: Name of the Node
        
    Returns:
        str: Entity ID in the format 'gnode:Node:<name>'
             Example: 'gnode:Node:kind-control-plane'
    """
    return f"gnode:Node:{name}"

@tool
def kg_get_entity_of_storage_class(name: str) -> str:
    """
    Get the entity ID for a StorageClass in the Knowledge Graph
    
    Args:
        name: Name of the StorageClass
        
    Returns:
        str: Entity ID in the format 'gnode:StorageClass:name'
    """
    return f"gnode:StorageClass:{name}"

@tool
def kg_get_entity_of_lvg(name: str) -> str:
    """
    Get the entity ID for a LogicalVolumeGroup in the Knowledge Graph
    
    Args:
        name: Name of the LVG
        
    Returns:
        str: Entity ID in the format 'gnode:LVG:name'
    """
    return f"gnode:LVG:{name}"

@tool
def kg_get_entity_of_ac(name: str) -> str:
    """
    Get the entity ID for an AvailableCapacity in the Knowledge Graph
    
    Args:
        name: Name of the AC
        
    Returns:
        str: Entity ID in the format 'gnode:AC:name'
    """
    return f"gnode:AC:{name}"

@tool
def kg_get_entity_of_volume(namespace: str, name: str) -> str:
    """
    Get the entity ID for a Volume in the Knowledge Graph
    
    Args:
        namespace: Namespace of the Volume
        name: Name of the Volume
        
    Returns:
        str: Entity ID in the format 'gnode:Volume:namespace/name'
    """
    return f"gnode:Volume:{namespace}/{name}"

@tool
def kg_get_entity_of_system(entity_name: str) -> str:
    """
    Get the entity ID for a System entity in the Knowledge Graph
    
    Args:
        entity_name: Name of the System entity
        
    Returns:
        str: Entity ID in the format 'gnode:System:entity_name'
    """
    return f"gnode:System:{entity_name}"

@tool
def kg_get_entity_of_cluster_node(name: str) -> str:
    """
    Get the entity ID for a ClusterNode in the Knowledge Graph
    
    Args:
        name: Name of the ClusterNode
        
    Returns:
        str: Entity ID in the format 'gnode:ClusterNode:name'
    """
    return f"gnode:ClusterNode:{name}"

@tool
def kg_get_entity_of_historical_experience(experience_id: str) -> str:
    """
    Get the entity ID for a HistoricalExperience in the Knowledge Graph
    
    Args:
        experience_id: ID of the HistoricalExperience
        
    Returns:
        str: Entity ID in the format 'gnode:HistoricalExperience:experience_id'
    """
    return f"gnode:HistoricalExperience:{experience_id}"
</file>

<file path="tests/test_langgraph_tools.py">
#!/usr/bin/env python3
"""
LangGraph Tools Test Script

This script tests all langgraph tools in the cluster-storage-troubleshooting system
and reports which tools have issues.
"""

import os
import sys
import yaml
import json
import asyncio
import logging
from typing import Dict, List, Any
from rich.console import Console
from rich.table import Table
from rich.panel import Panel

# Import necessary components
from troubleshooting.graph import create_troubleshooting_graph_with_context
from knowledge_graph.knowledge_graph import KnowledgeGraph
from tools.core.knowledge_graph import initialize_knowledge_graph

# Import all tool modules
from tools.registry import (
    get_all_tools,
    get_knowledge_graph_tools,
    get_kubernetes_tools,
    get_diagnostic_tools,
    get_testing_tools,
    get_phase1_tools,
    get_phase2_tools
)

# Initialize rich console for nice output
console = Console()

#os.environ['LANGCHAIN_TRACING_V2'] = "true"   
#os.environ['LANGCHAIN_ENDPOINT'] = "https://api.smith.langchain.com"   
#os.environ['LANGCHAIN_API_KEY'] = "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"   
#os.environ['LANGCHAIN_PROJECT'] = "pr-silver-bank-1"

def load_config():
    """Load configuration from config.yaml"""
    try:
        with open('config.yaml', 'r') as f:
            config = yaml.safe_load(f)
        
        # Ensure the config has the necessary fields for testing
        if 'llm' not in config:
            config['llm'] = {
                'model': 'gpt-3.5-turbo',
                'api_key': 'dummy-key-for-testing',
                'api_endpoint': 'https://api.openai.com/v1',
                'temperature': 0.1,
                'max_tokens': 1000
            }
        
        if 'logging' not in config:
            config['logging'] = {
                'file': 'troubleshoot.log',
                'stdout': True
            }
        
        if 'troubleshoot' not in config:
            config['troubleshoot'] = {
                'timeout_seconds': 1800
            }
            
        return config
    except Exception as e:
        console.print(f"[bold red]Failed to load configuration:[/bold red] {str(e)}")
        console.print("[yellow]Using default test configuration...[/yellow]")
        
        # Return a default test configuration
        return {
            'llm': {
                'model': 'gpt-3.5-turbo',
                'api_key': 'dummy-key-for-testing',
                'api_endpoint': 'https://api.openai.com/v1',
                'temperature': 0.1,
                'max_tokens': 1000
            },
            'logging': {
                'file': 'troubleshoot.log',
                'stdout': True
            },
            'troubleshoot': {
                'timeout_seconds': 1800
            }
        }

async def test_tool(tool_func, test_args=None, config_data=None):
    """Test a single tool function with optional arguments"""
    # Handle both regular functions and StructuredTool objects
    if hasattr(tool_func, 'name'):
        # For StructuredTool objects from langchain
        tool_name = tool_func.name
    elif hasattr(tool_func, '__name__'):
        # For regular Python functions
        tool_name = tool_func.__name__
    else:
        # Fallback
        tool_name = str(tool_func)

    try:
        # Ensure we have a config object for StructuredTool._run()
        if config_data is None:
            config_data = {
                "llm": {
                    "model": "gpt-3.5-turbo",
                    "api_key": "dummy-key-for-testing",
                    "api_endpoint": "https://api.openai.com/v1",
                    "temperature": 0.1,
                    "max_tokens": 1000
                },
                "logging": {
                    "file": "troubleshoot.log",
                    "stdout": True
                },
                "troubleshoot": {
                    "timeout_seconds": 1800
                }
            }
        
        # Prepare arguments with config
        if test_args is None:
            args_with_config = {"config": config_data}
        else:
            args_with_config = {**test_args, "config": config_data}
        
        result = None
        if hasattr(tool_func, '_run'):
            # For StructuredTool objects
            if test_args is None:
                # Call with only config
                if asyncio.iscoroutinefunction(tool_func._run):
                    result = await tool_func._run(config=config_data)
                else:
                    result = tool_func._run(config=config_data)
            else:
                # Call with test args and config
                if asyncio.iscoroutinefunction(tool_func._run):
                    result = await tool_func._run(**test_args, config=config_data)
                else:
                    result = tool_func._run(**test_args, config=config_data)
        else:
            # For regular functions
            if test_args is None:
                # Call with no args
                if asyncio.iscoroutinefunction(tool_func):
                    result = await tool_func()
                else:
                    result = tool_func()
            else:
                # Call with test args
                if asyncio.iscoroutinefunction(tool_func):
                    result = await tool_func(**test_args)
                else:
                    result = tool_func(**test_args)
        
        return {
            "name": tool_name,
            "status": "success",
            "result": result
        }
    except Exception as e:
        console.print(f"[bold red]Error testing tool '{tool_name}':[/bold red] {str(e)}")
        return {
            "name": tool_name,
            "status": "error",
            "error": str(e)
        }

async def test_knowledge_graph_tools(kg, config_data=None):
    """Test all Knowledge Graph tools"""
    console.print(Panel("Testing Knowledge Graph Tools", style="bold blue"))
    
    # Initialize test arguments for each KG tool
    test_args = {
        "kg_get_entity_info": {"entity_type": "Pod", "id": "gnode:Pod:default/test-pod-1-0"},
        "kg_get_related_entities": {"entity_type": "Pod", "id": "gnode:Pod:default/test-pod-1-0"},
        "kg_get_all_issues": {},
        "kg_find_path": {
            "source_entity_type": "Pod", 
            "source_id": "gnode:Pod:default/test-pod-1-0",
            "target_entity_type": "Node", 
            "target_id": "gnode:Node:kind-control-plane"
        },
        "kg_get_summary": None,
        "kg_analyze_issues": None,
        "kg_print_graph": {"include_details": True, "include_issues": True},
        "kg_list_entity_types": {},
        "kg_list_entities": {"entity_type": "Pod"},
        "kg_list_relationship_types": {}
    }
    
    results = []
    for tool in get_knowledge_graph_tools():
        # Get tool name safely
        if hasattr(tool, 'name'):
            tool_name = tool.name
        elif hasattr(tool, '__name__'):
            tool_name = tool.__name__
        else:
            tool_name = str(tool)
            
        args = test_args.get(tool_name, {})
        result = await test_tool(tool, args, config_data)
        results.append(result)
        
        # Print result
        status_color = "green" if result["status"] == "success" else "red"
        console.print(f"[bold]{result['name']}[/bold]: [{status_color}]{result['status']}[/{status_color}]")
        console.print(f"Result: {result.get('result', 'No result')[:500]}")
    
    return results

async def test_kubernetes_tools(config_data=None):
    """Test all Kubernetes tools"""
    console.print(Panel("Testing Kubernetes Tools", style="bold blue"))
    
    # Initialize test arguments for each Kubernetes tool
    test_args = {
        "kubectl_get": {"resource_type": "pods", "namespace": "default"},
        "kubectl_describe": {"resource_type": "pod", "resource_name": "test-pod-1-0", "namespace": "default"},
        "kubectl_apply": {"yaml_content": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod-1-0\nspec:\n  containers:\n  - name: test-container\n    image: nginx"},
        "kubectl_delete": {"resource_type": "pod", "resource_name": "test-pod-1-0", "namespace": "default"},
        "kubectl_exec": {"pod_name": "test-pod-1-0", "namespace": "default", "command": "ls -la"},
        "kubectl_logs": {"pod_name": "test-pod-1-0", "namespace": "default"},
        "kubectl_get_drive": {},
        "kubectl_get_csibmnode": {},
        "kubectl_get_availablecapacity": {},
        "kubectl_get_logicalvolumegroup": {},
        "kubectl_get_storageclass": {},
        "kubectl_get_csidrivers": {}
    }
    
    results = []
    for tool in get_kubernetes_tools():
        # Get tool name safely
        if hasattr(tool, 'name'):
            tool_name = tool.name
        elif hasattr(tool, '__name__'):
            tool_name = tool.__name__
        else:
            tool_name = str(tool)
            
        args = test_args.get(tool_name, {})
        result = await test_tool(tool, args, config_data)
        results.append(result)
        
        # Print result
        status_color = "green" if result["status"] == "success" else "red"
        console.print(f"[bold]{result['name']}[/bold]: [{status_color}]{result['status']}[/{status_color}]")
        console.print(f"Result: {result.get('result', 'No result')[:500]}")
    
    return results

async def test_diagnostic_tools(config_data=None):
    """Test all diagnostic tools"""
    console.print(Panel("Testing Diagnostic Tools", style="bold blue"))
    
    # Initialize test arguments for each diagnostic tool
    test_args = {
        "smartctl_check": {"node_name": "kind-control-plane", "device_path": "/dev/sda"},
        "fio_performance_test": {"node_name": "kind-control-plane", "device_path": "/dev/sda"},
        "fsck_check": {"node_name": "kind-control-plane", "device_path": "/dev/sda"},
        "xfs_repair_check": {"node_name": "kind-control-plane", "device_path": "/dev/sda"},
        "ssh_execute": {"node_name": "kind-control-plane", "command": "ls -la"},
        "df_command": {"path": "/tmp", "options": "-h"},
        "lsblk_command": {"options": "-f"},
        "mount_command": {"options": "-t xfs"},
        "dmesg_command": {"options": "-T"},
        "journalctl_command": {"options": "-u kubelet"},
        # New disk tools
        "detect_disk_jitter": {"duration_minutes": 1, "check_interval_seconds": 10, "node_name": "kind-control-plane"},
        "run_disk_readonly_test": {"node_name": "kind-control-plane", "device_path": "/dev/sda", "duration_minutes": 1},
        "test_disk_io_performance": {"node_name": "kind-control-plane", "device_path": "/dev/sda", "duration_seconds": 10},
        "check_disk_health": {"node_name": "kind-control-plane", "device_path": "/dev/sda"},
        "analyze_disk_space_usage": {"node_name": "kind-control-plane", "mount_path": "/"},
        "scan_disk_error_logs": {"node_name": "kind-control-plane", "hours_back": 1}
    }
    
    results = []
    for tool in get_diagnostic_tools():
        # Get tool name safely
        if hasattr(tool, 'name'):
            tool_name = tool.name
        elif hasattr(tool, '__name__'):
            tool_name = tool.__name__
        else:
            tool_name = str(tool)
            
        args = test_args.get(tool_name, {})
        result = await test_tool(tool, args, config_data)
        results.append(result)
        
        # Print result
        status_color = "green" if result["status"] == "success" else "red"
        console.print(f"[bold]{result['name']}[/bold]: [{status_color}]{result['status']}[/{status_color}]")
        console.print(f"Result: {result.get('result', 'No result')[:500]}")
    
    return results

async def test_testing_tools(config_data=None):
    """Test all testing tools"""
    console.print(Panel("Testing Testing Tools", style="bold blue"))
    
    # Initialize test arguments for each testing tool
    test_args = {
        "create_test_pod": {"pod_name": "test-pod-1-0", "namespace": "default"},
        "create_test_pvc": {"pvc_name": "www-1-test-pod-1-0", "namespace": "default", "size": "1Gi"},
        "create_test_storage_class": {"sc_name": "test-sc"},
        "run_volume_io_test": {"pod_name": "test-pod-1-0", "namespace": "default", "mount_path": "/log"},
        "validate_volume_mount": {"pod_name": "test-pod-1-0", "namespace": "default", "mount_path": "/log"},
        "test_volume_permissions": {"pod_name": "test-pod-1-0", "namespace": "default", "mount_path": "/log"},
        "run_volume_stress_test": {"pod_name": "test-pod-1-0", "namespace": "default", "mount_path": "/log"},
        "cleanup_test_resources": {"namespace": "default"},
        "list_test_resources": {"namespace": "default"},
        "cleanup_specific_test_pod": {"pod_name": "test-pod-1-0", "namespace": "default"},
        "cleanup_orphaned_pvs": {},
        "force_cleanup_stuck_resources": {"namespace": "default"},
        # Additional volume testing tools
        "verify_volume_mount": {"pod_name": "test-pod-1-0", "namespace": "default", "mount_path": "/log"},
        "test_volume_io_performance": {"pod_name": "test-pod-1-0", "namespace": "default", "mount_path": "/log"},
        "monitor_volume_latency": {"pod_name": "test-pod-1-0", "namespace": "default", "mount_path": "/log", "duration": 1},
        "check_pod_volume_filesystem": {"pod_name": "test-pod-1-0", "namespace": "default", "mount_path": "/log"},
        "analyze_volume_space_usage": {"pod_name": "test-pod-1-0", "namespace": "default", "mount_path": "/log"},
        "check_volume_data_integrity": {"pod_name": "test-pod-1-0", "namespace": "default", "mount_path": "/log"},
        "run_disk_readonly_test": {"node_name": "kind-control-plane", "device_path": "/dev/sda", "duration_minutes": 1},
        "test_disk_io_performance": {"node_name": "kind-control-plane", "device_path": "/dev/sda", "duration_seconds": 10},
    }

    results = []
    for tool in get_testing_tools():
        # Get tool name safely
        if hasattr(tool, 'name'):
            tool_name = tool.name
        elif hasattr(tool, '__name__'):
            tool_name = tool.__name__
        else:
            tool_name = str(tool)
            
        args = test_args.get(tool_name, {})
        result = await test_tool(tool, args, config_data)
        results.append(result)
        
        # Print result
        status_color = "green" if result["status"] == "success" else "red"
        console.print(f"[bold]{result['name']}[/bold]: [{status_color}]{result['status']}[/{status_color}]")
        console.print(f"Result: {result.get('result', 'No result')[:500]}")
    
    return results

async def test_langgraph_components(config_data):
    """Test LangGraph components"""
    console.print(Panel("Testing LangGraph Components", style="bold blue"))
    
    try:
        # Create a minimal test context
        collected_info = {
            "pod_info": {"test-pod-1-0": {"status": "Running"}},
            "pvc_info": {"www-1-test-pod-1-0": {"status": "Bound"}},
            "pv_info": {"test-pv": {"status": "Bound"}},
            "node_info": {"kind-control-plane": {"status": "Ready"}},
            "csi_driver_info": {},
            "system_info": {},
            "knowledge_graph_summary": {},
            "issues": []
        }
        
        # Test Phase 1 graph
        phase1_graph = create_troubleshooting_graph_with_context(
            collected_info=collected_info,
            phase="phase1",
            config_data=config_data
        )
        
        console.print("[bold]Phase 1 Graph Creation[/bold]: [green]success[/green]")
        
        # Test Phase 2 graph
        phase2_graph = create_troubleshooting_graph_with_context(
            collected_info=collected_info,
            phase="phase2",
            config_data=config_data
        )
        
        console.print("[bold]Phase 2 Graph Creation[/bold]: [green]success[/green]")
        
        return {
            "phase1_graph": "success",
            "phase2_graph": "success"
        }
        
    except Exception as e:
        console.print(f"[bold]LangGraph Component Test[/bold]: [red]error[/red] - {str(e)}")
        return {
            "status": "error",
            "error": str(e)
        }

def generate_summary_report(all_results):
    """Generate a summary report of all test results"""
    console.print(Panel("Test Results Summary", style="bold green"))
    
    # Check if we have any results
    if not all_results:
        console.print("[bold red]No test results available![/bold red]")
        return
    
    # Create a table for the summary
    table = Table(title="Tool Test Results")
    table.add_column("Category", style="cyan")
    table.add_column("Total Tools", style="magenta")
    table.add_column("Success", style="green")
    table.add_column("Failed", style="red")
    table.add_column("Success Rate", style="yellow")
    
    # Add rows for each category
    categories = all_results.keys()
    for category in categories:
        if category == "langgraph":
            continue  # Skip langgraph components from the tool count
        
        # Check if the category has results
        if category not in all_results or not all_results[category]:
            continue
            
        results = all_results[category]
        total = len(results)
        success = sum(1 for r in results if r["status"] == "success")
        failed = total - success
        success_rate = f"{(success/total)*100:.1f}%" if total > 0 else "N/A"
        
        table.add_row(
            category,
            str(total),
            str(success),
            str(failed),
            success_rate
        )
    
    # Add a total row
    all_tools = []
    for category in categories:
        if category != "langgraph":
            all_tools.extend(all_results[category])
    
    total = len(all_tools)
    success = sum(1 for r in all_tools if r["status"] == "success")
    failed = total - success
    success_rate = f"{(success/total)*100:.1f}%" if total > 0 else "N/A"
    
    table.add_row(
        "TOTAL",
        str(total),
        str(success),
        str(failed),
        success_rate,
        style="bold"
    )
    
    console.print(table)
    
    # List failed tools
    if failed > 0:
        console.print(Panel("Failed Tools", style="bold red"))
        for tool in all_tools:
            if tool["status"] == "error":
                console.print(f"[bold]{tool['name']}[/bold]: {tool.get('error', 'Unknown error')}")

async def main():
    """Main function to run all tests"""
    # Parse command line arguments
    import argparse
    parser = argparse.ArgumentParser(description="Test all langgraph tools in the system")
    parser.add_argument("--output", "-o", default="tool_test_results.json", help="Output file path for test results")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose output")
    parser.add_argument("--category", "-c", help="Test only a specific category (knowledge_graph, kubernetes, diagnostic, testing)")
    args = parser.parse_args()
    
    # Set up output file path
    output_file = args.output
    
    # Configure verbosity
    verbose = args.verbose
    
    console.print(Panel(
        "[bold]LangGraph Tools Test Script[/bold]\n\n"
        "This script tests all langgraph tools in the cluster-storage-troubleshooting system\n"
        "and reports which tools have issues.",
        style="bold green"
    ))
    
    if verbose:
        console.print("[yellow]Verbose mode enabled[/yellow]")
    
    # Load configuration
    config_data = load_config()
    
    # Initialize knowledge graph - create a test instance for testing purposes
    kg = KnowledgeGraph()
    # Pass the KG instance explicitly as required by the updated initialize_knowledge_graph function
    initialize_knowledge_graph(kg)
    
    # Add some test data to the knowledge graph
    pod_id = kg.add_gnode_pod("test-pod-1-0", "default", status="Running")
    node_id = kg.add_gnode_node("kind-control-plane", status="Ready")
    pvc_id = kg.add_gnode_pvc("www-1-test-pod-1-0", "default", status="Bound")
    pv_id = kg.add_gnode_pv("test-pv", status="Bound")
    
    # Add relationships
    kg.add_relationship(pod_id, node_id, "runs_on")
    kg.add_relationship(pod_id, pvc_id, "uses")
    kg.add_relationship(pvc_id, pv_id, "bound_to")
    
    # Add some test issues
    kg.add_issue(pod_id, "pod_crash", "Pod is crashing", severity="medium")
    kg.add_issue(node_id, "disk_pressure", "Node is experiencing disk pressure", severity="high")
    
    # Run tests based on category selection
    all_results = {}
    category = args.category
    
    try:
        # Test LangGraph components (always test these)
        all_results["langgraph"] = await test_langgraph_components(config_data)
        
        # Test specific category or all categories
        if category is None or category == "all":
            console.print("[yellow]Testing all tool categories...[/yellow]")
            # Test Knowledge Graph tools
            all_results["knowledge_graph"] = await test_knowledge_graph_tools(kg, config_data)
            
            # Test Kubernetes tools
            #all_results["kubernetes"] = await test_kubernetes_tools(config_data)
            
            # Test Diagnostic tools
            #all_results["diagnostic"] = await test_diagnostic_tools(config_data)
            
            # Test Testing tools
            #all_results["testing"] = await test_testing_tools(config_data)
        elif category == "knowledge_graph":
            console.print("[yellow]Testing only Knowledge Graph tools...[/yellow]")
            all_results["knowledge_graph"] = await test_knowledge_graph_tools(kg, config_data)
        elif category == "kubernetes":
            console.print("[yellow]Testing only Kubernetes tools...[/yellow]")
            all_results["kubernetes"] = await test_kubernetes_tools(config_data)
        elif category == "diagnostic":
            console.print("[yellow]Testing only Diagnostic tools...[/yellow]")
            all_results["diagnostic"] = await test_diagnostic_tools(config_data)
        elif category == "testing":
            console.print("[yellow]Testing only Testing tools...[/yellow]")
            all_results["testing"] = await test_testing_tools(config_data)
        else:
            console.print(f"[bold red]Unknown category: {category}[/bold red]")
            console.print("[yellow]Available categories: knowledge_graph, kubernetes, diagnostic, testing[/yellow]")
            return
    except Exception as e:
        console.print(f"[bold red]Error during test execution:[/bold red] {str(e)}")
        console.print("[yellow]Continuing with available results...[/yellow]")
    
    # Generate summary report
    generate_summary_report(all_results)
    
    # Save results to file
    try:
        with open(output_file, 'w') as f:
            json.dump(all_results, f, indent=2, default=str)
        console.print(Panel(f"Test results saved to {output_file}", style="bold blue"))
    except Exception as e:
        console.print(f"[bold red]Error saving results to file:[/bold red] {str(e)}")

if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="tools/registry.py">
#!/usr/bin/env python3
"""
Tool registry for Kubernetes volume troubleshooting.

This module provides centralized tool registration and discovery,
making it easy to access all available tools from different categories.
Supports phase-based tool selection for investigation (Phase 1) and 
action (Phase 2) workflows.
"""

from typing import List, Dict, Any
from tools.core.mcp_adapter import get_mcp_adapter

# Import all tool modules
from tools.core.knowledge_graph import (
    kg_get_entity_info,
    kg_get_related_entities,
    kg_get_all_issues,
    kg_find_path,
    kg_get_summary,
    kg_analyze_issues,
    kg_print_graph,
    initialize_knowledge_graph,
    get_knowledge_graph
)

from tools.kubernetes.core import (
    kubectl_get,
    kubectl_describe,
    kubectl_apply,
    kubectl_delete,
    kubectl_exec,
    kubectl_logs,
    kubectl_ls_pod_volume
)

from tools.kubernetes.csi_baremetal import (
    kubectl_get_drive,
    kubectl_get_csibmnode,
    kubectl_get_availablecapacity,
    kubectl_get_logicalvolumegroup,
    kubectl_get_storageclass,
    kubectl_get_csidrivers
)

from tools.diagnostics.hardware import (
    smartctl_check,
    fio_performance_test,
    fsck_check,
    xfs_repair_check,
    #ssh_execute
)

from tools.diagnostics.system import (
    df_command,
    lsblk_command,
    mount_command,
    dmesg_command,
    journalctl_command,
    get_system_hardware_info
)

# Import new disk check tools
from tools.diagnostics.disk_monitoring import (
    detect_disk_jitter
)

from tools.diagnostics.disk_performance import (
    run_disk_readonly_test,
    test_disk_io_performance
)

from tools.diagnostics.disk_analysis import (
    check_disk_health,
    analyze_disk_space_usage,
    scan_disk_error_logs
)

# Import testing tools
from tools.testing.pod_creation import (
    create_test_pod,
    create_test_pvc,
    create_test_storage_class
)

from tools.testing.volume_testing import (
    run_volume_io_test,
    validate_volume_mount,
    test_volume_permissions,
    run_volume_stress_test,
    verify_volume_mount,
    test_volume_io_performance,
    monitor_volume_latency,
    check_pod_volume_filesystem,
    analyze_volume_space_usage,
    check_volume_data_integrity
)

from tools.testing.resource_cleanup import (
    cleanup_test_resources,
    list_test_resources,
    cleanup_specific_test_pod,
    cleanup_orphaned_pvs,
    force_cleanup_stuck_resources
)

def get_all_tools() -> List[Any]:
    """
    Get all available tools for troubleshooting
    
    Returns:
        List[Any]: List of all tool callables
    """
    return [
        # Knowledge Graph tools
        kg_get_entity_info,
        kg_get_related_entities,
        kg_get_all_issues,
        kg_find_path,
        kg_get_summary,
        kg_analyze_issues,
        kg_print_graph,
        
        # Kubernetes core tools
        kubectl_get,
        kubectl_describe,
        kubectl_apply,
        kubectl_delete,
        kubectl_exec,
        kubectl_logs,
        kubectl_ls_pod_volume,
        
        # CSI Baremetal specific tools
        kubectl_get_drive,
        kubectl_get_csibmnode,
        kubectl_get_availablecapacity,
        kubectl_get_logicalvolumegroup,
        kubectl_get_storageclass,
        kubectl_get_csidrivers,
        
        # Hardware diagnostic tools
        smartctl_check,
        fio_performance_test,
        fsck_check,
        xfs_repair_check,
        #ssh_execute,
        
        # System diagnostic tools
        df_command,
        lsblk_command,
        mount_command,
        dmesg_command,
        journalctl_command,
        get_system_hardware_info,
        
        # New disk check tools
        detect_disk_jitter,
        run_disk_readonly_test,
        test_disk_io_performance,
        check_disk_health,
        analyze_disk_space_usage,
        scan_disk_error_logs,
        
        # Volume testing tools
        run_volume_io_test,
        validate_volume_mount,
        test_volume_permissions,
        run_volume_stress_test,
        verify_volume_mount,
        test_volume_io_performance,
        monitor_volume_latency,
        check_pod_volume_filesystem,
        analyze_volume_space_usage,
        check_volume_data_integrity
    ]

def get_knowledge_graph_tools() -> List[Any]:
    """
    Get Knowledge Graph specific tools
    
    Returns:
        List[Any]: List of Knowledge Graph tool callables
    """
    return [
        kg_get_entity_info,
        kg_get_related_entities,
        kg_get_all_issues,
        kg_find_path,
        kg_get_summary,
        kg_analyze_issues,
        kg_print_graph
    ]

def get_kubernetes_tools() -> List[Any]:
    """
    Get Kubernetes related tools (core + CSI Baremetal)
    
    Returns:
        List[Any]: List of Kubernetes tool callables
    """
    return [
        # Core Kubernetes tools
        kubectl_get,
        kubectl_describe,
        kubectl_apply,
        kubectl_delete,
        kubectl_exec,
        kubectl_logs,
        kubectl_ls_pod_volume,
        
        # CSI Baremetal specific tools
        kubectl_get_drive,
        kubectl_get_csibmnode,
        kubectl_get_availablecapacity,
        kubectl_get_logicalvolumegroup,
        kubectl_get_storageclass,
        kubectl_get_csidrivers
    ]

def get_diagnostic_tools() -> List[Any]:
    """
    Get diagnostic tools (hardware + system)
    
    Returns:
        List[Any]: List of diagnostic tool callables
    """
    return [
        # Hardware diagnostic tools
        smartctl_check,
        fio_performance_test,
        fsck_check,
        xfs_repair_check,
        #ssh_execute,
        
        # System diagnostic tools
        df_command,
        lsblk_command,
        mount_command,
        dmesg_command,
        journalctl_command,
        get_system_hardware_info,
        
        # New disk check tools
        detect_disk_jitter,
        run_disk_readonly_test,
        test_disk_io_performance,
        check_disk_health,
        analyze_disk_space_usage,
        scan_disk_error_logs
    ]

def get_phase1_tools() -> List[Any]:
    """
    Get Phase 1 investigation tools (read-only, information gathering)
    
    Phase 1 focuses on comprehensive investigation, root cause analysis,
    and evidence collection without any destructive operations.
    
    Returns:
        List[Any]: List of Phase 1 tool callables
    """
    return [
        # Knowledge Graph tools - Full analysis capabilities
        kg_get_entity_info,
        kg_get_related_entities,
        kg_get_all_issues,
        kg_find_path,
        kg_get_summary,
        kg_analyze_issues,
        kg_print_graph,
        
        # Read-only Kubernetes tools
        kubectl_get,
        kubectl_describe,
        kubectl_logs,
        kubectl_exec,  # Limited to read-only commands
        kubectl_ls_pod_volume,  # New tool for listing pod volume contents
        
        # CSI Baremetal information tools
        kubectl_get_drive,
        kubectl_get_csibmnode,
        kubectl_get_availablecapacity,
        kubectl_get_logicalvolumegroup,
        kubectl_get_storageclass,
        kubectl_get_csidrivers,
        
        # System information tools
        df_command,
        lsblk_command,
        mount_command,
        dmesg_command,
        journalctl_command,
        get_system_hardware_info,
        
        # Hardware information tools
        smartctl_check,  # Read-only disk health check
        xfs_repair_check,  # Read-only file system check
        #ssh_execute,     # Limited to read-only operations
        
        # New read-only disk check tools
        detect_disk_jitter,  # Monitoring tool
        check_disk_health,   # Disk health assessment
        analyze_disk_space_usage,  # Space usage analysis
        scan_disk_error_logs,  # Log scanning
        run_disk_readonly_test,
        test_disk_io_performance,  # Read-only I/O performance test

        # Volume testing tools - Read-only checks
        run_volume_io_test,
        verify_volume_mount,
        test_volume_io_performance,
        test_volume_permissions,
        run_volume_stress_test,  # Non-destructive stress test
        monitor_volume_latency,
        check_pod_volume_filesystem,
        analyze_volume_space_usage,
        check_volume_data_integrity,
    ]

def get_phase2_tools() -> List[Any]:
    """
    Get Phase 2 tools (Phase 1 + Action tools)
    
    Phase 2 includes all Phase 1 investigation tools PLUS action tools
    for remediation, testing, and resource management.
    
    Returns:
        List[Any]: List of Phase 2 tool callables
    """
    return get_phase1_tools() + [
        # Additional Kubernetes action tools
        kubectl_apply,
        kubectl_delete,
        
        # Hardware action tools
        fio_performance_test,
        fsck_check,
        
        # New disk performance testing tools
        run_disk_readonly_test,   # Read-only test
        test_disk_io_performance, # I/O performance test
        
        # Testing tools - Pod/Resource creation
        #create_test_pod,
        #create_test_pvc,
        #create_test_storage_class,
        
        # Testing tools - Volume testing
        run_volume_io_test,
        validate_volume_mount,
        test_volume_permissions,
        run_volume_stress_test,
        verify_volume_mount,
        test_volume_io_performance,
        monitor_volume_latency,
        check_pod_volume_filesystem,
        analyze_volume_space_usage,
        check_volume_data_integrity,
        
        # Testing tools - Resource cleanup
        #cleanup_test_resources,
        #list_test_resources,
        #cleanup_specific_test_pod,
        #cleanup_orphaned_pvs,
        #force_cleanup_stuck_resources
    ]

def get_testing_tools() -> List[Any]:
    """
    Get testing and validation tools for Phase 2
    
    Returns:
        List[Any]: List of testing tool callables
    """
    return [
        # Pod/Resource creation tools
        #create_test_pod,
        #create_test_pvc,
        #create_test_storage_class,
        
        # Volume testing tools
        run_volume_io_test,
        validate_volume_mount,
        test_volume_permissions,
        run_volume_stress_test,
        
        # Resource cleanup tools
        #cleanup_test_resources,
        #list_test_resources,
        #cleanup_specific_test_pod,
        #cleanup_orphaned_pvs,
        #force_cleanup_stuck_resources,
        
        # New disk performance testing tools
        run_disk_readonly_test,
        test_disk_io_performance,
        verify_volume_mount,
        test_volume_io_performance,
        monitor_volume_latency,
        check_pod_volume_filesystem,
        analyze_volume_space_usage,
        check_volume_data_integrity
    ]

def get_remediation_tools() -> List[Any]:
    """
    Get tools needed for remediation and analysis phases
    This is the main function used by the troubleshooting system
    
    Returns:
        List[Any]: List of tool callables for investigation and remediation
    """
    return get_all_tools()

# Maintain backward compatibility
define_remediation_tools = get_remediation_tools
</file>

<file path="phases/rule_based_plan_generator.py">
#!/usr/bin/env python3
"""
Rule-based Plan Generator for Investigation Planning

This module contains utilities for generating investigation plans using rule-based approaches.
"""

import logging
from typing import Dict, List, Any, Set
from knowledge_graph import KnowledgeGraph
from phases.utils import validate_knowledge_graph, handle_exception

logger = logging.getLogger(__name__)

class RuleBasedPlanGenerator:
    """
    Generates preliminary investigation steps using rule-based approaches
    
    Uses predefined rules to analyze Knowledge Graph data and generate
    a limited number of critical initial investigation steps based on 
    issue severity and historical experience.
    """
    
    def __init__(self, knowledge_graph):
        """
        Initialize the Rule-based Plan Generator
        
        Args:
            knowledge_graph: KnowledgeGraph instance from Phase 0
        """
        self.kg = knowledge_graph
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        
        # Validate knowledge_graph is a KnowledgeGraph instance
        validate_knowledge_graph(self.kg, self.__class__.__name__)
    
    def generate_preliminary_steps(self, pod_name: str, namespace: str, volume_path: str,
                                  target_entities: Dict[str, str], issues_analysis: Dict[str, Any],
                                  historical_experience: List[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        Generate preliminary investigation steps based on rule-based prioritization
        
        Args:
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod
            volume_path: Path of the volume with I/O error
            target_entities: Dictionary of target entity IDs
            issues_analysis: Analysis of existing issues
            historical_experience: Historical experience data (optional)
            
        Returns:
            List[Dict[str, Any]]: List of preliminary investigation steps (1-3 steps)
        """
        try:
            # Step 1: Determine investigation priority based on issue severity and historical experience
            investigation_priority = self._determine_investigation_priority(
                issues_analysis, target_entities, historical_experience
            )
            
            # Step 2: Generate a limited set of preliminary steps (1-3) based on priorities
            preliminary_steps = self._generate_priority_steps(
                target_entities, investigation_priority, issues_analysis, volume_path, 
                max_steps=10  # Limit to 3 preliminary steps
            )
            
            return preliminary_steps
            
        except Exception as e:
            error_msg = handle_exception("generate_preliminary_steps", e, self.logger)
            return self._generate_basic_fallback_steps()
    
    def _determine_investigation_priority(self, issues_analysis: Dict[str, Any], 
                                        target_entities: Dict[str, str],
                                        historical_experience: List[Dict[str, Any]] = None) -> List[str]:
        """
        Determine investigation priority based on issue severity, target entities, and historical experience
        
        Args:
            issues_analysis: Analysis of existing issues
            target_entities: Dictionary of target entity IDs
            historical_experience: Historical experience data (optional)
            
        Returns:
            List[str]: Ordered list of investigation priorities
        """
        priorities = []

        # Add priorities from critical issues on target entities
        priorities.extend(self._get_priorities_from_critical_issues(issues_analysis, target_entities))

        # Add priorities from historical experience
        priorities.extend(self._get_priorities_from_historical_experience(historical_experience))

        # Add priorities from high severity issues
        priorities.extend(self._get_priorities_from_high_issues(issues_analysis, target_entities))
        
        # Add lower priority issues
        priorities.extend(self._get_priorities_from_medium_issues(issues_analysis))
        
        # Always include basic investigation and hardware verification as fallback
        if "basic_investigation" not in priorities:
            priorities.append("basic_investigation")
            
        if "hardware_verification" not in priorities:
            priorities.append("hardware_verification")
        
        return priorities
    
    def _get_priorities_from_historical_experience(self, historical_experience: List[Dict[str, Any]]) -> List[str]:
        """
        Extract investigation priorities from historical experience data
        
        Args:
            historical_experience: Historical experience data
            
        Returns:
            List[str]: Priorities derived from historical experience
        """
        priorities = []
        
        if not historical_experience:
            return priorities
            
        for experience in historical_experience:
            attributes = experience.get('attributes', {})
            
            # Check for diagnosis (new format) or root_cause (old format)
            root_cause = attributes.get('diagnosis', attributes.get('root_cause', '')).lower()
            
            if not root_cause:
                continue
                
            # Check for hardware-related issues
            if any(term in root_cause for term in ['hardware failure', 'disk failure', 'bad sector', 'drive failure']):
                priorities.append("hardware_verification")
            
            # Check for network-related issues
            elif any(term in root_cause for term in ['network', 'connectivity', 'timeout']):
                priorities.append("network_verification")
            
            # Check for configuration-related issues
            elif any(term in root_cause for term in ['configuration', 'misconfigured', 'permission', 'setting']):
                priorities.append("config_verification")
            
            # Check for filesystem-related issues
            elif any(term in root_cause for term in ['filesystem', 'corruption', 'xfs', 'ext4', 'mount']):
                priorities.append("filesystem_verification")
        
        return priorities
    
    def _get_priorities_from_critical_issues(self, issues_analysis: Dict[str, Any], 
                                           target_entities: Dict[str, str]) -> List[str]:
        """
        Extract investigation priorities from critical issues
        
        Args:
            issues_analysis: Analysis of existing issues
            target_entities: Dictionary of target entity IDs
            
        Returns:
            List[str]: Priorities derived from critical issues
        """
        priorities = []
        
        # High priority: Critical issues affecting target entities
        target_entity_ids = set(target_entities.values())
        critical_issues_on_targets = [
            issue for issue in issues_analysis["by_severity"]["critical"]
            if issue.get('node_id') in target_entity_ids
        ]
        
        if critical_issues_on_targets:
            priorities.append("critical_target_issues")
        
        # Medium-high priority: Critical issues on any entities  
        if issues_analysis["by_severity"]["critical"]:
            priorities.append("critical_system_issues")
            
        return priorities
    
    def _get_priorities_from_high_issues(self, issues_analysis: Dict[str, Any], 
                                       target_entities: Dict[str, str]) -> List[str]:
        """
        Extract investigation priorities from high severity issues
        
        Args:
            issues_analysis: Analysis of existing issues
            target_entities: Dictionary of target entity IDs
            
        Returns:
            List[str]: Priorities derived from high severity issues
        """
        priorities = []
        
        # Medium priority: High severity issues on target entities
        target_entity_ids = set(target_entities.values())
        high_issues_on_targets = [
            issue for issue in issues_analysis["by_severity"]["high"]
            if issue.get('node_id') in target_entity_ids
        ]
        
        if high_issues_on_targets:
            priorities.append("high_target_issues")
        
        # Lower priority: High severity issues on any entities
        if issues_analysis["by_severity"]["high"]:
            priorities.append("high_system_issues")
            
        return priorities
    
    def _get_priorities_from_medium_issues(self, issues_analysis: Dict[str, Any]) -> List[str]:
        """
        Extract investigation priorities from medium severity issues
        
        Args:
            issues_analysis: Analysis of existing issues
            
        Returns:
            List[str]: Priorities derived from medium severity issues
        """
        priorities = []
        
        if issues_analysis["by_severity"]["medium"]:
            priorities.append("medium_issues")
            
        return priorities
    
    def _generate_priority_steps(self, target_entities: Dict[str, str], 
                               priorities: List[str], issues_analysis: Dict[str, Any],
                               volume_path: str, max_steps: int = 3) -> List[Dict[str, Any]]:
        """
        Generate a limited number of high-priority investigation steps
        
        Args:
            target_entities: Dictionary of target entity IDs
            priorities: List of investigation priorities  
            issues_analysis: Analysis of existing issues
            volume_path: Volume path with I/O error
            max_steps: Maximum number of steps to generate
            
        Returns:
            List[Dict[str, Any]]: List of preliminary investigation steps
        """
        all_potential_steps = []
        
        # Add steps for each priority category
        self._add_critical_issue_steps(all_potential_steps, priorities, target_entities)
        self._add_hardware_verification_steps(all_potential_steps, priorities, target_entities)
        self._add_drive_investigation_steps(all_potential_steps, target_entities)
        self._add_volume_investigation_steps(all_potential_steps, target_entities, volume_path)
        self._add_pod_investigation_steps(all_potential_steps, target_entities)
        self._add_network_verification_steps(all_potential_steps, priorities)
        
        # Sort steps by priority score and limit to max_steps
        return self._select_and_format_steps(all_potential_steps, max_steps)
    
    def _add_critical_issue_steps(self, steps_list: List[Dict[str, Any]], 
                                priorities: List[str], target_entities: Dict[str, str]) -> None:
        """
        Add steps for critical issues analysis
        
        Args:
            steps_list: List to add steps to
            priorities: List of investigation priorities
            target_entities: Dictionary of target entity IDs
        """
        if "critical_target_issues" in priorities or "critical_system_issues" in priorities:
            steps_list.append({
                "step": None,  # Will be set later
                "description": "Get all critical issues that may be causing volume I/O errors",
                "tool": "kg_get_all_issues",
                "arguments": {"severity": "primary"},
                "expected": "List of critical issues affecting the system",
                "priority": "critical",
                "category": "issue_analysis",
                "priority_score": 150  # Highest priority
            })
    
    def _add_hardware_verification_steps(self, steps_list: List[Dict[str, Any]], 
                                       priorities: List[str], target_entities: Dict[str, str]) -> None:
        """
        Add steps for hardware verification
        
        Args:
            steps_list: List to add steps to
            priorities: List of investigation priorities
            target_entities: Dictionary of target entity IDs
        """
        if "hardware_verification" in priorities:
            node_id = target_entities.get("node", "") if "node" in target_entities else None
            if node_id == None:
                return

            tag = node_id.split(':')
            node_name = tag[-1]

            # Add comprehensive disk health check
            steps_list.append({
                "step": None,
                "description": "Check disk health status using SMART data on the affected node",
                "tool": "check_disk_health",
                "arguments": {"node_name": node_name, "device_path": "/dev/sda"},
                "expected": "Disk health assessment with key metrics and status",
                "priority": "high",
                "category": "hardware_investigation",
                "priority_score": 95
            })
            
            # Add disk error log scanning
            steps_list.append({
                "step": None,
                "description": "Scan system logs for disk-related errors on the affected node",
                "tool": "scan_disk_error_logs",
                "arguments": {"node_name": node_name, "hours_back": 24},
                "expected": "Summary of disk-related errors with actionable insights",
                "priority": "high",
                "category": "hardware_investigation",
                "priority_score": 85
            })
    
    def _add_pod_investigation_steps(self, steps_list: List[Dict[str, Any]], 
                                   target_entities: Dict[str, str]) -> None:
        """
        Add steps for pod-specific investigation
        
        Args:
            steps_list: List to add steps to
            target_entities: Dictionary of target entity IDs
        """
        if "pod" in target_entities:
            pod_id = target_entities["pod"]
            steps_list.append({
                "step": None,
                "description": f"Get detailed information about the problem pod and its current state",
                "tool": "kg_get_entity_info",
                "arguments": {"entity_type": "Pod", "entity_id": pod_id},
                "expected": "Pod configuration, status, and any detected issues",
                "priority": "critical",
                "category": "entity_investigation",
                "priority_score": 85
            })
    
    def _add_drive_investigation_steps(self, steps_list: List[Dict[str, Any]], 
                                     target_entities: Dict[str, str]) -> None:
        """
        Add steps for drive-specific investigation
        
        Args:
            steps_list: List to add steps to
            target_entities: Dictionary of target entity IDs
        """
        if "drive" in target_entities:
            drive_id = target_entities["drive"].split(":")[-1] if "drive" in target_entities else None
            if drive_id == None:
                return

            node = target_entities.get("node", "").split(":")[-1] if "node" in target_entities else None
            
            # Get drive entity information from knowledge graph
            steps_list.append({
                "step": None,
                "description": "Get detailed Drive information including health status and metrics",
                "tool": "kg_get_entity_info",
                "arguments": {"entity_type": "Drive", "entity_id": drive_id},
                "expected": "Drive health status, SMART data, and any hardware issues",
                "priority": "high",
                "category": "hardware_investigation",
                "priority_score": 88
            })
            
            # If we have the node, add disk performance testing
            if node:
                # Add disk read-only test
                steps_list.append({
                    "step": None,
                    "description": "Perform read-only test on the disk to verify readability",
                    "tool": "run_disk_readonly_test",
                    "arguments": {
                        "node_name": node,
                        "device_path": f"/dev/{drive_id}",
                        "duration_minutes": 5
                    },
                    "expected": "Disk read performance metrics and error detection",
                    "priority": "high",
                    "category": "hardware_investigation",
                    "priority_score": 85
                })
                
                # Add disk IO performance test
                steps_list.append({
                    "step": None,
                    "description": "Measure disk I/O performance under different workloads",
                    "tool": "test_disk_io_performance",
                    "arguments": {
                        "node_name": node,
                        "device_path": f"/dev/{drive_id}",
                        "test_types": ["read", "randread"],
                        "duration_seconds": 30
                    },
                    "expected": "Disk I/O performance metrics including IOPS and throughput",
                    "priority": "medium",
                    "category": "hardware_investigation",
                    "priority_score": 75
                })
                
                # Add disk jitter detection
                steps_list.append({
                    "step": None,
                    "description": "Detect intermittent online/offline jitter in disk status",
                    "tool": "detect_disk_jitter",
                    "arguments": {
                        "duration_minutes": 5,
                        "node_name": node,
                        "drive_uuid": drive_id
                    },
                    "expected": "Report on disk status stability and any detected jitter",
                    "priority": "medium",
                    "category": "hardware_investigation",
                    "priority_score": 70
                })
    
    def _add_volume_investigation_steps(self, steps_list: List[Dict[str, Any]],
                                       target_entities: Dict[str, str],
                                       volume_path: str) -> None:
        """
        Add steps for volume-specific investigation
        
        Args:
            steps_list: List to add steps to
            target_entities: Dictionary of target entity IDs
            volume_path: Path of the volume with I/O error
        """
        if "pod" in target_entities:
            pod_id = target_entities["pod"]
            tag = pod_id.split('/')
            pod_name = tag[-1]
            namespace = tag[0].split(':')[-1]  # Default namespace, could be extracted from pod_id if available
            
            # Add volume mount validation
            steps_list.append({
                "step": None,
                "description": "Verify that the pod volume is correctly mounted and accessible",
                "tool": "verify_volume_mount",
                "arguments": {
                    "pod_name": pod_name,
                    "namespace": namespace,
                    "mount_path": volume_path
                },
                "expected": "Volume mount verification with accessibility and filesystem details",
                "priority": "critical",
                "category": "volume_investigation",
                "priority_score": 92
            })
            
            # Add volume I/O test
            steps_list.append({
                "step": None,
                "description": "Run I/O tests on the volume to check for read/write errors",
                "tool": "run_volume_io_test",
                "arguments": {
                    "pod_name": pod_name,
                    "namespace": namespace,
                    "mount_path": volume_path
                },
                "expected": "Results of read/write tests on the volume",
                "priority": "high", 
                "category": "volume_investigation",
                "priority_score": 86
            })
            
            # Add volume performance test
            steps_list.append({
                "step": None,
                "description": "Test I/O performance of the pod volume including speeds and latency",
                "tool": "test_volume_io_performance",
                "arguments": {
                    "pod_name": pod_name,
                    "namespace": namespace,
                    "mount_path": volume_path,
                    "test_duration": 30
                },
                "expected": "Volume I/O performance metrics for read/write operations",
                "priority": "medium",
                "category": "volume_investigation",
                "priority_score": 80
            })
            
            # Add filesystem check
            steps_list.append({
                "step": None,
                "description": "Perform a non-destructive filesystem check on the pod volume",
                "tool": "check_pod_volume_filesystem",
                "arguments": {
                    "pod_name": pod_name,
                    "namespace": namespace,
                    "mount_path": volume_path
                },
                "expected": "Filesystem health check results and any detected issues",
                "priority": "medium",
                "category": "volume_investigation",
                "priority_score": 78
            })
            
            # Add volume space usage analysis
            steps_list.append({
                "step": None,
                "description": "Analyze volume space usage to identify potential space issues",
                "tool": "analyze_volume_space_usage",
                "arguments": {
                    "pod_name": pod_name,
                    "namespace": namespace,
                    "mount_path": volume_path
                },
                "expected": "Volume space usage analysis with directories and file patterns",
                "priority": "medium",
                "category": "volume_investigation",
                "priority_score": 75
            })
            
    def _add_network_verification_steps(self, steps_list: List[Dict[str, Any]], 
                                      priorities: List[str]) -> None:
        """
        Add steps for network verification
        
        Args:
            steps_list: List to add steps to
            priorities: List of investigation priorities
        """
        if "network_verification" in priorities:
            steps_list.append({
                "step": None,
                "description": "Check network connectivity between pod and storage",
                "tool": "kg_query_relationships",
                "arguments": {"source": "pod", "target": "network"},
                "expected": "Network connectivity and health status",
                "priority": "high",
                "category": "network_investigation",
                "priority_score": 75
            })
    
    def _select_and_format_steps(self, all_potential_steps: List[Dict[str, Any]], 
                               max_steps: int) -> List[Dict[str, Any]]:
        """
        Select and format steps based on priority score
        
        Args:
            all_potential_steps: List of all potential steps
            max_steps: Maximum number of steps to select
            
        Returns:
            List[Dict[str, Any]]: Selected and formatted steps
        """
        # Sort steps by priority score
        all_potential_steps.sort(key=lambda x: x.get("priority_score", 0), reverse=True)
        
        # Take only the top max_steps steps
        selected_steps = all_potential_steps[:max_steps]
        
        # Format steps with step numbers
        formatted_steps = []
        for i, step in enumerate(selected_steps, 1):
            step["step"] = i
            formatted_steps.append(step)
        
        return formatted_steps
    
    def _generate_basic_fallback_steps(self) -> List[Dict[str, Any]]:
        """
        Generate basic fallback steps when all else fails
        
        Returns:
            List[Dict[str, Any]]: List of basic fallback steps
        """
        return [{
            "step": 1,
            "description": "Get all critical issues from Knowledge Graph",
            "tool": "kg_get_all_issues",
            "arguments": {"severity": "primary"},
            "expected": "List of critical issues affecting the system",
            "priority": "critical",
            "category": "issue_analysis"
        }]
</file>

<file path="troubleshooting/troubleshoot.py">
#!/usr/bin/env python3
"""
Kubernetes Volume I/O Error Troubleshooting Script with Phase 0 Information Collection

This script uses a 3-phase approach:
- Phase 0: Information Collection - Pre-collect all diagnostic data upfront
- Phase 1: ReAct Investigation - Actively investigate using tools with pre-collected data as base knowledge
- Phase 2: Remediation - Execute fix plan based on analysis

Enhanced with Knowledge Graph integration and ReAct methodology for comprehensive root cause analysis.
"""

import os
import sys
import yaml
import logging
import asyncio
import time
import json
import argparse
from typing import Dict, List, Any, Optional, Tuple
from langgraph.graph import StateGraph
from kubernetes import config
#from troubleshooting.graph import create_troubleshooting_graph_with_context
from information_collector import ComprehensiveInformationCollector
from phases import (
    run_information_collection_phase,
    run_plan_phase,
    run_analysis_phase_with_plan,
    run_remediation_phase
)
import tempfile
import json
import os
from phases.chat_mode import ChatMode
from tools.core.mcp_adapter import initialize_mcp_adapter, get_mcp_adapter
from rich.logging import RichHandler
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, TimeElapsedColumn
from rich.panel import Panel
from rich.table import Table
from rich.tree import Tree
from rich import print as rprint

# Initialize rich console
console = Console()
file_console = Console(file=open('troubleshoot.log', 'w'))

# Custom filter for internal logging
class InternalLogFilter(logging.Filter):
    """Filter out internal logging messages from console output"""
    def filter(self, record):
        # Check if this is an internal log that should be filtered from console
        
        # Filter out logs from LangGraph module
        if 'LangGraph' in record.msg or getattr(record, 'name', '').startswith('langgraph'):
            return False
            
        # Filter out logs from standard logging modules
        if getattr(record, 'name', '') in ['kubernetes', 'urllib3', 'asyncio', 'langchain', 'httpx']:
            return False
            
        # Filter out specific log patterns that are meant for internal debugging
        internal_patterns = [
            'Executing command:',
            'Command output:',
            'Starting enhanced logging',
            'Loaded',
            'Building',
            'Adding node',
            'Adding edge',
            'Adding conditional',
            'Compiling graph',
            'Model response:',
            'Model invoking tool:',
            'Tool arguments:',
            'Using Phase',
            'Processing state',
            'Graph compilation',
            'Running',
            'Starting'
        ]
        
        if any(pattern in record.msg for pattern in internal_patterns):
            return False
            
        # Allow all other logs to pass to console
        return True

# Set up logging handlers for module-specific loggers
def configure_module_loggers():
    """Set up file handlers for module-specific loggers"""
    # Create file handler for log file
    file_handler = logging.FileHandler('troubleshoot.log', mode='a')
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    
    # Configure langgraph logger
    langgraph_logger = logging.getLogger('langgraph')
    langgraph_logger.addHandler(file_handler)
    
    # Configure knowledge_graph logger
    kg_logger = logging.getLogger('knowledge_graph')
    kg_logger.addHandler(file_handler)
    
    # Configure knowledge_graph.tools logger
    kg_tools_logger = logging.getLogger('knowledge_graph.tools')
    kg_tools_logger.addHandler(file_handler)
    
    # Configure other module loggers as needed
    for module in ['tools', 'information_collector', 'kubernetes', 'urllib3']:
        module_logger = logging.getLogger(module)
        module_logger.addHandler(file_handler)

# Global variables
CONFIG_DATA = None
INTERACTIVE_MODE = False
SSH_CLIENTS = {}
KNOWLEDGE_GRAPH = None
RESULTS_DIR = os.path.join(tempfile.gettempdir(), "k8s-troubleshooting-results")

#os.environ['LANGCHAIN_TRACING_V2'] = "true"   
#os.environ['LANGCHAIN_ENDPOINT'] = "https://api.smith.langchain.com"   
#os.environ['LANGCHAIN_API_KEY'] = "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"   
#os.environ['LANGCHAIN_PROJECT'] = "pr-silver-bank-1"

def load_config():
    """Load configuration from config.yaml"""
    try:
        with open('config.yaml', 'r') as f:
            return yaml.safe_load(f)
    except Exception as e:
        logging.error(f"Failed to load configuration: {e}")
        sys.exit(1)

def setup_results_dir():
    """Set up the directory for storing troubleshooting results"""
    try:
        if not os.path.exists(RESULTS_DIR):
            os.makedirs(RESULTS_DIR)
            logging.debug(f"Created results directory: {RESULTS_DIR}")
    except Exception as e:
        logging.error(f"Failed to create results directory: {e}")

def write_investigation_result(pod_name, namespace, volume_path, result_summary):
    """
    Write investigation result to a file for the monitor to pick up
    
    Args:
        pod_name: Name of the pod
        namespace: Namespace of the pod
        volume_path: Path of the volume
        result_summary: Summary of the investigation result
    """
    try:
        # Create a unique filename based on pod details
        filename = f"{namespace}_{pod_name}_{volume_path.replace('/', '_')}.json"
        filepath = os.path.join(RESULTS_DIR, filename)
        
        # Create a result object
        result_data = {
            "pod_name": pod_name,
            "namespace": namespace,
            "volume_path": volume_path,
            "timestamp": time.time(),
            "result_summary": result_summary
        }
        
        # Write to file
        with open(filepath, 'w') as f:
            json.dump(result_data, f)
            
        logging.info(f"Investigation result written to {filepath}")
    except Exception as e:
        logging.error(f"Failed to write investigation result: {e}")

def setup_logging(config_data):
    """Configure logging based on configuration with rich formatting"""
    log_file = config_data['logging']['file']
    log_to_stdout = config_data['logging']['stdout']
    
    handlers = []
    if log_file:
        # File handler for regular log file with timestamps
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s'
        ))
        handlers.append(file_handler)
        
        # Rich console file handler for enhanced log file
        file_console.log("Starting enhanced logging to troubleshoot.log")
    
    if log_to_stdout:
        # Rich console handler for beautiful terminal output - with filter
        rich_handler = RichHandler(
            rich_tracebacks=True,
            console=console,
            show_time=True,
            show_level=True,
            show_path=False,
            enable_link_path=False
        )
        # Add filter to prevent internal logs from going to console
        rich_handler.addFilter(InternalLogFilter())
        handlers.append(rich_handler)
    
    # Configure the root logger
    logging.basicConfig(
        level=logging.INFO,
        format='%(message)s',
        datefmt='[%X]',
        handlers=handlers
    )
    
    # Configure module-specific loggers
    configure_module_loggers()
    
    # Log startup message to file only
    logging.info("Logging initialized - internal logs redirected to log file only")

async def run_information_collection_phase_wrapper(pod_name: str, namespace: str, volume_path: str) -> Dict[str, Any]:
    """
    Wrapper for Phase 0: Information Collection from phases module
    
    Args:
        pod_name: Name of the pod with the error
        namespace: Namespace of the pod
        volume_path: Path of the volume with I/O error
        
    Returns:
        Dict[str, Any]: Pre-collected diagnostic information
    """
    global CONFIG_DATA, KNOWLEDGE_GRAPH
    
    # Call the actual implementation from phases module
    collected_info = await run_information_collection_phase(pod_name, namespace, volume_path, CONFIG_DATA)
    
    # Update the global knowledge graph
    KNOWLEDGE_GRAPH = collected_info.get('knowledge_graph')
    
    # Initialize the Knowledge Graph in the tools module with the one from Phase0
    if KNOWLEDGE_GRAPH:
        from tools.core.knowledge_graph import initialize_knowledge_graph
        initialize_knowledge_graph(KNOWLEDGE_GRAPH)
        logging.info("Knowledge Graph from Phase0 initialized for tools")
    else:
        logging.warning("No Knowledge Graph available from Phase0")
    
    return collected_info

async def run_analysis_phase_wrapper(pod_name: str, namespace: str, volume_path: str, 
                                  collected_info: Dict[str, Any], investigation_plan: str,
                                  message_list: List[Dict[str, str]] = None) -> Tuple[str, bool, str, List[Dict[str, str]]]:
    """
    Wrapper for Phase 1: ReAct Investigation from phases module
    
    Args:
        pod_name: Name of the pod with the error
        namespace: Namespace of the pod
        volume_path: Path of the volume with I/O error
        collected_info: Pre-collected diagnostic information from Phase 0
        investigation_plan: Investigation Plan generated by the Plan Phase
        message_list: Optional message list for chat mode
        
    Returns:
        Tuple[str, bool, List[Dict[str, str]]]: (Analysis result, Skip Phase2 flag, Updated message list)
    """
    global CONFIG_DATA

    # Call the actual implementation from phases module
    return await run_analysis_phase_with_plan(
        pod_name, namespace, volume_path, collected_info, investigation_plan, CONFIG_DATA, message_list
    )

async def run_remediation_phase_wrapper(phase1_final_response: str, collected_info: Dict[str, Any], 
                                      message_list: List[Dict[str, str]] = None) -> Tuple[str, List[Dict[str, str]]]:
    """
    Wrapper for Phase 2: Remediation from phases module
    
    Args:
        phase1_final_response: Response from Phase 1 containing root cause and fix plan
        collected_info: Pre-collected diagnostic information from Phase 0
        message_list: Optional message list for chat mode
        
    Returns:
        Tuple[str, List[Dict[str, str]]]: (Remediation result, Updated message list)
    """
    global CONFIG_DATA
    
    # Call the actual implementation from phases module
    return await run_remediation_phase(phase1_final_response, collected_info, CONFIG_DATA, message_list)

async def run_comprehensive_troubleshooting(pod_name: str, namespace: str, volume_path: str) -> Dict[str, Any]:
    """
    Run comprehensive 3-phase troubleshooting
    
    Args:
        pod_name: Name of the pod with the error
        namespace: Namespace of the pod
        volume_path: Path of the volume with I/O error
        
    Returns:
        Dict[str, Any]: Complete troubleshooting results
    """
    global CONFIG_DATA, KNOWLEDGE_GRAPH

    start_time = time.time()
    
    results = {
        "pod_name": pod_name,
        "namespace": namespace,
        "volume_path": volume_path,
        "start_time": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(start_time)),
        "phases": {}
    }

    try:
        # Phase 0: Information Collection
        console.print("\n")
        console.print(Panel(
            f"[bold white]Starting troubleshooting for Pod: [green]{namespace}/{pod_name}[/green]\n"
            f"Volume Path: [blue]{volume_path}[/blue]\n"
            f"Start Time: [yellow]{results['start_time']}[/yellow]",
            title="[bold cyan]KUBERNETES VOLUME TROUBLESHOOTING",
            border_style="cyan",
            padding=(1, 2)
        ))
        
        collected_info = await run_information_collection_phase_wrapper(pod_name, namespace, volume_path)
        results["phases"]["phase_0_collection"] = {
            "status": "completed",
            "summary": collected_info.get("knowledge_graph_summary", {}),
            "duration": time.time() - start_time
        }
        
        if "collection_error" in collected_info:
            results["phases"]["phase_0_collection"]["status"] = "failed"
            results["phases"]["phase_0_collection"]["error"] = collected_info["collection_error"]
            return results
        
        # Add Knowledge Graph to collected_info for Plan Phase
        collected_info["knowledge_graph"] = KNOWLEDGE_GRAPH
        
        plan_phase_start = time.time()
        
        # Plan Phase: Generate Investigation Plan
        try:
            # Initialize chat mode
            chat_mode_enabled = CONFIG_DATA.get('chat_mode', {}).get('enabled', True)
            chat_mode_entry_points = CONFIG_DATA.get('chat_mode', {}).get('entry_points', ["plan_phase", "phase1"])
            
            # Create chat mode instance
            chat_mode = ChatMode()
            # Skip chat mode if disabled or plan_phase entry point not enabled
            if not chat_mode_enabled or "plan_phase" not in chat_mode_entry_points:
                investigation_plan, plan_phase_message_list = await run_plan_phase(
                    pod_name, namespace, volume_path, collected_info, CONFIG_DATA
                )
                
                # Print the Investigation Plan to the console
                console.print("\n")
                console.print(Panel(
                    f"[bold white]{investigation_plan}",
                    title="[bold blue]INVESTIGATION PLAN",
                    border_style="blue",
                    padding=(1, 2)
                ))
                
                results["phases"]["plan_phase"] = {
                    "status": "completed",
                    "investigation_plan": investigation_plan[:5000] + "..." if len(investigation_plan) > 5000 else investigation_plan,
                    "duration": time.time() - plan_phase_start
                }
            else:
                # Initialize message list for Plan Phase
                plan_phase_message_list = None
                investigation_plan = None
                exit_flag = False
                
                while True:
                    # Generate or regenerate Investigation Plan
                    investigation_plan, plan_phase_message_list = await run_plan_phase(
                        pod_name, namespace, volume_path, collected_info, CONFIG_DATA, plan_phase_message_list
                    )
                    
                    # Print the Investigation Plan to the console
                    console.print(Panel(
                        f"[bold white]{investigation_plan}",
                        title="[bold blue]INVESTIGATION PLAN",
                        border_style="blue",
                        padding=(1, 2)
                    ))
                    
                    # Enter chat mode after Plan Phase
                    plan_phase_message_list, exit_flag = chat_mode.chat_after_plan_phase(
                        plan_phase_message_list, investigation_plan
                    )
                    
                    # Exit if requested
                    if exit_flag:
                        console.print("[bold red]Exiting program as requested by user[/bold red]")
                        sys.exit(0)
                    
                    # Break loop if user approved the plan
                    if plan_phase_message_list[-1]["role"] != "user":
                        break
            
            results["phases"]["plan_phase"] = {
                "status": "completed",
                "investigation_plan": investigation_plan[:5000] + "..." if len(investigation_plan) > 5000 else investigation_plan,  # Truncate for summary
                "duration": time.time() - plan_phase_start
            }
            
        except Exception as e:
            error_msg = f"Error during Plan Phase: {str(e)}"
            logging.error(error_msg)
            results["phases"]["plan_phase"] = {
                "status": "failed",
                "error": error_msg,
                "duration": time.time() - plan_phase_start
            }
            # Use a basic fallback plan
            investigation_plan = f"""Investigation Plan:
Target: Pod {namespace}/{pod_name}, Volume Path: {volume_path}
Generated Steps: 3 fallback steps (Plan Phase failed)

Step 1: Get all critical issues | Tool: kg_get_all_issues(severity='primary') | Expected: Critical issues in the system
Step 2: Analyze issue patterns | Tool: kg_analyze_issues() | Expected: Root cause analysis and patterns
Step 3: Get system overview | Tool: kg_get_summary() | Expected: Overall system health statistics

Fallback Steps (if main steps fail):
Step F1: Print Knowledge Graph | Tool: kg_print_graph(include_details=True, include_issues=True) | Expected: Complete system visualization | Trigger: plan_phase_failed
"""

        phase_1_start = time.time()
        
        # Initialize message list for Phase1
        phase1_message_list = None
        phase1_final_response = None
        exit_flag = False
        skip_phase2 = False
        
        # Skip chat mode if disabled or phase1 entry point not enabled
        if not chat_mode_enabled or "phase1" not in chat_mode_entry_points:
            # Run Phase1 analysis without chat mode
            phase1_final_response, skip_phase2, summary, phase1_message_list = await run_analysis_phase_wrapper(
                pod_name, namespace, volume_path, collected_info, investigation_plan
            )
            
            # Print the Fix Plan to the console
            console.print("\n")
            console.print(Panel(
                f"[bold white]{phase1_final_response}",
                title="[bold blue]FIX PLAN",
                border_style="blue",
                padding=(1, 2)
            ))

            console.print(Panel(
                f"[bold white]{summary}",
                title="[bold magenta]Event Summary",
                border_style="magenta",
                padding=(1, 2)
            ))

            results["phases"]["phase_1_analysis"] = {
                "status": "completed",
                "final_response": str(phase1_final_response),
                "duration": time.time() - phase_1_start,
                "skip_phase2": "true" if skip_phase2 else "false"
            }
        else:
            # Use chat mode for Phase1
            while True:
                # Run Phase1 analysis
                phase1_final_response, skip_phase2, summary, phase1_message_list = await run_analysis_phase_wrapper(
                    pod_name, namespace, volume_path, collected_info, investigation_plan, phase1_message_list
                )
                
                # Print the Fix Plan to the console
                console.print(Panel(
                    f"[bold white]{phase1_final_response}",
                    title="[bold blue]FIX PLAN",
                    border_style="blue",
                    padding=(1, 2)
                ))

                console.print(Panel(
                    f"[bold white]{summary}",
                    title="[bold magenta]Event Summary",
                    border_style="magenta",
                    padding=(1, 2)
                ))
                # Enter chat mode after Phase1
                phase1_message_list, exit_flag = chat_mode.chat_after_phase1(
                    phase1_message_list, phase1_final_response
                )
                
                # Exit if requested
                if exit_flag:
                    console.print("[bold red]Exiting program as requested by user[/bold red]")
                    sys.exit(0)
                
                # Break loop if user approved the plan
                if phase1_message_list[-1]["role"] != "user":
                    break
            
            results["phases"]["phase_1_analysis"] = {
                "status": "completed",
                "final_response": str(phase1_final_response),
                "duration": time.time() - phase_1_start,
                "skip_phase2": "true" if skip_phase2 else "false"
            }
        
        # Only proceed to Phase 2 if not skipped
        remediation_result = None
        skip_phase2 = True
        if not skip_phase2:
            phase_2_start = time.time()
            
            remediation_result, _ = await run_remediation_phase_wrapper(phase1_final_response, collected_info, phase1_message_list)
            
            results["phases"]["phase_2_remediation"] = {
                "status": "completed",
                "result": remediation_result,
                "duration": time.time() - phase_2_start
            }
        else:
            # Phase 2 skipped - add to results
            console.print("\n")
            console.print(Panel(
                "[bold white]Phase 2 skipped - no remediation needed or manual intervention required",
                title="[bold yellow]PHASE 2: SKIPPED",
                border_style="yellow",
                padding=(1, 2)
            ))
            results["phases"]["phase_2_remediation"] = {
                "status": "skipped",
                "result": "Phase 2 skipped - no remediation needed or manual intervention required",
                "reason": "No issues detected or manual intervention required",
                "duration": 0
            }

        # Final summary
        total_duration = time.time() - start_time
        results["total_duration"] = total_duration
        results["status"] = "completed"

        # Create a rich formatted summary table
        summary_table = Table(
            title="[bold]TROUBLESHOOTING SUMMARY",
            #show_header=True,
            header_style="bold cyan",
            #box=True,
            border_style="blue",
            #safe_box=True  # Explicitly set safe_box to True
        )

        # Add columns
        summary_table.add_column("Phase", style="dim")
        summary_table.add_column("Duration", justify="right")
        summary_table.add_column("Status", justify="center")
        
        # Add rows for each phase
        summary_table.add_row(
            "Phase 0: Information Collection", 
            f"{results['phases']['phase_0_collection']['duration']:.2f}s",
            "[green]Completed"
        )
        # Add Plan Phase row
        plan_phase_status = "[green]Completed" if results["phases"].get("plan_phase", {}).get("status") == "completed" else "[red]Failed"
        plan_phase_duration = results["phases"].get("plan_phase", {}).get("duration", 0)
        summary_table.add_row(
            "Plan Phase: Investigation Plan", 
            f"{plan_phase_duration:.2f}s",
            plan_phase_status
        )
        summary_table.add_row(
            "Phase 1: ReAct Investigation", 
            f"{results['phases']['phase_1_analysis']['duration']:.2f}s",
            "[green]Completed"
        )
        
        # Add Phase 2 row with appropriate status
        if results["phases"]["phase_2_remediation"]["status"] == "completed":
            summary_table.add_row(
                "Phase 2: Remediation", 
                f"{results['phases']['phase_2_remediation']['duration']:.2f}s",
                "[green]Completed"
            )
        else:
            summary_table.add_row(
                "Phase 2: Remediation", 
                "0.00s",
                "[yellow]Skipped"
            )
        summary_table.add_row(
            "Total", 
            f"{total_duration:.2f}s", 
            "[bold green]Completed"
        )
        # Create root cause and resolution panels - ensure strings for content
        # Convert values to strings first to avoid 'bool' has no attribute 'substitute' errors
        root_cause_str = phase1_final_response if phase1_final_response is not None else "Unknown"
        remediation_result_str = remediation_result if remediation_result is not None else "No result"
        root_cause_panel = Panel(
            f"[bold yellow]{root_cause_str}",
            title="[bold red]Root Cause",
            border_style="red",
            padding=(1, 2),
            safe_box=True  # Explicitly set safe_box to True
        )
        resolution_panel = Panel(
            f"[bold green]{remediation_result_str}",
            title="[bold blue]Resolution Status",
            border_style="green",
            padding=(1, 2),
            safe_box=True  # Explicitly set safe_box to True
        )

        try:
            console.print(summary_table)
        except Exception as e:
            console.print(f"Error printing rich summary table: {e}")

        console.print("\n")
        console.print(root_cause_panel)
        console.print("\n")
        console.print(resolution_panel)
        
        return results
        
    except Exception as e:
        error_msg = f"Critical error during troubleshooting: {str(e)}"
        logging.error(error_msg)
        results["status"] = "failed"
        results["error"] = error_msg
        results["total_duration"] = time.time() - start_time
        return results

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description="Kubernetes Volume I/O Error Troubleshooting Script")
    parser.add_argument("pod_name", help="Name of the pod with the error")
    parser.add_argument("namespace", help="Namespace of the pod")
    parser.add_argument("volume_path", help="Path of the volume with I/O error")
    parser.add_argument("--interactive", "-i", action="store_true", 
                       help="Enable interactive mode for command confirmation")
    parser.add_argument("--config", "-c", default="config.yaml",
                       help="Path to configuration file (default: config.yaml)")
    parser.add_argument("--output", "-o", help="Output file for results (JSON format)")
    parser.add_argument("--verbose", "-v", action="store_true",
                       help="Enable verbose logging")
    
    return parser.parse_args()

async def main():
    """Main function"""
    global CONFIG_DATA, INTERACTIVE_MODE, KNOWLEDGE_GRAPH
    
    try:
        # Parse arguments
        args = parse_arguments()
        
        # Set interactive mode
        INTERACTIVE_MODE = args.interactive
        
        # Load configuration
        CONFIG_DATA = load_config()

        # Setup logging and results directory
        setup_logging(CONFIG_DATA)
        setup_results_dir()
        
        if args.verbose:
            logging.getLogger().setLevel(logging.DEBUG)
        
        # Validate inputs
        if not args.pod_name or not args.namespace or not args.volume_path:
            logging.error("Pod name, namespace, and volume path are required")
            sys.exit(1)
        llm_provider = CONFIG_DATA.get("llm").get("provider")
        current_api_key = CONFIG_DATA.get("llm").get(llm_provider, "openai").get("api_key")
        if len(current_api_key) < 10:
            logging.error("AI key is empty!")
            sys.exit(1)

        # Initialize MCP adapter
        mcp_adapter = await initialize_mcp_adapter(CONFIG_DATA)

        # Initialize Kubernetes configuration
        try:
            config.load_incluster_config()
            logging.info("Loaded in-cluster Kubernetes configuration")
        except:
            try:
                config.load_kube_config()
                logging.info("Loaded kubeconfig from default location")
            except Exception as e:
                logging.error(f"Failed to load Kubernetes configuration: {e}")
                sys.exit(1)
        
        # Run comprehensive troubleshooting
        results = await run_comprehensive_troubleshooting(
            args.pod_name, args.namespace, args.volume_path
        )
        
        # Save results if output file specified
        if args.output:
            try:
                with open(args.output, 'w') as f:
                    json.dump(results, f, indent=2)
                logging.info(f"Results saved to {args.output}")
            except Exception as e:
                logging.error(f"Failed to save results to {args.output}: {e}")
        
        # Exit with appropriate code
        if results["status"] == "completed":
            sys.exit(0)
        else:
            sys.exit(1)
            
    except KeyboardInterrupt:
        logging.info("Troubleshooting interrupted by user")
        sys.exit(130)
    except Exception as e:
        logging.error(f"Critical error in main: {str(e)}")
        sys.exit(1)
    finally:
        # Clean up SSH connections
        for client in SSH_CLIENTS.values():
            try:
                client.close()
            except:
                pass
                
        # Clean up MCP connections
        mcp_adapter = get_mcp_adapter()
        if mcp_adapter:
            await mcp_adapter.close()

if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="information_collector/knowledge_builder.py">
"""
Knowledge Graph Builder

Contains methods for building enhanced Knowledge Graph from tool outputs.
"""

import yaml
import logging
import re
from typing import Dict, List, Any, Optional, Tuple
from .base import InformationCollectorBase
from .metadata_parsers import MetadataParsers


class KnowledgeBuilder(MetadataParsers):
    """Knowledge Graph construction from tool outputs with rich CSI metadata"""
    
    # Constants for event parsing
    EVENT_SEVERITY_LEVELS = {
        'Warning': 'medium',
        'Error': 'high',
        'Failed': 'high',
        'Normal': 'low'
    }
    
    async def _build_knowledge_graph_from_tools(self, 
                                               target_pod: str = None, 
                                               target_namespace: str = None,
                                               target_volume_path: str = None,
                                               volume_chain: Dict[str, List[str]] = None) -> 'KnowledgeGraph':
        """Build enhanced Knowledge Graph from tool outputs with rich CSI metadata"""
        logging.info("Building Knowledge Graph from tool outputs with CSI metadata...")
        
        # Reset Knowledge Graph
        self.knowledge_graph = self.knowledge_graph.__class__()
        
        # Load historical experience data
        await self._load_historical_experience()
        
        # Process kubectl describe data for all resources
        await self._process_kubectl_describe_data()
        
        # Process target pod first if specified
        if target_pod and target_namespace:
            # Extract pod metadata
            pod_metadata = self._parse_pod_metadata(target_pod, target_namespace)
            
            # Enrich pod metadata with describe data if available
            pod_describe = self.collected_data.get('kubernetes', {}).get('target_pod_describe', '')
            if pod_describe:
                describe_attributes = self._parse_pod_describe_data(pod_describe)
                pod_metadata.update(describe_attributes)
            
            pod_id = self.knowledge_graph.add_gnode_pod(
                target_pod, target_namespace,
                volume_path=target_volume_path or '',
                is_target=True,
                **pod_metadata
            )
            
            # Add pod logs as issues if they contain errors
            pod_logs = self.collected_data.get('logs', {}).get('target_pod_logs', '')
            if pod_logs and ('error' in pod_logs.lower() or 'failed' in pod_logs.lower()):
                self.knowledge_graph.add_issue(
                    pod_id,
                    "pod_error",
                    "Pod logs contain error messages",
                    "medium"
                )
                
            # Add events from describe data as issues
            if pod_describe:
                events = self._extract_events_from_describe(pod_describe)
                self._add_events_as_issues(pod_id, events)
        
        # Process volume chain entities with enhanced metadata
        if volume_chain:
            # Add PVCs with metadata
            for pvc_key in volume_chain.get('pvcs', []):
                namespace, name = pvc_key.split('/', 1)
                pvc_metadata = self._parse_pvc_metadata(name, namespace)
                
                # Enrich PVC metadata with describe data if available
                pvc_describe = self.collected_data.get('describe', {}).get('pvcs', '')
                if pvc_describe:
                    describe_attributes = self._parse_pvc_describe_data(pvc_describe)
                    pvc_metadata.update(describe_attributes)
                
                pvc_id = self.knowledge_graph.add_gnode_pvc(name, namespace, **pvc_metadata)
                
                if pvc_metadata['AccessModes'] not in ['ReadWriteOnce', 'ReadWriteMany']:
                    self.knowledge_graph.add_issue(
                        pvc_id,
                        "pvc_access_mode",
                        f"PVC access mode issue: {pvc_metadata['AccessModes']}",
                        "critical" if pvc_metadata['AccessModes'] == 'ReadOnlyMany' else "high"
                    )

                # Link to target pod if it exists
                if target_pod and target_namespace:
                    pod_id = f"gnode:Pod:{target_namespace}/{target_pod}"
                    self.knowledge_graph.add_relationship(pod_id, pvc_id, "uses")
                    
                # Add events from describe data as issues
                if pvc_describe:
                    events = self._extract_events_from_describe(pvc_describe)
                    self._add_events_as_issues(pvc_id, events)
            
            # Add PVs with metadata
            for pv_name in volume_chain.get('pvs', []):
                pv_metadata = self._parse_pv_metadata(pv_name)
                
                # Enrich PV metadata with describe data if available
                pv_describe = self.collected_data.get('describe', {}).get('pvs', '')
                if pv_describe:
                    describe_attributes = self._parse_pv_describe_data(pv_describe)
                    pv_metadata.update(describe_attributes)
                
                pv_id = self.knowledge_graph.add_gnode_pv(pv_name, **pv_metadata)
                
                # Link to PVCs
                for pvc_key in volume_chain.get('pvcs', []):
                    pvc_id = f"gnode:PVC:{pvc_key}"
                    self.knowledge_graph.add_relationship(pvc_id, pv_id, "bound_to")
                    
                # Add events from describe data as issues
                if pv_describe:
                    events = self._extract_events_from_describe(pv_describe)
                    self._add_events_as_issues(pv_id, events)

            # Add Volumes with metadata
            volume_chain_id = None
            for vol_name in volume_chain.get('volumes', []):
                vol_metadata = self._parse_vol_metadata(vol_name)
                
                # Enrich Volume metadata with describe data if available
                vol_describe = self.collected_data.get('describe', {}).get('volumes', '')
                if vol_describe:
                    describe_attributes = self._parse_volume_describe_data(vol_describe)
                    vol_metadata.update(describe_attributes)
                
                volume_chain_id = self.knowledge_graph.add_gnode_volume(vol_name, target_namespace, **vol_metadata)
                
                if vol_metadata.get('Health') not in ['GOOD']:
                    self.knowledge_graph.add_issue(
                        volume_chain_id,
                        "volume_health",
                        f"Volume health issue: {vol_metadata.get('Health')}",
                        "critical"
                    )

                if vol_metadata.get('CSIStatus') in ['FAILED']:
                    self.knowledge_graph.add_issue(
                        volume_chain_id,
                        "volume_health",
                        f"Volume CSIStatus issue: {vol_metadata.get('CSIStatus')}",
                        "critical"
                    )

                if vol_metadata.get('OperationalStatus') not in ['OPERATIVE']:
                    self.knowledge_graph.add_issue(
                        volume_chain_id,
                        "volume_health",
                        f"Volume OperationalStatus issue: {vol_metadata.get('OperationalStatus')}",
                        "critical"
                    )

                self.knowledge_graph.add_relationship(pvc_id, volume_chain_id, "bound_to")
                
                # Add events from describe data as issues
                if vol_describe:
                    events = self._extract_events_from_describe(vol_describe)
                    self._add_events_as_issues(volume_chain_id, events)

            # Add drives with comprehensive CSI metadata
            drive_id = None
            for drive_uuid in volume_chain.get('drives', []):
                drive_info = self._parse_comprehensive_drive_info(drive_uuid)
                
                # Enrich Drive metadata with describe data if available
                drive_describe = self.collected_data.get('describe', {}).get('drives', '')
                if drive_describe:
                    describe_attributes = self._parse_drive_describe_data(drive_describe)
                    drive_info.update(describe_attributes)
                
                drive_id = self.knowledge_graph.add_gnode_drive(drive_uuid, **drive_info)

                # Link drives to volume chains
                if volume_chain_id:
                    self.knowledge_graph.add_relationship(volume_chain_id, drive_id, "bound_to")

                # Add issues for unhealthy drives
                if drive_info.get('Health') not in ['GOOD']:
                    self.knowledge_graph.add_issue(
                        drive_id,
                        "disk_health",
                        f"Drive health issue: {drive_info.get('Health')}",
                        "critical" if drive_info.get('Health') == 'BAD' else "high"
                    )

                if drive_info.get('Usage') not in ['IN_USE']:
                    self.knowledge_graph.add_issue(
                        drive_id,
                        "disk_health",
                        f"Drive usage issue: {drive_info.get('Usage')}",
                        "critical"
                    )

                if drive_info.get('Status') not in ['ONLINE']:
                    self.knowledge_graph.add_issue(
                        drive_id,
                        "disk_health",
                        f"Drive status issue: {drive_info.get('Status')}",
                        "critical"
                    )
        
                # Add issues for system drives under high usage
                if drive_info.get('IsSystem') and drive_info.get('Usage') == 'IN_USE':
                    self.knowledge_graph.add_issue(
                        drive_id,
                        "system_drive_usage",
                        "System drive is in heavy use",
                        "medium"
                    )
                
                # Link to PVs
                for pv_name in volume_chain.get('pvs', []):
                    pv_id = f"gnode:PV:{pv_name}"
                    self.knowledge_graph.add_relationship(pv_id, drive_id, "maps_to")
                    
                # Add events from describe data as issues
                if drive_describe:
                    events = self._extract_events_from_describe(drive_describe)
                    self._add_events_as_issues(drive_id, events)
            
            # Add nodes with enhanced metadata
            for node_name in volume_chain.get('nodes', []):
                node_info = self._parse_comprehensive_node_info(node_name)
                
                # Enrich Node metadata with describe data if available
                node_describe = self.collected_data.get('describe', {}).get('nodes', '')
                if node_describe:
                    describe_attributes = self._parse_node_describe_data(node_describe)
                    node_info.update(describe_attributes)
                
                node_id = self.knowledge_graph.add_gnode_node(node_name, **node_info)
                
                # Add issues for unhealthy nodes
                if not node_info.get('Ready') or node_info.get('DiskPressure'):
                    self.knowledge_graph.add_issue(
                        node_id,
                        "node_health",
                        f"Node issues: Ready={node_info.get('Ready')}, DiskPressure={node_info.get('DiskPressure')}",
                        "high" if not node_info.get('Ready') else "medium"
                    )
                
                # Link drives to nodes
                if drive_id:
                    self.knowledge_graph.add_relationship(drive_id, node_id, "located_on")
                    self.knowledge_graph.add_relationship(node_id, drive_id, "related_to")
        
                # Link to target pod if it exists
                if target_pod and target_namespace:
                    pod_id = f"gnode:Pod:{target_namespace}/{target_pod}"
                    self.knowledge_graph.add_relationship(pod_id, node_id, "located_on")
                    self.knowledge_graph.add_relationship(node_id, pod_id, "related_to")
                    
                # Add events from describe data as issues
                if node_describe:
                    events = self._extract_events_from_describe(node_describe)
                    self._add_events_as_issues(node_id, events)
                
        # Add CSI Baremetal specific entities
        await self._add_csi_baremetal_entities()
        
        # Add all collected drives and nodes (including cluster nodes)
        await self._add_all_drives_and_nodes()
        
        # Add Volume entities based on PVCs
        await self._add_volume_entities()
        
        # Create enhanced CSI relationships
        await self._create_enhanced_csi_relationships()
        
        # Add System entities (logs, SMART data)
        await self._add_system_entities()
        
        # Add log-based issues to knowledge graph
        await self._add_log_based_issues()
        
        # Add SMART data analysis
        await self._add_smart_data_analysis()
        
        # Add enhanced log analysis
        await self._add_enhanced_log_analysis()
        
        # Log final summary
        summary = self.knowledge_graph.get_summary()
        logging.info(f"Enhanced Knowledge Graph built: {summary['total_nodes']} nodes, "
                    f"{summary['total_edges']} edges, {summary['total_issues']} issues")
        
        return self.knowledge_graph
    
    async def _add_csi_baremetal_entities(self):
        """Add CSI Baremetal specific entities to knowledge graph"""
        try:
            # Add logical volume groups
            lvg_output = self.collected_data.get('csi_baremetal', {}).get('lvgs', '')
            if lvg_output:
                self._process_lvg_entities(lvg_output)
            
            # Add available capacity entities
            ac_output = self.collected_data.get('csi_baremetal', {}).get('available_capacity', '')
            if ac_output:
                self._process_available_capacity_entities(ac_output)
            
            # Add CSI Baremetal node entities
            csibm_nodes_output = self.collected_data.get('csi_baremetal', {}).get('nodes', '')
            if csibm_nodes_output:
                self._process_csibm_node_entities(csibm_nodes_output)
        
        except Exception as e:
            error_msg = f"Error adding CSI Baremetal entities: {str(e)}"
            logging.error(error_msg)
            self.collected_data['errors'].append(error_msg)
    
    def _process_lvg_entities(self, lvg_output: str):
        """Process logical volume group entities"""
        try:
            # Parse the YAML output
            lvg_data = yaml.safe_load(lvg_output)
            
            # Process LVGs from the parsed YAML
            if lvg_data:
                # Handle different YAML structures
                lvg_items = []
                if isinstance(lvg_data, dict) and 'items' in lvg_data and isinstance(lvg_data['items'], list):
                    lvg_items = lvg_data['items']
                elif isinstance(lvg_data, list):
                    lvg_items = lvg_data
                
                for lvg in lvg_items:
                    if lvg.get('kind') == 'LogicalVolumeGroup' or 'LogicalVolumeGroup' in lvg.get('kind', ''):
                        # Extract LVG name
                        lvg_name = lvg.get('metadata', {}).get('name')
                        if not lvg_name:
                            continue
                        
                        # Extract LVG health
                        health = lvg.get('spec', {}).get('Health', 'ReadDataError')
                        
                        # Extract drive UUIDs
                        current_drives = lvg.get('spec', {}).get('Locations', [])
                        if not isinstance(current_drives, list):
                            current_drives = []
                        
                        # Add LVG to knowledge graph
                        lvg_id = self.knowledge_graph.add_gnode_lvg(lvg_name, Health=health, drive_uuids=current_drives)
                        
                        # Add issues for unhealthy LVGs
                        if health not in ['GOOD', 'HEALTHY']:
                            self.knowledge_graph.add_issue(
                                lvg_id,
                                "lvg_health",
                                f"LVG health issue: {health}",
                                "high"
                            )
                        
                        # Process LVG relationships
                        self._finalize_lvg_entity(lvg_name, current_drives)
            
        except Exception as e:
            logging.warning(f"Error processing LVG entities with yaml package: {e}")
            # Fallback to the old method in case of parsing errors
            try:
                lines = lvg_output.split('\n')
                current_lvg = None
                current_drives = []
                
                for line in lines:
                    if 'name:' in line and 'metadata:' not in line:
                        # Process previous LVG if exists
                        if current_lvg:
                            self._finalize_lvg_entity(current_lvg, current_drives)
                        
                        current_lvg = line.split('name:')[-1].strip()
                        current_drives = []
                    elif current_lvg and 'health:' in line:
                        health = line.split('health:')[-1].strip()
                        
                        # Add LVG to knowledge graph
                        lvg_id = self.knowledge_graph.add_gnode_lvg(current_lvg, Health=health, drive_uuids=current_drives)
                        
                        # Add issues for unhealthy LVGs
                        if health not in ['GOOD', 'HEALTHY']:
                            self.knowledge_graph.add_issue(
                                lvg_id,
                                "lvg_health",
                                f"LVG health issue: {health}",
                                "high"
                            )
                    elif current_lvg and 'drive:' in line:
                        # Extract drive UUID from LVG
                        drive_uuid = line.split('drive:')[-1].strip()
                        if drive_uuid:
                            current_drives.append(drive_uuid)
                
                # Process last LVG
                if current_lvg:
                    self._finalize_lvg_entity(current_lvg, current_drives)
            except Exception as fallback_error:
                logging.warning(f"Fallback processing of LVG entities also failed: {fallback_error}")
    
    def _finalize_lvg_entity(self, lvg_name: str, drive_uuids: List[str]):
        """Finalize LVG entity with drive relationships"""
        try:
            lvg_id = f"gnode:LVG:{lvg_name}"
            
            # Add LVGDrive relationships
            for drive_uuid in drive_uuids:
                drive_id = f"gnode:Drive:{drive_uuid}"
                if self.knowledge_graph.graph.has_node(drive_id):
                    self.knowledge_graph.add_relationship(lvg_id, drive_id, "contains")
                    logging.debug(f"Added LVGDrive relationship: {lvg_id}  {drive_id}")
                    
        except Exception as e:
            logging.warning(f"Error finalizing LVG entity {lvg_name}: {e}")
    
    def _process_available_capacity_entities(self, ac_output: str):
        """Process only relevant available capacity entities"""
        try:
            # Get relevant drive UUIDs to filter ACs
            relevant_drives = self._get_relevant_drive_uuids()
            
            # Parse the YAML output
            ac_data = yaml.safe_load(ac_output)
            
            # Process ACs from the parsed YAML
            if ac_data:
                # Handle different YAML structures
                ac_items = []
                if isinstance(ac_data, dict) and 'items' in ac_data and isinstance(ac_data['items'], list):
                    ac_items = ac_data['items']
                elif isinstance(ac_data, list):
                    ac_items = ac_data
                
                for ac in ac_items:
                    if ac.get('kind') == 'AvailableCapacity' or 'AvailableCapacity' in ac.get('kind', ''):
                        # Extract AC name
                        ac_name = ac.get('metadata', {}).get('name')
                        if not ac_name:
                            continue
                        
                        # Extract AC properties
                        ac_info = {
                            'size': str(ac.get('spec', {}).get('size', '')),
                            'storage_class': ac.get('spec', {}).get('storageClass', ''),
                            'location': ac.get('spec', {}).get('location', '')
                        }
                        
                        # Process AC if relevant
                        if self._is_ac_relevant(ac_name, ac_info, relevant_drives):
                            self._finalize_ac_entity(ac_name, ac_info)
            
        except Exception as e:
            logging.warning(f"Error processing Available Capacity entities with yaml package: {e}")
            # Fallback to the old method in case of parsing errors
            try:
                lines = ac_output.split('\n')
                current_ac = None
                ac_info = {}
                
                for line in lines:
                    if 'name:' in line and 'metadata:' not in line:
                        # Process previous AC if exists and relevant
                        if current_ac and self._is_ac_relevant(current_ac, ac_info, relevant_drives):
                            self._finalize_ac_entity(current_ac, ac_info)
                        
                        current_ac = line.split('name:')[-1].strip()
                        ac_info = {}
                    elif current_ac:
                        # Extract AC properties
                        if 'size:' in line:
                            ac_info['size'] = line.split('size:')[-1].strip()
                        elif 'storageClass:' in line:
                            ac_info['storage_class'] = line.split('storageClass:')[-1].strip()
                        elif 'location:' in line:
                            ac_info['location'] = line.split('location:')[-1].strip()
                
                # Process last AC if relevant
                if current_ac and self._is_ac_relevant(current_ac, ac_info, relevant_drives):
                    self._finalize_ac_entity(current_ac, ac_info)
            except Exception as fallback_error:
                logging.warning(f"Fallback processing of Available Capacity entities also failed: {fallback_error}")
    
    def _is_ac_relevant(self, ac_name: str, ac_info: Dict[str, str], relevant_drives: set) -> bool:
        """Check if AC is relevant to the current volume troubleshooting"""
        try:
            # AC is relevant if its location matches a relevant drive
            location = ac_info.get('location', '')
            if location in relevant_drives:
                return True
            
            # Also include ACs that might be on the same nodes as relevant drives
            # This is a more conservative approach to ensure we don't miss related capacity
            return False
            
        except Exception as e:
            logging.warning(f"Error checking AC relevance for {ac_name}: {e}")
            return False
    
    def _finalize_ac_entity(self, ac_name: str, ac_info: Dict[str, str]):
        """Finalize AC entity with properties"""
        try:
            ac_id = self.knowledge_graph.add_gnode_ac(
                ac_name,
                size=ac_info.get('size', ''),
                storage_class=ac_info.get('storage_class', ''),
                location=ac_info.get('location', '')
            )
            
            # Add ACNode relationship if location is specified
            location = ac_info.get('location')
            if location:
                node_id = f"gnode:Node:{location}"
                if self.knowledge_graph.graph.has_node(node_id):
                    self.knowledge_graph.add_relationship(ac_id, node_id, "available_on")
                    logging.debug(f"Added ACNode relationship: {ac_id}  {node_id}")
                    
        except Exception as e:
            logging.warning(f"Error finalizing AC entity {ac_name}: {e}")
    
    def _process_csibm_node_entities(self, csibm_nodes_output: str):
        """Process CSI Baremetal node entities"""
        try:
            lines = csibm_nodes_output.split('\n')
            
            for line in lines:
                if 'name:' in line and 'metadata:' not in line:
                    csibm_node_name = line.split('name:')[-1].strip()
                    # CSI Baremetal nodes provide additional node context
                    # These will be linked to regular nodes in _add_all_drives_and_nodes
                    
        except Exception as e:
            logging.warning(f"Error processing CSI Baremetal node entities: {e}")
    
    async def _add_all_drives_and_nodes(self):
        """Add all collected drives and cluster nodes to the knowledge graph"""
        try:
            logging.info("Adding all collected drives and cluster nodes to knowledge graph...")
            
            # Process all drives from CSI Baremetal data
            await self._process_all_drives()
            
            # Process all cluster nodes
            await self._process_all_cluster_nodes()
            
            # Create drivenode relationships
            await self._create_drive_node_relationships()
            
            # Create PVdrive relationships
            await self._create_pv_drive_relationships()
            
        except Exception as e:
            error_msg = f"Error adding all drives and nodes: {str(e)}"
            logging.error(error_msg)
            self.collected_data['errors'].append(error_msg)
    
    async def _process_all_drives(self):
        """Process only relevant drives from collected CSI Baremetal data"""
        try:
            drives_output = self.collected_data.get('csi_baremetal', {}).get('drives', '')
            if not drives_output:
                logging.warning("No drives data found in collected CSI Baremetal information")
                return
            
            # Get relevant drive UUIDs from volume locations and LVGs
            relevant_drives = self._get_relevant_drive_uuids()
            
            lines = drives_output.split('\n')
            current_drive = None
            drive_info = {}
            
            for line in lines:
                if 'name:' in line and 'metadata:' not in line:
                    # Process previous drive if exists and relevant
                    if current_drive and current_drive in relevant_drives:
                        self._finalize_drive_entity(current_drive, drive_info)
                    
                    current_drive = line.split('name:')[-1].strip()
                    drive_info = {}
                elif current_drive and current_drive in relevant_drives:
                    # Extract drive properties only for relevant drives
                    if 'health:' in line:
                        drive_info['Health'] = line.split('health:')[-1].strip()
                    elif 'status:' in line:
                        drive_info['Status'] = line.split('status:')[-1].strip()
                    elif 'path:' in line:
                        drive_info['Path'] = line.split('path:')[-1].strip()
                    elif 'usage:' in line:
                        drive_info['Usage'] = line.split('usage:')[-1].strip()
                    elif 'size:' in line:
                        drive_info['Size'] = line.split('size:')[-1].strip()
                    elif 'type:' in line:
                        drive_info['Type'] = line.split('type:')[-1].strip()
                    elif 'node:' in line:
                        drive_info['NodeName'] = line.split('node:')[-1].strip()
                    elif 'serialNumber:' in line:
                        drive_info['SerialNumber'] = line.split('serialNumber:')[-1].strip()
            
            # Process last drive if relevant
            if current_drive and current_drive in relevant_drives:
                self._finalize_drive_entity(current_drive, drive_info)
                
            logging.info(f"Processed {len(relevant_drives)} relevant drives from CSI Baremetal data")
            
        except Exception as e:
            logging.warning(f"Error processing relevant drives: {e}")
    
    def _get_relevant_drive_uuids(self) -> set:
        """Get relevant drive UUIDs from volume locations and LVGs"""
        relevant_drives = set()
        
        try:
            # Get drive UUIDs from CSI Volume locations
            volumes_output = self.collected_data.get('csi_baremetal', {}).get('volumes', '')
            if volumes_output:
                volume_locations = self._parse_volume_locations(volumes_output)
                for location in volume_locations.values():
                    if self._is_drive_uuid(location):
                        relevant_drives.add(location)
            
            # Get drive UUIDs from LVG locations
            lvg_output = self.collected_data.get('csi_baremetal', {}).get('lvgs', '')
            if lvg_output:
                lvg_drives = self._parse_lvg_drive_locations(lvg_output)
                relevant_drives.update(lvg_drives)
            
            # Also include drives from volume chain if available
            volume_chain_drives = getattr(self, '_volume_chain_drives', set())
            relevant_drives.update(volume_chain_drives)
            
            logging.info(f"Found {len(relevant_drives)} relevant drive UUIDs")
            return relevant_drives
            
        except Exception as e:
            logging.warning(f"Error getting relevant drive UUIDs: {e}")
            return set()
    
    def _parse_lvg_drive_locations(self, lvg_output: str) -> set:
        """Parse LVG output to extract drive UUIDs from LOCATIONS property"""
        drive_uuids = set()
        
        try:
            lines = lvg_output.split('\n')
            current_lvg = None
            
            for line in lines:
                line = line.strip()
                if 'name:' in line and 'metadata:' not in line:
                    current_lvg = line.split('name:')[-1].strip()
                elif current_lvg and 'locations:' in line.lower():
                    # Extract drive UUIDs from locations array
                    locations_str = line.split('locations:')[-1].strip()
                    # Handle array format like ["uuid1", "uuid2"]
                    if '[' in locations_str and ']' in locations_str:
                        locations_str = locations_str.strip('[]')
                        for location in locations_str.split(','):
                            location = location.strip().strip('"\'')
                            if self._is_drive_uuid(location):
                                drive_uuids.add(location)
                    current_lvg = None  # Reset for next LVG
            
        except Exception as e:
            logging.warning(f"Error parsing LVG drive locations: {e}")
        
        return drive_uuids
    
    def _finalize_drive_entity(self, drive_uuid: str, drive_info: Dict[str, str]):
        """Finalize drive entity with comprehensive information"""
        try:
            # Add drive to knowledge graph with all collected information
            drive_id = self.knowledge_graph.add_gnode_drive(drive_uuid, **drive_info)
            
            # Add issues for unhealthy drives
            health = drive_info.get('Health', 'UNKNOWN')
            if health in ['SUSPECT', 'BAD', 'FAILED']:
                severity = "critical" if health in ['BAD', 'FAILED'] else "high"
                self.knowledge_graph.add_issue(
                    drive_id,
                    "disk_health",
                    f"Drive health issue: {health}",
                    severity
                )
            
            # Add issues for drives with high usage
            usage = drive_info.get('Usage', 'UNKNOWN')
            if usage == 'IN_USE' and drive_info.get('Type') == 'SYSTEM':
                self.knowledge_graph.add_issue(
                    drive_id,
                    "system_drive_usage",
                    "System drive is in heavy use",
                    "medium"
                )
            
            logging.debug(f"Added Drive entity: {drive_id} with health {health}")
            
        except Exception as e:
            logging.warning(f"Error finalizing drive entity {drive_uuid}: {e}")
    
    async def _process_all_cluster_nodes(self):
        """Process only actual cluster nodes from kubectl get node output"""
        try:
            nodes_output = self.collected_data.get('kubernetes', {}).get('nodes', '')
            if not nodes_output:
                logging.warning("No nodes data found in collected Kubernetes information")
                return
            
            # Get actual cluster node names from kubectl get node output
            cluster_nodes = self._parse_cluster_node_names(nodes_output)
            
            # Process only these cluster nodes
            for node_name in cluster_nodes:
                node_info = self._parse_node_info_from_output(node_name, nodes_output)
                self._finalize_cluster_node_entity(node_name, node_info)
                
            logging.info(f"Processed {len(cluster_nodes)} actual cluster nodes from Kubernetes data")
            
        except Exception as e:
            logging.warning(f"Error processing cluster nodes: {e}")
    
    def _parse_cluster_node_names(self, nodes_output: str) -> List[str]:
        """Parse actual cluster node names from kubectl get node output"""
        cluster_nodes = []
        
        try:
            lines = nodes_output.split('\n')
            
            newNodeData = False
            for line in lines:
                line = line.strip()
                if 'kind: Node' in line:
                    newNodeData = True

                # Look for node names that match cluster naming pattern
                if 'name:' in line and 'metadata:' not in line and 'hostname:' not in line and newNodeData:
                    node_name = line.split(':')[-1].strip()
                    # Filter out non-cluster nodes (CSI nodes, PV nodes, etc.)
                    if not self._is_not_cluster_node(node_name):
                        cluster_nodes.append(node_name)
                        newNodeData = False
            
        except Exception as e:
            logging.warning(f"Error parsing cluster node names: {e}")
        
        return cluster_nodes
    
    def _is_not_cluster_node(self, node_name: str) -> bool:
        """Check if node name represents an actual cluster node"""
        # Filter out CSI-specific nodes, PV nodes, and other non-cluster entities
        exclude_patterns = [
            'csi/',
            'driver.',
            'pvc-',
            'kubernetes.io/',
            '-' * 8,  # UUID-like patterns
        ]
        
        for pattern in exclude_patterns:
            if pattern in node_name:
                return False
        
        # Cluster nodes typically have domain-like names
        return '.' in node_name or node_name.endswith('.local') or len(node_name.split('.')) > 1
    
    def _parse_node_info_from_output(self, node_name: str, nodes_output: str) -> Dict[str, Any]:
        """
        Parse node information for a specific node from the output using YAML parser
        
        This function parses the complex YAML structure of node status, including:
        - Node addresses (InternalIP, Hostname)
        - Allocatable resources (CPU, memory, storage, etc.)
        - Capacity information
        - Node conditions (Ready, DiskPressure, MemoryPressure, etc.)
        
        Args:
            node_name: Name of the node to parse information for
            nodes_output: YAML output containing node information
            
        Returns:
            Dictionary containing parsed node information
        """
        node_info = {}
        
        try:
            # Parse the YAML output
            nodes_data = yaml.safe_load(nodes_output)
            
            # Find the node with matching name
            target_node = None
            if nodes_data['items'] != None and isinstance(nodes_data.get('items'), list):
                # List of nodes case
                for node in nodes_data.get('items'):
                    if node.get('kind') == 'Node' and node.get('metadata', {}).get('name') == node_name:
                        target_node = node
                        break
            
            if not target_node:
                logging.warning(f"Node {node_name} not found in the provided YAML output")
                return node_info
            
            # Extract status information
            status = target_node.get('status', {})
            
            # Extract addresses
            addresses = status.get('addresses', [])
            if addresses:
                node_info['Addresses'] = addresses
                # Extract specific address types for convenience
                for addr in addresses:
                    if addr.get('type') == 'InternalIP':
                        node_info['InternalIP'] = addr.get('address', '')
                    elif addr.get('type') == 'Hostname':
                        node_info['Hostname'] = addr.get('address', '')
            
            # Extract allocatable resources
            allocatable = status.get('allocatable', {})
            if allocatable:
                node_info['Allocatable'] = allocatable
                # Extract specific allocatable resources for convenience
                node_info['AllocatableCPU'] = allocatable.get('cpu', '')
                node_info['AllocatableMemory'] = allocatable.get('memory', '')
                node_info['AllocatableStorage'] = allocatable.get('ephemeral-storage', '')
                node_info['AllocatablePods'] = allocatable.get('pods', '')
            
            # Extract capacity information
            capacity = status.get('capacity', {})
            if capacity:
                node_info['Capacity'] = capacity
                # Extract specific capacity information for convenience
                node_info['CapacityCPU'] = capacity.get('cpu', '')
                node_info['CapacityMemory'] = capacity.get('memory', '')
                node_info['CapacityStorage'] = capacity.get('ephemeral-storage', '')
                node_info['CapacityPods'] = capacity.get('pods', '')
            
            # Process conditions
            conditions = status.get('conditions', [])
            if conditions:
                node_info['Conditions'] = conditions
                # Extract specific condition statuses for convenience
                for condition in conditions:
                    condition_type = condition.get('type', '')
                    condition_status = condition.get('status', '').lower() == 'true'
                    
                    if condition_type == 'Ready':
                        node_info['Ready'] = condition_status
                    elif condition_type == 'DiskPressure':
                        node_info['DiskPressure'] = condition_status
                    elif condition_type == 'MemoryPressure':
                        node_info['MemoryPressure'] = condition_status
                    elif condition_type == 'PIDPressure':
                        node_info['PIDPressure'] = condition_status
                    elif condition_type == 'NetworkUnavailable':
                        node_info['NetworkUnavailable'] = condition_status
                    elif condition_type == 'EtcdIsVoter':
                        node_info['EtcdIsVoter'] = condition_status
        
        except Exception as e:
            logging.warning(f"Error parsing node info for {node_name}: {e}")
        
        return node_info
    
    def _finalize_cluster_node_entity(self, node_name: str, node_info: Dict[str, Any]):
        """Finalize cluster node entity with comprehensive information"""
        try:
            # Add node to knowledge graph with all collected information
            node_id = self.knowledge_graph.add_gnode_node(node_name, **node_info)
            
            # Add issues for unhealthy nodes
            ready = node_info.get('Ready', True)
            disk_pressure = node_info.get('DiskPressure', False)
            memory_pressure = node_info.get('MemoryPressure', False)
            
            if not ready:
                self.knowledge_graph.add_issue(
                    node_id,
                    "node_not_ready",
                    f"Node {node_name} is not ready",
                    "critical"
                )
            
            if disk_pressure:
                self.knowledge_graph.add_issue(
                    node_id,
                    "disk_pressure",
                    f"Node {node_name} is experiencing disk pressure",
                    "high"
                )
            
            if memory_pressure:
                self.knowledge_graph.add_issue(
                    node_id,
                    "memory_pressure",
                    f"Node {node_name} is experiencing memory pressure",
                    "medium"
                )
            
            logging.debug(f"Added Node entity: {node_id} with Ready={ready}, DiskPressure={disk_pressure}")
            
        except Exception as e:
            logging.warning(f"Error finalizing cluster node entity {node_name}: {e}")
    
    async def _create_drive_node_relationships(self):
        """Create DriveNode relationships based on collected data"""
        try:
            # Get all drive nodes
            drive_nodes = self.knowledge_graph.find_nodes_by_type('Drive')
            
            for drive_id in drive_nodes:
                drive_attrs = self.knowledge_graph.graph.nodes[drive_id]
                node_name = drive_attrs.get('NodeName')
                
                if node_name:
                    node_id = f"gnode:Node:{node_name}"
                    if self.knowledge_graph.graph.has_node(node_id):
                        self.knowledge_graph.add_relationship(drive_id, node_id, "located_on")
                        logging.debug(f"Added DriveNode relationship: {drive_id}  {node_id}")
            
        except Exception as e:
            logging.warning(f"Error creating drivenode relationships: {e}")
    
    async def _create_pv_drive_relationships(self):
        """Create PVDrive relationships based on collected data"""
        try:
            # Get all PV nodes
            pv_nodes = self.knowledge_graph.find_nodes_by_type('PV')
            
            for pv_id in pv_nodes:
                pv_attrs = self.knowledge_graph.graph.nodes[pv_id]
                disk_path = pv_attrs.get('diskPath')
                
                if disk_path:
                    # Find drive with matching path
                    drive_nodes = self.knowledge_graph.find_nodes_by_type('Drive')
                    for drive_id in drive_nodes:
                        drive_attrs = self.knowledge_graph.graph.nodes[drive_id]
                        drive_path = drive_attrs.get('Path')
                        
                        if drive_path and drive_path == disk_path:
                            self.knowledge_graph.add_relationship(pv_id, drive_id, "maps_to")
                            logging.debug(f"Added PVDrive relationship: {pv_id}  {drive_id}")
                            break
            
        except Exception as e:
            logging.warning(f"Error creating PVdrive relationships: {e}")
    
    async def _add_volume_entities(self):
        """Add Volume entities to the knowledge graph based on PVC relationships"""
        try:
            logging.info("Adding Volume entities to knowledge graph based on PVCs...")
            
            # Process volumes only for existing PVCs in the knowledge graph
            self._process_volumes_from_pvcs()
                
        except Exception as e:
            error_msg = f"Error adding Volume entities: {str(e)}"
            logging.error(error_msg)
            self.collected_data['errors'].append(error_msg)
    
    def _process_volumes_from_pvcs(self):
        """Process Volume entities based on existing PVCs in the knowledge graph"""
        try:
            # Get all PVC nodes from the knowledge graph
            pvc_nodes = self.knowledge_graph.find_nodes_by_type('PVC')
            
            for pvc_id in pvc_nodes:
                pvc_attrs = self.knowledge_graph.graph.nodes[pvc_id]
                pvc_name = pvc_attrs.get('name')
                pvc_namespace = pvc_attrs.get('namespace')
                
                if not pvc_name or not pvc_namespace:
                    continue
                
                # Extract volume information from PVC properties and related PV
                volume_info = self._extract_volume_info_from_pvc(pvc_id, pvc_attrs)
                
                if volume_info:
                    # Create volume entity
                    volume_id = self.knowledge_graph.add_gnode_volume(
                        volume_info['name'],
                        volume_info['namespace'],
                        **volume_info['attributes']
                    )
                    
                    # Add PVCVolume edge
                    self.knowledge_graph.add_relationship(pvc_id, volume_id, "bound_to")
                    
                    # Add VolumeStorage relationships based on storage type
                    self._add_volume_storage_relationships(volume_id, volume_info)
                    
                    logging.debug(f"Added Volume {volume_id} for PVC {pvc_id}")
                    
        except Exception as e:
            logging.warning(f"Error processing volumes from PVCs: {e}")
    
    def _extract_volume_info_from_pvc(self, pvc_id: str, pvc_attrs: Dict[str, Any]) -> Dict[str, Any]:
        """Extract volume information from PVC and its bound PV"""
        try:
            # Find the PV bound to this PVC
            bound_pvs = self.knowledge_graph.find_connected_nodes(pvc_id, "bound_to")
            
            if not bound_pvs:
                # No bound PV, create volume from PVC info only
                volume_name = f"vol-{pvc_attrs.get('name', 'unknown')}"
                return {
                    'name': volume_name,
                    'namespace': pvc_attrs.get('namespace', 'default'),
                    'attributes': {
                        'Health': 'UNKNOWN',
                        'LocationType': 'PVC_ONLY',
                        'size': pvc_attrs.get('StorageSize', ''),
                        'storage_class': pvc_attrs.get('storageClass', ''),
                        'location': '',
                        'Usage': pvc_attrs.get('Phase', 'UNKNOWN'),
                        'source_pvc': pvc_id
                    }
                }
            
            # Get PV information
            pv_id = bound_pvs[0]
            pv_attrs = self.knowledge_graph.graph.nodes[pv_id]
            
            # Determine volume name from PV or generate one
            volume_name = pv_attrs.get('volumeName', f"vol-{pv_attrs.get('name', 'unknown')}")
            
            # Determine storage type and health
            health = self._determine_volume_health(pv_id, pv_attrs)
            location_type = self._determine_location_type(pv_id, pv_attrs)
            usage = self._determine_volume_usage(pv_id, pv_attrs)
            
            return {
                'name': volume_name,
                'namespace': pvc_attrs.get('namespace', 'default'),
                'attributes': {
                    'Health': health,
                    'LocationType': location_type,
                    'size': pv_attrs.get('Capacity', pvc_attrs.get('StorageSize', '')),
                    'storage_class': pv_attrs.get('storageClass', ''),
                    'location': pv_attrs.get('nodeAffinity', ''),
                    'Usage': usage,
                    'source_pvc': pvc_id,
                    'source_pv': pv_id
                }
            }
            
        except Exception as e:
            logging.warning(f"Error extracting volume info from PVC {pvc_id}: {e}")
            return None
    
    def _determine_volume_health(self, pv_id: str, pv_attrs: Dict[str, Any]) -> str:
        """Determine volume health based on PV and connected storage"""
        try:
            # Check PV phase
            pv_phase = pv_attrs.get('Phase', 'Unknown')
            if pv_phase != 'Bound':
                return 'SUSPECT'
            
            # Check connected drives health
            connected_drives = self.knowledge_graph.find_connected_nodes(pv_id, "maps_to")
            for drive_id in connected_drives:
                drive_attrs = self.knowledge_graph.graph.nodes[drive_id]
                drive_health = drive_attrs.get('Health', 'UNKNOWN')
                if drive_health in ['SUSPECT', 'BAD']:
                    return drive_health
            
            # Check connected LVGs health
            for drive_id in connected_drives:
                # Find LVGs that contain this drive
                for lvg_id in self.knowledge_graph.find_nodes_by_type('LVG'):
                    lvg_drives = self.knowledge_graph.find_connected_nodes(lvg_id, "contains")
                    if drive_id in lvg_drives:
                        lvg_attrs = self.knowledge_graph.graph.nodes[lvg_id]
                        lvg_health = lvg_attrs.get('Health', 'UNKNOWN')
                        if lvg_health not in ['GOOD', 'HEALTHY']:
                            return 'SUSPECT'
            
            return 'GOOD'
            
        except Exception as e:
            logging.warning(f"Error determining volume health for PV {pv_id}: {e}")
            return 'UNKNOWN'
    
    def _determine_location_type(self, pv_id: str, pv_attrs: Dict[str, Any]) -> str:
        """Determine volume location type (LVG or DRIVE)"""
        try:
            # Check if PV maps to drives that are part of LVGs
            connected_drives = self.knowledge_graph.find_connected_nodes(pv_id, "maps_to")
            
            for drive_id in connected_drives:
                # Check if this drive is contained in any LVG
                for lvg_id in self.knowledge_graph.find_nodes_by_type('LVG'):
                    lvg_drives = self.knowledge_graph.find_connected_nodes(lvg_id, "contains")
                    if drive_id in lvg_drives:
                        return 'LVG'
            
            # If no LVG found, it's direct drive usage
            if connected_drives:
                return 'DRIVE'
            
            return 'UNKNOWN'
            
        except Exception as e:
            logging.warning(f"Error determining location type for PV {pv_id}: {e}")
            return 'UNKNOWN'
    
    def _determine_volume_usage(self, pv_id: str, pv_attrs: Dict[str, Any]) -> str:
        """Determine volume usage status"""
        try:
            pv_phase = pv_attrs.get('Phase', 'Unknown')
            
            if pv_phase == 'Bound':
                return 'IN_USE'
            elif pv_phase == 'Available':
                return 'AVAILABLE'
            elif pv_phase == 'Released':
                return 'RELEASED'
            elif pv_phase == 'Failed':
                return 'FAILED'
            else:
                return 'UNKNOWN'
                
        except Exception as e:
            logging.warning(f"Error determining volume usage for PV {pv_id}: {e}")
            return 'UNKNOWN'
    
    def _add_volume_storage_relationships(self, volume_id: str, volume_info: Dict[str, Any]):
        """Add VolumeStorage relationships based on storage type"""
        try:
            location_type = volume_info['attributes'].get('LocationType', 'UNKNOWN')
            source_pv = volume_info['attributes'].get('source_pv')
            
            if not source_pv:
                return
            
            # Get drives connected to the PV
            connected_drives = self.knowledge_graph.find_connected_nodes(source_pv, "maps_to")
            
            if location_type == 'LVG':
                # Find LVG that contains the drives
                for drive_id in connected_drives:
                    for lvg_id in self.knowledge_graph.find_nodes_by_type('LVG'):
                        lvg_drives = self.knowledge_graph.find_connected_nodes(lvg_id, "contains")
                        if drive_id in lvg_drives:
                            # Add VolumeLVG relationship
                            self.knowledge_graph.add_relationship(volume_id, lvg_id, "bound_to")
                            logging.debug(f"Added VolumeLVG relationship: {volume_id}  {lvg_id}")
                            break
            
            elif location_type == 'DRIVE':
                # Add direct VolumeDrive relationships
                for drive_id in connected_drives:
                    self.knowledge_graph.add_relationship(volume_id, drive_id, "bound_to")
                    logging.debug(f"Added VolumeDrive relationship: {volume_id}  {drive_id}")
            
        except Exception as e:
            logging.warning(f"Error adding volume storage relationships for {volume_id}: {e}")
    
    async def _create_enhanced_csi_relationships(self):
        """Create enhanced CSI relationships based on Volume location mapping"""
        try:
            logging.info("Creating enhanced CSI relationships...")
            
            # Create Volume  Drive/LVG relationships based on CSI Volume data
            await self._create_volume_drive_relationships()
            
            logging.info("Enhanced CSI relationships created successfully")
            
        except Exception as e:
            error_msg = f"Error creating enhanced CSI relationships: {str(e)}"
            logging.error(error_msg)
            self.collected_data['errors'].append(error_msg)
    
    async def _create_volume_drive_relationships(self):
        """Create Volume  Drive/LVG relationships based on CSI Volume location data"""
        try:
            volumes_output = self.collected_data.get('csi_baremetal', {}).get('volumes', '')
            if not volumes_output:
                logging.warning("No CSI Volume data found for relationship creation")
                return
            
            # Parse CSI Volume data to extract location information
            volume_locations = self._parse_volume_locations(volumes_output)
            
            # Get all Volume nodes from knowledge graph
            volume_nodes = self.knowledge_graph.find_nodes_by_type('Volume')
            
            for volume_id in volume_nodes:
                volume_attrs = self.knowledge_graph.graph.nodes[volume_id]
                volume_name = volume_attrs.get('name')
                
                if not volume_name:
                    continue
                
                # Find location for this volume
                location = volume_locations.get(volume_name)
                if not location:
                    # Try to get location from volume attributes if not found in parsed data
                    location = volume_attrs.get('location')
                
                if not location:
                    logging.debug(f"No location found for volume {volume_name}")
                    continue
                
                # Enhanced logic: Determine if location is Drive UUID or LVG UUID
                if self._is_drive_uuid(location):
                    # Direct Volume  Drive relationship
                    drive_id = f"gnode:Drive:{location}"
                    if self.knowledge_graph.graph.has_node(drive_id):
                        self.knowledge_graph.add_relationship(volume_id, drive_id, "bound_to")
                        logging.debug(f"Added VolumeDrive relationship: {volume_id}  {drive_id}")
                    else:
                        logging.warning(f"Drive {location} not found for volume {volume_name}")
                else:
                    # Volume  LVG relationship (location is LVG UUID)
                    lvg_id = f"gnode:LVG:{location}"
                    if self.knowledge_graph.graph.has_node(lvg_id):
                        self.knowledge_graph.add_relationship(volume_id, lvg_id, "bound_to")
                        logging.debug(f"Added VolumeLVG relationship: {volume_id}  {lvg_id}")
                        
                        # Also create Volume  Drive relationships through LVG
                        await self._create_volume_to_drive_via_lvg(volume_id, lvg_id)
                    else:
                        logging.warning(f"LVG {location} not found for volume {volume_name}")
            
        except Exception as e:
            logging.warning(f"Error creating volumedrive relationships: {e}")
    
    def _parse_volume_locations(self, volumes_output: str) -> Dict[str, str]:
        """Parse CSI Volume output to extract volume name  location mapping"""
        volume_locations = {}
        
        try:
            lines = volumes_output.split('\n')
            current_volume = None
            
            for line in lines:
                line = line.strip()
                if 'name:' in line and 'metadata:' not in line:
                    current_volume = line.split('name:')[-1].strip()
                elif current_volume and 'location:' in line:
                    location = line.split('location:')[-1].strip()
                    if location:
                        volume_locations[current_volume] = location
                        current_volume = None  # Reset for next volume
            
        except Exception as e:
            logging.warning(f"Error parsing volume locations: {e}")
        
        return volume_locations
    
    def _is_drive_uuid(self, location: str) -> bool:
        """Check if location string is a Drive UUID format"""
        # Drive UUIDs are typically 36 characters with hyphens (UUID format)
        # e.g., "2a96dfec-47db-449d-9789-0d81660c2c4d"
        return len(location) == 36 and location.count('-') == 4
    
    async def _create_volume_to_drive_via_lvg(self, volume_id: str, lvg_id: str):
        """Create Volume  Drive relationships through LVG"""
        try:
            # Get drives contained in the LVG
            lvg_drives = self.knowledge_graph.find_connected_nodes(lvg_id, "contains")
            
            for drive_id in lvg_drives:
                # Add Volume  Drive relationship through LVG
                self.knowledge_graph.add_relationship(volume_id, drive_id, "bound_to")
                logging.debug(f"Added VolumeDrive (via LVG) relationship: {volume_id}  {drive_id}")
                
        except Exception as e:
            logging.warning(f"Error creating volumedrive relationships via LVG {lvg_id}: {e}")
    
    async def _add_system_entities(self):
        """Add System entities to the knowledge graph"""
        try:
            logging.info("Adding System entities to knowledge graph...")
            
            # Add kernel system entity
            kernel_id = self.knowledge_graph.add_gnode_system_entity(
                "kernel", "logs",
                description="Kernel logs and dmesg output",
                log_sources=["dmesg", "journal"]
            )
            
            # Add kubelet system entity
            kubelet_id = self.knowledge_graph.add_gnode_system_entity(
                "kubelet", "service",
                description="Kubelet service for pod and volume management",
                service_status="active"
            )
            
            # Add boot system entity
            boot_id = self.knowledge_graph.add_gnode_system_entity(
                "boot", "logs",
                description="Boot-time hardware and storage initialization",
                log_sources=["journal"]
            )
            
            # Add storage services system entity
            storage_services_id = self.knowledge_graph.add_gnode_system_entity(
                "storage_services", "service",
                description="Storage-related system services",
                services=["csi-baremetal-node", "csi-baremetal-controller"]
            )
            
            # Add SMART monitoring system entity if SMART data exists
            if self.collected_data.get('smart_data'):
                smart_id = self.knowledge_graph.add_gnode_system_entity(
                    "smart_monitoring", "hardware",
                    description="SMART drive health monitoring",
                    monitored_drives=list(self.collected_data['smart_data'].keys())
                )
            
            # Add hardware system entity with comprehensive information
            await self._add_hardware_system_entity()
                
        except Exception as e:
            error_msg = f"Error adding System entities: {str(e)}"
            logging.error(error_msg)
            self.collected_data['errors'].append(error_msg)
    
    async def _add_log_based_issues(self):
        """Add log-based issues to the knowledge graph"""
        try:
            logging.info("Analyzing logs for storage-related issues...")
            
            # Parse dmesg issues
            dmesg_issues = self._parse_dmesg_issues()
            for issue in dmesg_issues:
                # Add system-level issues to a general system entity
                system_id = "gnode:System:kernel"
                self.knowledge_graph.add_issue(
                    system_id,
                    issue['type'],
                    issue['description'],
                    issue['severity']
                )
            
            # Parse journal log issues
            journal_issues = self._parse_journal_issues()
            for issue in journal_issues:
                # Determine entity based on issue source
                if issue['source'] == 'journal_kubelet':
                    entity_id = "gnode:System:kubelet"
                elif issue['source'] == 'journal_boot':
                    entity_id = "gnode:System:boot"
                else:
                    entity_id = "gnode:System:storage_services"
                
                self.knowledge_graph.add_issue(
                    entity_id,
                    issue['type'],
                    issue['description'],
                    issue['severity']
                )
            
            total_log_issues = len(dmesg_issues) + len(journal_issues)
            logging.info(f"Added {total_log_issues} log-based issues to knowledge graph "
                        f"({len(dmesg_issues)} from dmesg, {len(journal_issues)} from journal)")
            
            # Store log analysis summary
            self.collected_data['log_analysis'] = {
                'dmesg_issues': dmesg_issues,
                'journal_issues': journal_issues,
                'total_issues': total_log_issues
            }
            
        except Exception as e:
            error_msg = f"Error adding log-based issues: {str(e)}"
            logging.error(error_msg)
            self.collected_data['errors'].append(error_msg)
    
    async def _add_smart_data_analysis(self):
        """Add SMART data analysis to the knowledge graph"""
        try:
            smart_data = self.collected_data.get('smart_data', {})
            if not smart_data:
                return
                
            logging.info("Analyzing SMART data for drive health issues...")
            
            for drive_uuid, smart_output in smart_data.items():
                drive_id = f"gnode:Drive:{drive_uuid}"
                
                # Parse SMART data for health indicators
                smart_issues = self._parse_smart_data_issues(smart_output, drive_uuid)
                
                for issue in smart_issues:
                    self.knowledge_graph.add_issue(
                        drive_id,
                        issue['type'],
                        issue['description'],
                        issue['severity']
                    )
                
                # Add relationship between drive and SMART monitoring system
                smart_system_id = "gnode:System:smart_monitoring"
                if self.knowledge_graph.graph.has_node(smart_system_id):
                    self.knowledge_graph.add_relationship(
                        smart_system_id, drive_id, "monitors"
                    )
            
            logging.info(f"SMART data analysis completed for {len(smart_data)} drives")
            
        except Exception as e:
            error_msg = f"Error adding SMART data analysis: {str(e)}"
            logging.error(error_msg)
            self.collected_data['errors'].append(error_msg)
    
    async def _add_enhanced_log_analysis(self):
        """Add enhanced log analysis to the knowledge graph"""
        try:
            enhanced_logs = self.collected_data.get('enhanced_logs', {})
            service_logs = self.collected_data.get('service_logs', {})
            
            if not enhanced_logs and not service_logs:
                return
                
            logging.info("Analyzing enhanced logs for storage issues...")
            
            # Analyze enhanced dmesg patterns
            for _, node_log_output in enhanced_logs.items():
                for pattern_key, log_output in node_log_output.items():
                    if log_output and log_output.strip():
                        # Extract pattern type from key
                        pattern_type = pattern_key.replace('dmesg_', '').replace('_', ' ')
                        
                        # Add issue to kernel system entity
                        self.knowledge_graph.add_issue(
                            "gnode:System:kernel",
                            "enhanced_log_pattern",
                            f"Enhanced log analysis detected {pattern_type} issues, log {log_output}",
                            "medium"
                        )
            
            # Analyze service logs
            for _, service_log_output in service_logs.items():
                for service_name, log_output in service_log_output.items():
                    if log_output and ('error' in log_output.lower() or 'failed' in log_output.lower()):
                        # Determine system entity based on service
                        if service_name == 'kubelet':
                            entity_id = "gnode:System:kubelet"
                        else:
                            entity_id = "gnode:System:storage_services"
                        
                        self.knowledge_graph.add_issue(
                            entity_id,
                            "service_error",
                            f"Service {service_name} logs contain error messages",
                            "medium"
                        )
            
            logging.info("Enhanced log analysis completed")
            
        except Exception as e:
            error_msg = f"Error adding enhanced log analysis: {str(e)}"
            logging.error(error_msg)
            self.collected_data['errors'].append(error_msg)
            
    async def _process_kubectl_describe_data(self):
        """Process kubectl describe data for all resources and add to Knowledge Graph"""
        try:
            logging.info("Processing kubectl describe data for all resources")
            
            # Check if describe data is available
            if 'describe' not in self.collected_data:
                logging.info("No kubectl describe data available to process")
                return
            
            # Process each resource type's describe data
            describe_data = self.collected_data['describe']
            
            # Log the available describe data types
            logging.info(f"Available describe data types: {list(describe_data.keys())}")
            
        except Exception as e:
            error_msg = f"Error processing kubectl describe data: {str(e)}"
            logging.error(error_msg)
            self.collected_data['errors'].append(error_msg)
    
    def _parse_pod_describe_data(self, describe_output: str) -> Dict[str, Any]:
        """
        Parse pod describe data and extract attributes
        
        Args:
            describe_output: Output from kubectl describe pod command
            
        Returns:
            Dictionary of pod attributes
        """
        attributes = {}
        
        try:
            lines = describe_output.split('\n')
            current_section = None
            
            for line in lines:
                line = line.strip()
                
                # Skip empty lines
                if not line:
                    continue
                
                # Check for section headers
                if line.endswith(':') and not ':' in line[:-1]:
                    current_section = line[:-1]
                    continue
                
                # Extract key-value pairs
                if ':' in line and current_section != 'Events':
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip()
                    
                    # Store as attributes with proper naming
                    attr_key = key.replace(' ', '')
                    attributes[attr_key] = value
                    
                    # Extract specific important attributes
                    if key == 'Node':
                        # Format: Node: node-name/10.0.0.1
                        node_parts = value.split('/', 1)
                        if len(node_parts) > 0:
                            attributes['NodeName'] = node_parts[0]
                    elif key == 'Status':
                        attributes['PodStatus'] = value
                    elif key == 'QoS Class':
                        attributes['QoSClass'] = value
                    elif key == 'IP':
                        attributes['PodIP'] = value
                    elif key == 'Priority':
                        try:
                            attributes['Priority'] = int(value)
                        except:
                            attributes['Priority'] = value
            
            # Extract container information
            container_info = self._extract_container_info(describe_output)
            if container_info:
                attributes['Containers'] = container_info
                
            return attributes
            
        except Exception as e:
            logging.warning(f"Error parsing pod describe data: {e}")
            return {}
    
    def _extract_container_info(self, describe_output: str) -> List[Dict[str, Any]]:
        """
        Extract container information from pod describe output
        
        Args:
            describe_output: Output from kubectl describe pod command
            
        Returns:
            List of container info dictionaries
        """
        containers = []
        current_container = None
        in_container_section = False
        
        try:
            lines = describe_output.split('\n')
            
            for line in lines:
                line = line.strip()
                
                # Start of Containers section
                if line == 'Containers:':
                    in_container_section = True
                    continue
                
                # End of Containers section
                if in_container_section and line.endswith(':') and not ':' in line[:-1] and line != 'Containers:':
                    in_container_section = False
                    break
                
                # New container
                if in_container_section and not line.startswith(' ') and ':' in line:
                    # Save previous container
                    if current_container:
                        containers.append(current_container)
                    
                    container_name = line.split(':', 1)[0].strip()
                    current_container = {'name': container_name}
                    continue
                
                # Container attributes
                if in_container_section and current_container and ':' in line:
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip()
                    
                    # Store with proper naming
                    attr_key = key.replace(' ', '')
                    current_container[attr_key] = value
            
            # Add the last container
            if current_container:
                containers.append(current_container)
                
            return containers
            
        except Exception as e:
            logging.warning(f"Error extracting container info: {e}")
            return []
    
    def _parse_pvc_describe_data(self, describe_output: str) -> Dict[str, Any]:
        """
        Parse PVC describe data and extract attributes
        
        Args:
            describe_output: Output from kubectl describe pvc command
            
        Returns:
            Dictionary of PVC attributes
        """
        attributes = {}
        
        try:
            lines = describe_output.split('\n')
            
            for line in lines:
                line = line.strip()
                
                # Skip empty lines and section headers
                if not line or (line.endswith(':') and not ':' in line[:-1]):
                    continue
                
                # Extract key-value pairs
                if ':' in line and 'Events:' not in line:
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip()
                    
                    # Store as attributes with proper naming
                    attr_key = key.replace(' ', '')
                    attributes[attr_key] = value
                    
                    # Extract specific important attributes
                    if key == 'Status':
                        attributes['PVCStatus'] = value
                    elif key == 'Volume':
                        attributes['BoundVolume'] = value
                    elif key == 'Storage Class':
                        attributes['StorageClass'] = value
                    elif key == 'Access Modes':
                        attributes['AccessModes'] = value
                    elif key == 'VolumeMode':
                        attributes['VolumeMode'] = value
                    elif key == 'Capacity':
                        attributes['Capacity'] = value
            
            return attributes
            
        except Exception as e:
            logging.warning(f"Error parsing PVC describe data: {e}")
            return {}
    
    def _parse_pv_describe_data(self, describe_output: str) -> Dict[str, Any]:
        """
        Parse PV describe data and extract attributes
        
        Args:
            describe_output: Output from kubectl describe pv command
            
        Returns:
            Dictionary of PV attributes
        """
        attributes = {}
        
        try:
            lines = describe_output.split('\n')
            
            for line in lines:
                line = line.strip()
                
                # Skip empty lines and section headers
                if not line or (line.endswith(':') and not ':' in line[:-1]):
                    continue
                
                # Extract key-value pairs
                if ':' in line and 'Events:' not in line:
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip()
                    
                    # Store as attributes with proper naming
                    attr_key = key.replace(' ', '')
                    attributes[attr_key] = value
                    
                    # Extract specific important attributes
                    if key == 'Status':
                        attributes['PVStatus'] = value
                    elif key == 'Claim':
                        attributes['BoundClaim'] = value
                    elif key == 'Storage Class':
                        attributes['StorageClass'] = value
                    elif key == 'Access Modes':
                        attributes['AccessModes'] = value
                    elif key == 'VolumeMode':
                        attributes['VolumeMode'] = value
                    elif key == 'Capacity':
                        attributes['Capacity'] = value
                    elif key == 'Node Affinity':
                        attributes['NodeAffinity'] = value
            
            # Extract CSI volume attributes if present
            csi_attributes = self._extract_csi_attributes(describe_output)
            if csi_attributes:
                attributes.update(csi_attributes)
                
            return attributes
            
        except Exception as e:
            logging.warning(f"Error parsing PV describe data: {e}")
            return {}
    
    def _extract_csi_attributes(self, describe_output: str) -> Dict[str, Any]:
        """
        Extract CSI-specific attributes from describe output
        
        Args:
            describe_output: Output from kubectl describe command
            
        Returns:
            Dictionary of CSI attributes
        """
        csi_attributes = {}
        in_csi_section = False
        
        try:
            lines = describe_output.split('\n')
            
            for line in lines:
                line = line.strip()
                
                # Start of CSI section
                if 'CSI:' in line:
                    in_csi_section = True
                    continue
                
                # End of CSI section
                if in_csi_section and line.endswith(':') and not ':' in line[:-1]:
                    in_csi_section = False
                    break
                
                # Extract CSI attributes
                if in_csi_section and ':' in line:
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip()
                    
                    # Store with CSI prefix
                    attr_key = f"CSI{key.replace(' ', '')}"
                    csi_attributes[attr_key] = value
                    
                    # Extract volumeHandle specifically
                    if key == 'volumeHandle':
                        csi_attributes['CSIVolumeHandle'] = value
                    elif key == 'driver':
                        csi_attributes['CSIDriver'] = value
            
            return csi_attributes
            
        except Exception as e:
            logging.warning(f"Error extracting CSI attributes: {e}")
            return {}
    
    def _parse_volume_describe_data(self, describe_output: str) -> Dict[str, Any]:
        """
        Parse Volume describe data and extract attributes
        
        Args:
            describe_output: Output from kubectl describe volume command
            
        Returns:
            Dictionary of Volume attributes
        """
        attributes = {}
        
        try:
            lines = describe_output.split('\n')
            
            for line in lines:
                line = line.strip()
                
                # Skip empty lines and section headers
                if not line or (line.endswith(':') and not ':' in line[:-1]):
                    continue
                
                # Extract key-value pairs
                if ':' in line and 'Events:' not in line:
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip()
                    
                    # Store as attributes with proper naming
                    attr_key = key.replace(' ', '')
                    attributes[attr_key] = value
                    
                    # Extract specific important attributes
                    if key == 'Status':
                        attributes['VolumeStatus'] = value
                    elif key == 'Health':
                        attributes['Health'] = value
                    elif key == 'CSI Status':
                        attributes['CSIStatus'] = value
                    elif key == 'Location':
                        attributes['Location'] = value
                    elif key == 'Storage Class':
                        attributes['StorageClass'] = value
                    elif key == 'Size':
                        attributes['Size'] = value
            
            return attributes
            
        except Exception as e:
            logging.warning(f"Error parsing Volume describe data: {e}")
            return {}
    
    def _parse_drive_describe_data(self, describe_output: str) -> Dict[str, Any]:
        """
        Parse Drive describe data and extract attributes
        
        Args:
            describe_output: Output from kubectl describe drive command
            
        Returns:
            Dictionary of Drive attributes
        """
        attributes = {}
        
        try:
            lines = describe_output.split('\n')
            
            for line in lines:
                line = line.strip()
                
                # Skip empty lines and section headers
                if not line or (line.endswith(':') and not ':' in line[:-1]):
                    continue
                
                # Extract key-value pairs
                if ':' in line and 'Events:' not in line:
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip()
                    
                    # Store as attributes with proper naming
                    attr_key = key.replace(' ', '')
                    attributes[attr_key] = value
                    
                    # Extract specific important attributes
                    if key == 'Health':
                        attributes['Health'] = value
                    elif key == 'Status':
                        attributes['Status'] = value
                    elif key == 'Path':
                        attributes['Path'] = value
                    elif key == 'Node':
                        attributes['NodeName'] = value
                    elif key == 'Size':
                        attributes['Size'] = value
                    elif key == 'Type':
                        attributes['Type'] = value
                    elif key == 'Usage':
                        attributes['Usage'] = value
                    elif key == 'Serial Number':
                        attributes['SerialNumber'] = value
            
            return attributes
            
        except Exception as e:
            logging.warning(f"Error parsing Drive describe data: {e}")
            return {}
    
    def _parse_node_describe_data(self, describe_output: str) -> Dict[str, Any]:
        """
        Parse Node describe data and extract attributes
        
        Args:
            describe_output: Output from kubectl describe node command
            
        Returns:
            Dictionary of Node attributes
        """
        attributes = {}
        
        try:
            lines = describe_output.split('\n')
            current_section = None
            
            for line in lines:
                line = line.strip()
                
                # Skip empty lines
                if not line:
                    continue
                
                # Check for section headers
                if line.endswith(':') and not ':' in line[:-1]:
                    current_section = line[:-1]
                    continue
                
                # Extract key-value pairs
                if ':' in line and current_section != 'Events':
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip()
                    
                    # Store as attributes with proper naming
                    attr_key = f"{current_section.replace(' ', '')}{key.replace(' ', '')}" if current_section else key.replace(' ', '')
                    attributes[attr_key] = value
            
            # Extract node conditions
            conditions = self._extract_node_conditions(describe_output)
            if conditions:
                attributes['Conditions'] = conditions
                
                # Set specific condition flags for easier access
                for condition in conditions:
                    condition_type = condition.get('Type')
                    condition_status = condition.get('Status') == 'True'
                    if condition_type:
                        attributes[condition_type] = condition_status
            
            # Extract node capacity and allocatable resources
            capacity = self._extract_node_resources(describe_output, 'Capacity')
            if capacity:
                attributes['Capacity'] = capacity
                
            allocatable = self._extract_node_resources(describe_output, 'Allocatable')
            if allocatable:
                attributes['Allocatable'] = allocatable
                
            return attributes
            
        except Exception as e:
            logging.warning(f"Error parsing Node describe data: {e}")
            return {}
    
    def _extract_node_conditions(self, describe_output: str) -> List[Dict[str, str]]:
        """
        Extract node conditions from node describe output
        
        Args:
            describe_output: Output from kubectl describe node command
            
        Returns:
            List of condition dictionaries
        """
        conditions = []
        in_conditions_section = False
        current_condition = None
        
        try:
            lines = describe_output.split('\n')
            
            for line in lines:
                line = line.strip()
                
                # Start of Conditions section
                if line == 'Conditions:':
                    in_conditions_section = True
                    continue
                
                # End of Conditions section
                if in_conditions_section and line.endswith(':') and not ':' in line[:-1] and line != 'Conditions:':
                    in_conditions_section = False
                    break
                
                # New condition type
                if in_conditions_section and line.startswith('Type:'):
                    # Save previous condition
                    if current_condition:
                        conditions.append(current_condition)
                    
                    current_condition = {'Type': line.split(':', 1)[1].strip()}
                    continue
                
                # Condition attributes
                if in_conditions_section and current_condition and ':' in line:
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip()
                    current_condition[key] = value
            
            # Add the last condition
            if current_condition:
                conditions.append(current_condition)
                
            return conditions
            
        except Exception as e:
            logging.warning(f"Error extracting node conditions: {e}")
            return []
    
    def _extract_node_resources(self, describe_output: str, section_name: str) -> Dict[str, str]:
        """
        Extract node resource information (Capacity or Allocatable)
        
        Args:
            describe_output: Output from kubectl describe node command
            section_name: Section name to extract ('Capacity' or 'Allocatable')
            
        Returns:
            Dictionary of resource values
        """
        resources = {}
        in_section = False
        
        try:
            lines = describe_output.split('\n')
            
            for line in lines:
                line = line.strip()
                
                # Start of section
                if line == f'{section_name}:':
                    in_section = True
                    continue
                
                # End of section
                if in_section and line.endswith(':') and not ':' in line[:-1]:
                    in_section = False
                    break
                
                # Resource values
                if in_section and ':' in line:
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip()
                    resources[key] = value
            
            return resources
            
        except Exception as e:
            logging.warning(f"Error extracting node {section_name}: {e}")
            return {}
    
    def _parse_storage_class_describe_data(self, describe_output: str) -> Dict[str, Any]:
        """
        Parse StorageClass describe data and extract attributes
        
        Args:
            describe_output: Output from kubectl describe storageclass command
            
        Returns:
            Dictionary of StorageClass attributes
        """
        attributes = {}
        
        try:
            lines = describe_output.split('\n')
            
            for line in lines:
                line = line.strip()
                
                # Skip empty lines and section headers
                if not line or (line.endswith(':') and not ':' in line[:-1]):
                    continue
                
                # Extract key-value pairs
                if ':' in line and 'Events:' not in line:
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip()
                    
                    # Store as attributes with proper naming
                    attr_key = key.replace(' ', '')
                    attributes[attr_key] = value
                    
                    # Extract specific important attributes
                    if key == 'Provisioner':
                        attributes['Provisioner'] = value
                    elif key == 'Reclaim Policy':
                        attributes['ReclaimPolicy'] = value
                    elif key == 'Volume Binding Mode':
                        attributes['VolumeBindingMode'] = value
                    elif key == 'Allow Volume Expansion':
                        attributes['AllowVolumeExpansion'] = (value.lower() == 'true')
            
            # Extract parameters
            parameters = self._extract_storage_class_parameters(describe_output)
            if parameters:
                attributes['Parameters'] = parameters
                
            return attributes
            
        except Exception as e:
            logging.warning(f"Error parsing StorageClass describe data: {e}")
            return {}
    
    def _extract_storage_class_parameters(self, describe_output: str) -> Dict[str, str]:
        """
        Extract StorageClass parameters from describe output
        
        Args:
            describe_output: Output from kubectl describe storageclass command
            
        Returns:
            Dictionary of parameters
        """
        parameters = {}
        in_parameters_section = False
        
        try:
            lines = describe_output.split('\n')
            
            for line in lines:
                line = line.strip()
                
                # Start of Parameters section
                if line == 'Parameters:':
                    in_parameters_section = True
                    continue
                
                # End of Parameters section
                if in_parameters_section and line.endswith(':') and not ':' in line[:-1]:
                    in_parameters_section = False
                    break
                
                # Parameter values
                if in_parameters_section and ':' in line:
                    key, value = line.split(':', 1)
                    key = key.strip()
                    value = value.strip()
                    parameters[key] = value
            
            return parameters
            
        except Exception as e:
            logging.warning(f"Error extracting StorageClass parameters: {e}")
            return {}
    
    def _extract_events_from_describe(self, describe_output: str) -> List[Dict[str, str]]:
        """
        Extract events from kubectl describe output
        
        Args:
            describe_output: Output from kubectl describe command
            
        Returns:
            List of event dictionaries with type, reason, message, etc.
        """
        events = []
        in_events_section = False
        
        try:
            lines = describe_output.split('\n')
            
            for i, line in enumerate(lines):
                if line.strip() == 'Events:':
                    in_events_section = True
                    continue
                
                if in_events_section and line.strip():
                    # Skip header line if present (contains Type, Reason, Age, etc.)
                    if any(header in line for header in ['Type', 'Reason', 'Age', 'From', 'Message']):
                        continue
                    
                    # Parse event line
                    event = self._parse_event_line(line)
                    if event:
                        events.append(event)
            
            return events
            
        except Exception as e:
            logging.warning(f"Error extracting events from describe output: {e}")
            return []
    
    def _parse_event_line(self, line: str) -> Optional[Dict[str, str]]:
        """
        Parse a single event line from kubectl describe output
        
        Args:
            line: Single line from events section
            
        Returns:
            Dictionary with event details or None if parsing failed
        """
        try:
            # Different kubectl versions have different formats for events
            # Try to handle common formats
            
            # Format 1: Type    Reason     Age    From               Message
            #           Normal  Scheduled  2m     default-scheduler  Successfully assigned default/pod-name to node-name
            parts = line.strip().split(None, 4)
            if len(parts) >= 5:
                return {
                    'type': parts[0],
                    'reason': parts[1],
                    'age': parts[2],
                    'from': parts[3],
                    'message': parts[4]
                }
            
            # Format 2: Warning  FailedMount  30s (x3 over 5m)  kubelet  MountVolume.SetUp failed...
            parts = re.split(r'\s+', line.strip(), 4)
            if len(parts) >= 5:
                return {
                    'type': parts[0],
                    'reason': parts[1],
                    'age': parts[2],
                    'from': parts[3],
                    'message': parts[4]
                }
            
            # Format 3: More compact format with fewer fields
            parts = re.split(r'\s+', line.strip(), 2)
            if len(parts) >= 3:
                return {
                    'type': parts[0],
                    'reason': parts[0],  # Use type as reason if not explicitly provided
                    'message': parts[2]
                }
            
            # If we can't parse the format but have Warning/Error keywords, create a basic event
            if any(keyword in line for keyword in ['Warning', 'Error', 'Failed']):
                return {
                    'type': 'Warning' if 'Warning' in line else 'Error',
                    'reason': 'Unknown',
                    'message': line.strip()
                }
            
            return None
            
        except Exception as e:
            logging.debug(f"Error parsing event line '{line}': {e}")
            return None
    
    def _add_events_as_issues(self, node_id: str, events: List[Dict[str, str]]):
        """
        Add events as issues to the Knowledge Graph
        
        Args:
            node_id: Node ID to attach issues to
            events: List of event dictionaries
        """
        try:
            # Filter for warning or error events
            for event in events:
                event_type = event.get('type', '')
                
                # Only process Warning or Error events
                if event_type not in ['Warning', 'Error', 'Failed']:
                    continue
                
                reason = event.get('reason', 'Unknown')
                message = event.get('message', 'No message provided')
                
                # Determine severity based on event type
                severity = self.EVENT_SEVERITY_LEVELS.get(event_type, 'medium')
                
                # Add issue to the Knowledge Graph
                self.knowledge_graph.add_issue(
                    node_id,
                    f"event_{reason.lower()}",
                    f"Event {reason}: {message}",
                    severity
                )
                
        except Exception as e:
            logging.warning(f"Error adding events as issues for node {node_id}: {e}")
            
    async def _add_hardware_system_entity(self):
        """Add hardware system entity with comprehensive information from system diagnostic tools"""
        try:
            import json
            from tools.diagnostics.system import get_system_hardware_info, df_command, lsblk_command, mount_command, dmesg_command
            
            logging.info("Adding hardware system entity with comprehensive information...")
            
            # Initialize hardware info dictionary
            hardware_info = {}
            
            # Get node names from collected data or use a default list
            node_names = self.collected_data.get('node_names', [])
            if not node_names and 'nodes' in self.collected_data.get('kubernetes', {}):
                # Extract node names from kubernetes data if available
                node_names = self._parse_cluster_node_names(self.collected_data['kubernetes']['nodes'])
            
            # If still no nodes found, use a default node name
            if not node_names:
                node_names = ['localhost']
                logging.warning("No node names found in collected data, using 'localhost' as default")
            
            # Collect hardware information for each node
            for node_name_item in node_names:
                node_hardware_info = {}
                
                # Get system hardware info (manufacturer, product name)
                try:
                    hw_info_str = get_system_hardware_info.invoke({"node_name": node_name_item})
                    hw_info = json.loads(hw_info_str)
                    node_hardware_info['system_info'] = hw_info
                except Exception as e:
                    logging.warning(f"Error getting hardware info for {node_name_item}: {e}")
                    node_hardware_info['system_info'] = {"error": str(e)}
                
                # Get disk space information
                try:
                    df_output = df_command.invoke({"node_name": node_name_item})
                    node_hardware_info['disk_space'] = df_output
                except Exception as e:
                    logging.warning(f"Error getting disk space for {node_name_item}: {e}")
                    node_hardware_info['disk_space'] = f"Error: {str(e)}"
                
                # Get block device information
                try:
                    lsblk_output = lsblk_command.invoke({"node_name": node_name_item, "options":"-o NAME,SIZE,TYPE,MOUNTPOINT"})
                    node_hardware_info['block_devices'] = lsblk_output
                except Exception as e:
                    logging.warning(f"Error getting block devices for {node_name_item}: {e}")
                    node_hardware_info['block_devices'] = f"Error: {str(e)}"
                
                # Get mount information
                try:
                    mount_output = mount_command.invoke({"node_name": node_name_item})
                    node_hardware_info['mounts'] = mount_output
                except Exception as e:
                    logging.warning(f"Error getting mount info for {node_name_item}: {e}")
                    node_hardware_info['mounts'] = f"Error: {str(e)}"
                
                # Get recent kernel messages related to storage
                try:
                    dmesg_output = dmesg_command.invoke({"node_name": node_name_item, "options": "| grep -i -E 'storage|disk|drive|volume|mount'"})
                    node_hardware_info['storage_messages'] = dmesg_output
                except Exception as e:
                    logging.warning(f"Error getting storage messages for {node_name_item}: {e}")
                    node_hardware_info['storage_messages'] = f"Error: {str(e)}"
                
                # Add to overall hardware info
                hardware_info[node_name_item] = node_hardware_info
            
            # Add hardware system entity to knowledge graph
            hardware_id = self.knowledge_graph.add_gnode_system_entity(
                "hardware", "system_info",
                description="System hardware information",
                hardware_info=hardware_info
            )
            
            # Analyze hardware info for issues
            hardware_issues = self._analyze_hardware_info(hardware_info, hardware_id)
            
            # Add issues to knowledge graph
            for issue in hardware_issues:
                self.knowledge_graph.add_issue(
                    hardware_id,
                    issue['type'],
                    issue['description'],
                    issue['severity']
                )
            
            # Link hardware entity to nodes
            for node_name in hardware_info.keys():
                node_id = f"gnode:Node:{node_name}"
                if self.knowledge_graph.graph.has_node(node_id):
                    self.knowledge_graph.add_relationship(hardware_id, node_id, "describes")
                    self.knowledge_graph.add_relationship(node_id, hardware_id, "described_by")
            
            logging.info(f"Added hardware system entity with information for {len(hardware_info)} nodes")
            
        except Exception as e:
            error_msg = f"Error adding hardware system entity: {str(e)}"
            logging.error(error_msg)
            self.collected_data['errors'].append(error_msg)
    
    def _analyze_hardware_info(self, hardware_info, hardware_id):
        """Analyze hardware information for potential issues"""
        issues = []
        
        for node_name, info in hardware_info.items():
            # Check disk space
            if 'disk_space' in info and isinstance(info['disk_space'], str):
                df_output = info['disk_space']
                # Look for filesystems with high usage (>90%)
                for line in df_output.split('\n'):
                    if '%' in line:
                        parts = line.split()
                        if len(parts) >= 5:
                            try:
                                # Extract usage percentage, typically in format like "94%"
                                usage_str = next((p for p in parts if p.endswith('%')), None)
                                if usage_str:
                                    usage = int(usage_str.strip('%'))
                                    if usage > 90:
                                        filesystem = parts[0] if len(parts) > 0 else "unknown"
                                        mount_point = parts[-1] if len(parts) > 5 else "unknown"
                                        issues.append({
                                            'type': 'disk_space',
                                            'description': f"High disk usage ({usage}%) on {filesystem} mounted at {mount_point} on node {node_name}",
                                            'severity': 'high' if usage > 95 else 'medium'
                                        })
                            except (ValueError, IndexError) as e:
                                logging.debug(f"Error parsing disk usage: {e} for line: {line}")
            
            # Check for storage-related error messages
            if 'storage_messages' in info and isinstance(info['storage_messages'], str):
                storage_msgs = info['storage_messages']
                error_keywords = ['error', 'fail', 'ioerr', 'i/o error', 'read-only', 'timeout']
                
                for line in storage_msgs.split('\n'):
                    if any(keyword in line.lower() for keyword in error_keywords):
                        issues.append({
                            'type': 'storage_error',
                            'description': f"Storage-related error on node {node_name}: {line.strip()}",
                            'severity': 'high'
                        })
            
            # Check system hardware info
            if 'system_info' in info and isinstance(info['system_info'], dict):
                system_info = info['system_info']
                
                # Check if this is a virtual machine
                vm_manufacturers = ["vmware", "qemu", "virtualbox", "xen", "kvm", "microsoft", "innotek", "parallels"]
                vm_products = ["virtual", "vm", "vmware", "kvm", "virtualbox", "xen", "hyperv", "qemu", "parallels"]

                is_vm = False
                vm_evidence = []

                # Check manufacturer
                if any(vm_term in system_info['manufacturer'].lower() for vm_term in vm_manufacturers):
                    is_vm = True
                    vm_evidence.append(f"Manufacturer '{system_info['manufacturer']}' indicates a virtual machine")

                # Check product name
                if any(vm_term in system_info['product_name'].lower() for vm_term in vm_products):
                    is_vm = True
                    vm_evidence.append(f"Product name '{system_info['product_name']}' indicates a virtual machine")

                if is_vm:
                    issues.append({
                        'type': 'virtual_machine',
                        'description': f"Node {node_name} is a virtual machine: {', '.join(vm_evidence)}",
                        'severity': 'high'
                    })

                # Check for error in system info collection
                if 'error' in system_info:
                    issues.append({
                        'type': 'hardware_info',
                        'description': f"Error collecting hardware info on node {node_name}: {system_info['error']}",
                        'severity': 'medium'
                    })
        
        return issues
    
    async def _load_historical_experience(self):
        """
        Load historical experience data from the configured file path
        and add it to the knowledge graph
        """
        import os
        import json
        
        try:
            # Get file path from configuration or use default if not configured
            historical_experience_file = self.config.get('historical_experience', {}).get('file_path', "historical_experience.json")
            logging.info(f"Loading historical experience data from {historical_experience_file}")
            
            if not os.path.exists(historical_experience_file):
                error_msg = f"Historical experience file {historical_experience_file} not found"
                logging.warning(error_msg)
                self.collected_data['errors'].append(error_msg)
                return
            
            try:
                with open(historical_experience_file, 'r') as f:
                    historical_experiences = json.load(f)
            except json.JSONDecodeError as e:
                error_msg = f"Error parsing historical experience file: {str(e)}"
                logging.error(error_msg)
                self.collected_data['errors'].append(error_msg)
                return
            except Exception as e:
                error_msg = f"Error reading historical experience file: {str(e)}"
                logging.error(error_msg)
                self.collected_data['errors'].append(error_msg)
                return
            
            if not isinstance(historical_experiences, list):
                error_msg = f"Historical experience data should be a list of objects"
                logging.error(error_msg)
                self.collected_data['errors'].append(error_msg)
                return
            
            # Add each historical experience to the knowledge graph
            for idx, experience in enumerate(historical_experiences):
                # Map new field names to old field names for backward compatibility
                field_mapping = {
                    'observation': 'phenomenon',
                    'diagnosis': 'root_cause',
                    'investigation': 'localization_method',
                    'resolution': 'resolution_method'
                }
                
                # Create a copy of experience with mapped fields
                mapped_experience = experience.copy()
                
                # For each new field name, check if it exists and map to old field name if not present
                for new_field, old_field in field_mapping.items():
                    # If new field exists but old field doesn't, copy value to old field
                    if new_field in experience and old_field not in experience:
                        mapped_experience[old_field] = experience[new_field]
                    # If old field exists but new field doesn't, copy value to new field
                    elif old_field in experience and new_field not in experience:
                        mapped_experience[new_field] = experience[old_field]
                
                # Add historical experience node to the knowledge graph with all attributes
                experience_id = f"hist_{idx+1}"
                he_id = self.knowledge_graph.add_gnode_historical_experience(
                    experience_id=experience_id,
                    **mapped_experience  # Pass all attributes to the updated method
                )
                
                # Link historical experience to related system components based on phenomenon
                self._link_historical_experience_to_components(he_id, experience)
            
            logging.info(f"Successfully loaded {len(historical_experiences)} historical experiences")
            
        except Exception as e:
            error_msg = f"Error loading historical experience data: {str(e)}"
            logging.error(error_msg)
            self.collected_data['errors'].append(error_msg)
    
    def _link_historical_experience_to_components(self, he_id: str, experience: Dict[str, str]):
        """
        Link historical experience node to related system components based on phenomenon
        
        Args:
            he_id: Historical experience node ID
            experience: Historical experience data
        """
        try:
            # Try to get phenomenon, fall back to observation if not found
            phenomenon = experience.get('phenomenon', experience.get('observation', '')).lower()
            
            # Link to logs if phenomenon mentions logs
            if 'logs' in phenomenon:
                log_nodes = self.knowledge_graph.find_nodes_by_type('System')
                for log_id in log_nodes:
                    node_attrs = self.knowledge_graph.graph.nodes[log_id]
                    if node_attrs.get('subtype') == 'logs':
                        self.knowledge_graph.add_relationship(he_id, log_id, "related_to")
                        logging.debug(f"Added relationship: {he_id} -> {log_id}")
            
            # Link to drives if phenomenon mentions volume/disk/drive
            if any(term in phenomenon for term in ['volume', 'disk', 'drive']):
                drive_nodes = self.knowledge_graph.find_nodes_by_type('Drive')
                for drive_id in drive_nodes:
                    self.knowledge_graph.add_relationship(he_id, drive_id, "related_to")
                    logging.debug(f"Added relationship: {he_id} -> {drive_id}")
            
            # Link to PVCs if phenomenon mentions PVC
            if 'pvc' in phenomenon:
                pvc_nodes = self.knowledge_graph.find_nodes_by_type('PVC')
                for pvc_id in pvc_nodes:
                    self.knowledge_graph.add_relationship(he_id, pvc_id, "related_to")
                    logging.debug(f"Added relationship: {he_id} -> {pvc_id}")
            
            # Link to pods if phenomenon mentions pod
            if 'pod' in phenomenon:
                pod_nodes = self.knowledge_graph.find_nodes_by_type('Pod')
                for pod_id in pod_nodes:
                    self.knowledge_graph.add_relationship(he_id, pod_id, "related_to")
                    logging.debug(f"Added relationship: {he_id} -> {pod_id}")
                    
        except Exception as e:
            logging.warning(f"Error linking historical experience {he_id} to components: {str(e)}")
</file>

<file path="phases/phase_remediation.py">
#!/usr/bin/env python3
"""
Phase 2: Remediation for Kubernetes Volume Troubleshooting

This module contains the implementation of Phase 2 (Remediation)
which executes fix plans based on analysis from Phase 1.
"""

import logging
import asyncio
from typing import Dict, List, Any, Optional, Tuple
from rich.console import Console
from rich.panel import Panel
from tools.core.mcp_adapter import get_mcp_adapter
from phases.llm_factory import LLMFactory
from langgraph.graph import StateGraph
from tools.diagnostics.hardware import xfs_repair_check  # Importing the xfs_repair_check tool
from phases.utils import format_historical_experiences_from_collected_info, handle_exception
from llm_graph.graphs.phase2_llm_graph import Phase2LLMGraph

logger = logging.getLogger(__name__)

class RemediationPhase:
    """
    Implementation of Phase 2: Remediation
    
    This class handles the implementation of fix plans to resolve
    the identified issues with volume I/O.
    """
    
    def __init__(self, collected_info: Dict[str, Any], config_data: Dict[str, Any]):
        """
        Initialize the Remediation Phase
        
        Args:
            collected_info: Pre-collected diagnostic information from Phase 0
            config_data: Configuration data for the system
        """
        self.collected_info = collected_info
        self.config_data = config_data
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.console = Console()
        self.interactive_mode = config_data.get('troubleshoot', {}).get('interactive_mode', False)
        
        # Get MCP adapter and tools
        self.mcp_adapter = get_mcp_adapter()
        self.mcp_tools = []
        
        # Get MCP tools for phase2 if available
        if self.mcp_adapter:
            self.mcp_tools = self.mcp_adapter.get_tools_for_phase('phase2')
            if self.mcp_tools:
                self.logger.info(f"Loaded {len(self.mcp_tools)} MCP tools for Phase2")
                
        # Initialize LLM factory
        self.llm_factory = LLMFactory(self.config_data)
    
    async def run_remediation_with_graph(self, query: str, graph: StateGraph, timeout_seconds: int = 1800) -> str:
        """
        Run remediation using the provided LangGraph StateGraph
        
        Args:
            query: The initial query to send to the graph
            graph: LangGraph StateGraph to use
            timeout_seconds: Maximum execution time in seconds
            
        Returns:
            str: Remediation result
        """
        try:
            formatted_query = {"messages": [{"role": "user", "content": query}]}
            
            # Show the remediation panel
            self.console.print("\n")
            self.console.print(Panel(
                "[yellow]Starting remediation with LangGraph...\nThis may take a few minutes to complete.", 
                title="[bold green]Remediation Phase",
                border_style="green"
            ))
            
            # Run graph with timeout
            try:
                response = await asyncio.wait_for(
                    graph.ainvoke(formatted_query, config={"recursion_limit": 100}),
                    timeout=timeout_seconds
                )
                self.console.print("[green]Remediation complete![/green]")
            except asyncio.TimeoutError:
                self.console.print("[red]Remediation timed out![/red]")
                return "Remediation phase timed out - manual intervention may be required"
            except Exception as e:
                self.console.print(f"[red]run_remediation_with_graph Remediation failed: {str(e)}[/red]")
                return f"run_remediation_with_graph Remediation failed: {str(e)}"
            
            # Extract remediation results
            return self._extract_final_message(response)
            
        except Exception as e:
            error_msg = handle_exception("run_remediation_with_graph", e, self.logger)
            return f"Error in remediation: {str(e)}"
    
    def _extract_final_message(self, response: Dict[str, Any]) -> str:
        """
        Extract the final message from a graph response
        
        Args:
            response: Response from the graph
            
        Returns:
            str: Final message content
        """
        if not response.get("messages"):
            return "Failed to generate remediation results"
            
        if isinstance(response["messages"], list):
            return response["messages"][-1].content
        else:
            return response["messages"].content
    
    async def execute_fix_plan(self, phase1_final_response: str, message_list: List[Dict[str, str]] = None) -> Tuple[str, List[Dict[str, str]]]:
        """
        Execute the fix plan from Phase 1 analysis
        
        Args:
            phase1_final_response: Response from Phase 1 containing root cause and fix plan
            message_list: Optional message list for chat mode
            
        Returns:
            Tuple[str, List[Dict[str, str]]]: (Remediation result, Updated message list)
        """
        try:
            # Initialize message list if not provided
            if message_list is None:
                # System prompt for Phase2
                system_prompt = """You are an expert Kubernetes storage troubleshooter. Your task is to execute the Fix Plan to resolve volume I/O errors in Kubernetes pods.

TASK:
1. Execute the Fix Plan to resolve the identified issues
2. Validate the fixes to ensure they resolved the problem
3. Provide a detailed report of the remediation actions taken

KNOWLEDGE GRAPH TOOLS USAGE:
- When using knowledge graph tools, use the parameters of entity_type and id format:
  * Entity ID formats:
    - Pod: "gnode:Pod:<namespace>/<name>" (example: "gnode:Pod:default/test-pod-1-0")
    - PVC: "gnode:PVC:<namespace>/<name>" (example: "gnode:PVC:default/test-pvc-1")
    - PV: "gnode:PV:<name>" (example: "gnode:PV:pv-test-123")
    - Drive: "gnode:Drive:<uuid>" (example: "gnode:Drive:a1b2c3d4-e5f6")
    - Node: "gnode:Node:<name>" (example: "gnode:Node:kind-control-plane")
    - StorageClass: "gnode:StorageClass:<name>" (example: "gnode:StorageClass:csi-baremetal-sc")
    - LVG: "gnode:LVG:<name>" (example: "gnode:LVG:lvg-1")
    - AC: "gnode:AC:<name>" (example: "gnode:AC:ac-node1-ssd")
    - Volume: "gnode:Volume:<namespace>/<name>" (example: "gnode:Volume:default/vol-1")
    - System: "gnode:System:<entity_name>" (example: "gnode:System:kernel")
    - ClusterNode: "gnode:ClusterNode:<name>" (example: "gnode:ClusterNode:worker-1")
    - HistoricalExperience: "gnode:HistoricalExperience:<experience_id>" (example: "gnode:HistoricalExperience:exp-001")

  * Helper tools for generating entity IDs:
    - Pod: kg_get_entity_of_pod(namespace, name)  returns "gnode:Pod:namespace/name"
    - PVC: kg_get_entity_of_pvc(namespace, name)  returns "gnode:PVC:namespace/name"
    - PV: kg_get_entity_of_pv(name)  returns "gnode:PV:name"
    - Drive: kg_get_entity_of_drive(uuid)  returns "gnode:Drive:uuid"
    - Node: kg_get_entity_of_node(name)  returns "gnode:Node:name"
    - StorageClass: kg_get_entity_of_storage_class(name)  returns "gnode:StorageClass:name"
    - LVG: kg_get_entity_of_lvg(name)  returns "gnode:LVG:name"
    - AC: kg_get_entity_of_ac(name)  returns "gnode:AC:name"
    - Volume: kg_get_entity_of_volume(namespace, name)  returns "gnode:Volume:namespace/name"
    - System: kg_get_entity_of_system(entity_name)  returns "gnode:System:entity_name"
    - ClusterNode: kg_get_entity_of_cluster_node(name)  returns "gnode:ClusterNode:name"
    - HistoricalExperience: kg_get_entity_of_historical_experience(experience_id)  returns "gnode:HistoricalExperience:experience_id"

- Start with discovery tools to understand what's in the Knowledge Graph:
  * Use kg_list_entity_types() to discover available entity types and their counts
  * Use kg_list_entities(entity_type) to find specific entities of a given type
  * Use kg_list_relationship_types() to understand how entities are related

- Then use detailed query tools:
  * Use kg_get_entity_info(entity_type, id) to retrieve detailed information about specific entities
  * Use kg_get_related_entities(entity_type, id) to understand relationships between components
  * Use kg_get_all_issues() to find already detected issues in the system
  * Use kg_find_path(source_entity_type, source_id, target_entity_type, target_id) to trace dependencies

CONSTRAINTS:
- Follow the Fix Plan step by step
- Use only the tools available in the Phase2 tool registry
- Validate each fix to ensure it was successful
- Provide a clear, detailed report of all actions taken

OUTPUT FORMAT:
Your response must include:
1. Actions Taken
2. Validation Results
3. Resolution Status
4. Recommendations
"""
                message_list = [
                    {"role": "system", "content": system_prompt},
                    {"role": "assistant", "content": "Fix Plan:\n" + phase1_final_response}
                ]
            
            # Check if streaming is enabled in config
            streaming_enabled = self.config_data.get('llm', {}).get('streaming', False)
            
            # Create Phase2 LLM Graph for remediation
            graph_instance = Phase2LLMGraph(self.config_data)
            graph = graph_instance.initialize_graph()
            
            # Extract and format historical experience data from collected_info
            historical_experiences_formatted = format_historical_experiences_from_collected_info(self.collected_info)
            
            # Updated query message with dynamic data for LangGraph workflow
            query = f"""Phase 2 - Remediation: Execute the fix plan to resolve the identified issue.

Root Cause and Fix Plan: {phase1_final_response}

HISTORICAL EXPERIENCE:
{historical_experiences_formatted}

<<< Note >>>: Please try to fix issue within 30 tool calls.
"""
            
            # Set timeout
            timeout_seconds = self.config_data['troubleshoot']['timeout_seconds']
            
            # Run remediation with graph
            formatted_query = {"messages": [{"role": "user", "content": query}]}
            
            # Show the remediation panel
            self.console.print("\n")
            self.console.print(Panel(
                "[yellow]Starting remediation with LangGraph...\nThis may take a few minutes to complete.", 
                title="[bold green]Remediation Phase",
                border_style="green"
            ))
            
            # Run graph with timeout
            try:
                response = await asyncio.wait_for(
                    graph.ainvoke(formatted_query, config={"recursion_limit": 100}),
                    timeout=timeout_seconds
                )
                self.console.print("[green]Remediation complete![/green]")
            except asyncio.TimeoutError:
                self.console.print("[red]Remediation timed out![/red]")
                remediation_result = "Remediation phase timed out - manual intervention may be required"
                
                # Add timeout message to message list
                message_list.append({"role": "assistant", "content": remediation_result})
                return remediation_result, message_list
            except Exception as e:
                self.console.print(f"[red]execute_fix_plan Remediation failed: {str(e)}[/red]")
                remediation_result = f"execute_fix_plan Remediation failed: {str(e)}"
                
                # Add error message to message list
                message_list.append({"role": "assistant", "content": remediation_result})
                return remediation_result, message_list
            
            # Extract remediation results
            remediation_result = self._extract_final_message(response)
            
            # Add remediation result to message list
            message_list.append({"role": "assistant", "content": remediation_result})
            
            return remediation_result, message_list

        except Exception as e:
            error_msg = handle_exception("execute_fix_plan", e, self.logger)
            
            # Add error message to message list if provided
            if message_list is not None:
                message_list.append({"role": "assistant", "content": error_msg})
            
            return error_msg, message_list


async def run_remediation_phase(phase1_final_response: str, collected_info: Dict[str, Any], 
                              config_data: Dict[str, Any], message_list: List[Dict[str, str]] = None) -> Tuple[str, List[Dict[str, str]]]:
    """
    Run Phase 2: Remediation based on analysis results
    
    Args:
        phase1_final_response: Response from Phase 1 containing root cause and fix plan
        collected_info: Pre-collected diagnostic information from Phase 0
        config_data: Configuration data
        
    Returns:
        Tuple[str, List[Dict[str, str]]]: (Remediation result, Updated message list)
    """
    logging.info("Starting Phase 2: Remediation")
    
    console = Console()
    console.print("\n")
    console.print(Panel(
        "[bold white]Executing fix plan to resolve identified issues...",
        title="[bold green]PHASE 2: REMEDIATION",
        border_style="green",
        padding=(1, 2)
    ))
    
    try:
        # Initialize the remediation phase
        phase = RemediationPhase(collected_info, config_data)
        
        # Execute the fix plan
        result, message_list = await phase.execute_fix_plan(phase1_final_response, message_list)
        
        return result, message_list
        
    except Exception as e:
        error_msg = handle_exception("run_remediation_phase", e, logger)
        # Add error message to message list if provided
        if message_list is not None:
            message_list.append({"role": "assistant", "content": error_msg})
        
        return error_msg, message_list
</file>

<file path="phases/phase_analysis.py">
#!/usr/bin/env python3
"""
Phase 1: ReAct Investigation for Kubernetes Volume Troubleshooting

This module contains the implementation of Phase 1 (ReAct Investigation)
which actively investigates using tools with pre-collected data as base knowledge.
"""

import logging
import asyncio
import datetime
from typing import Dict, List, Any, Optional, Tuple
from rich.console import Console
from rich.panel import Panel
from kubernetes import client
from phases.llm_factory import LLMFactory
from langchain_core.messages import SystemMessage, HumanMessage
from tools.core.mcp_adapter import get_mcp_adapter

from llm_graph.graphs.phase1_llm_graph import Phase1LLMGraph
from tools.diagnostics.hardware import xfs_repair_check  # Importing the xfs_repair_check tool
from phases.utils import format_historical_experiences_from_collected_info, handle_exception

logger = logging.getLogger(__name__)

class AnalysisPhase:
    """
    Implementation of Phase 1: ReAct Investigation
    
    This class handles the active investigation of volume I/O errors
    using the Investigation Plan and pre-collected data.
    """
    
    def __init__(self, collected_info: Dict[str, Any], config_data: Dict[str, Any]):
        """
        Initialize the Analysis Phase
        
        Args:
            collected_info: Pre-collected diagnostic information from Phase 0
            config_data: Configuration data for the system
        """
        self.collected_info = collected_info
        self.config_data = config_data
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.console = Console()
        
        # Get MCP adapter and tools
        self.mcp_adapter = get_mcp_adapter()
        self.mcp_tools = []
        
        # Get MCP tools for phase1 if available
        if self.mcp_adapter:
            self.mcp_tools = self.mcp_adapter.get_tools_for_phase('phase1')
            if self.mcp_tools:
                self.logger.info(f"Loaded {len(self.mcp_tools)} MCP tools for Phase1")
                
        # Initialize LLM factory
        self.llm_factory = LLMFactory(self.config_data)

    def _extract_final_message(self, response: Dict[str, Any]) -> str:
        """
        Extract the final message from a graph response
        
        Args:
            response: Response from the graph
            
        Returns:
            str: Final message content
        """
        if not response.get("messages"):
            return "Failed to generate analysis results"
            
        if isinstance(response["messages"], list):
            return response["messages"][-1].content
        else:
            return response["messages"].content
    
    async def run_investigation(self, pod_name: str, namespace: str, volume_path: str, 
                               investigation_plan: str, message_list: List[Dict[str, str]] = None) -> Tuple[str, List[Dict[str, str]]]:
        """
        Run the investigation based on the Investigation Plan
        
        Args:
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod
            volume_path: Path of the volume with I/O error
            investigation_plan: Investigation Plan generated by the Plan Phase
            message_list: Optional message list for chat mode
            
        Returns:
            Tuple[str, List[Dict[str, str]]]: (Analysis result, Updated message list)
        """
        try:
            if message_list == None:
                message_list = []
            
            # Initialize Phase1LLMGraph
            phase1_graph = Phase1LLMGraph(self.config_data)
            
            # Extract and format historical experience data from collected_info
            historical_experiences_formatted = format_historical_experiences_from_collected_info(self.collected_info)
            
            # Add investigation results to message list if not already present
            message_list.append({"role": "user", "content": "Investigation Results:\n" + investigation_plan})
            
            # Create system message using the prompt manager
            prompt_manager = phase1_graph.get_prompt_manager()
            system_prompt = prompt_manager.get_system_prompt()
            
            # Create user message using the prompt manager
            user_message = prompt_manager.format_user_query(
                query="",
                pod_name=pod_name,
                namespace=namespace,
                volume_path=volume_path,
                investigation_plan=investigation_plan,
                collected_info=self.collected_info
            )
            
            # Prepare initial state for the graph
            initial_state = {
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                "investigation_plan": investigation_plan,
                "pod_name": pod_name,
                "namespace": namespace,
                "volume_path": volume_path,
                "collected_info": self.collected_info
            }
            
            # Set timeout
            timeout_seconds = self.config_data['troubleshoot']['timeout_seconds']
            
            # First show the analysis panel
            self.console.print(Panel(
                "[yellow]Starting analysis with LangGraph...\nThis may take a few minutes to complete.", 
                title="[bold blue]Analysis Phase",
                border_style="blue"
            ))
            
            # Run graph with timeout
            try:
                response = await asyncio.wait_for(
                    phase1_graph.execute(initial_state),
                    timeout=timeout_seconds
                )
                self.console.print("[green]Analysis complete![/green]")
            except asyncio.TimeoutError:
                self.console.print("[red]Analysis timed out![/red]")
                raise
            except Exception as e:
                self.console.print(f"[red]Analysis failed: {str(e)}[/red]")
                raise
            
            # Extract analysis results
            final_message = response.get("analysis_results", "Failed to generate analysis results")
            
            # Add fix plan to message list
            message_list.append({"role": "assistant", "content": final_message})
            
            return final_message, message_list

        except Exception as e:
            error_msg = handle_exception("run_investigation", e, self.logger)
            
            # Add error message to message list if provided
            if message_list is not None:
                message_list.append({"role": "assistant", "content": error_msg})
            
            return error_msg, message_list


def summarize_investigation_results(results: str, llm) -> str:
    """
    Use LLM to summarize investigation results into a concise message (1024 chars)
    
    Args:
        results: Investigation results from Phase1
        llm: LLM instance to use for summarization
        
    Returns:
        str: Summarized investigation results (1024 chars)
    """
    # Prompt the LLM to summarize the results
    prompt = "Summarize the investigation results for Kubernetes pod volume I/O failures into a concise message under 1024 characters, highlighting primary issues."
    
    # Create messages for the LLM
    messages = [
        SystemMessage(content="You are an expert Kubernetes storage troubleshooter. Your task is to summarize investigation results concisely."),
        HumanMessage(content=f"{prompt}\n\nInvestigation Results:\n{results}")
    ]
    
    try:
        # Call the LLM
        response = llm.invoke(messages)
        
        # Extract the summary
        summary = response.content.strip()
        
        # Truncate if necessary
        if len(summary) > 1024:
            summary = summary[:1020] + "..."
            
        return summary
    except Exception as e:
        logging.error(f"Error in summarize_investigation_results: {e}")
        # Return a basic summary in case of failure
        return f"Investigation completed. Error generating detailed summary: {str(e)[:100]}..."

def send_k8s_event(namespace: str, resource_name: str, resource_kind: str, message: str) -> bool:
    """
    Send a Kubernetes event with investigation results
    
    Args:
        namespace: Namespace of the resource
        resource_name: Name of the resource
        resource_kind: Kind of the resource (e.g., Pod)
        message: Event message containing investigation summary
        
    Returns:
        bool: True if event was sent successfully, False otherwise
    """
    try:
        # Create Kubernetes API client
        v1 = client.CoreV1Api()
        
        # Create event metadata
        metadata = client.V1ObjectMeta(
            generate_name="troubleshooting-",
            namespace=namespace
        )
        
        # Create involved object reference
        involved_object = client.V1ObjectReference(
            kind=resource_kind,
            namespace=namespace,
            name=resource_name
        )
        
        # Create event source
        source = client.V1EventSource(
            component="troubleshooting-system"
        )
        
        # Create event
        event = client.CoreV1Event(
            metadata=metadata,
            involved_object=involved_object,
            reason="VolumeIOError",
            message=message,
            type="Error",
            source=source,
            first_timestamp=datetime.datetime.now(datetime.timezone.utc).isoformat(),
            last_timestamp=datetime.datetime.now(datetime.timezone.utc).isoformat(),
        )
        
        # Send event
        v1.create_namespaced_event(namespace, event)
        
        logging.info(f"Kubernetes event sent successfully for {resource_kind}/{resource_name} in namespace {namespace}")
        return True
    except Exception as e:
        logging.error(f"Error sending Kubernetes event: {e}")
        return False


async def run_analysis_phase_with_plan(pod_name: str, namespace: str, volume_path: str, 
                                     collected_info: Dict[str, Any], investigation_plan: str,
                                     config_data: Dict[str, Any], message_list: List[Dict[str, str]] = None) -> Tuple[str, bool, str, List[Dict[str, str]]]:
    """
    Run Phase 1: ReAct Investigation with pre-collected information as base knowledge
    
    Args:
        pod_name: Name of the pod with the error
        namespace: Namespace of the pod
        volume_path: Path of the volume with I/O error
        collected_info: Pre-collected diagnostic information from Phase 0
        investigation_plan: Investigation Plan generated by the Plan Phase
        config_data: Configuration data
        message_list: Optional message list for chat mode
        
    Returns:
        Tuple[str, bool, List[Dict[str, str]]]: (Analysis result, Skip Phase2 flag, Updated message list)
    """
    logging.info("Starting Phase 1: ReAct Investigation with Plan")
    
    console = Console()
    console.print("\n")
    console.print(Panel(
        "[bold white]Executing Investigation Plan to actively investigate volume I/O issue...",
        title="[bold magenta]PHASE 1: REACT INVESTIGATION WITH PLAN",
        border_style="magenta",
        padding=(1, 2)
    ))
    
    try:
        # Initialize the analysis phase
        phase = AnalysisPhase(collected_info, config_data)
        
        # Run the investigation
        result, message_list = await phase.run_investigation(pod_name, namespace, volume_path, investigation_plan, message_list)

        # Process the result
        return process_analysis_result(result, message_list, pod_name, namespace, config_data)
    
    except Exception as e:
        error_msg = handle_exception("run_analysis_phase_with_plan", e, logger)
        
        # Add error message to message list if provided
        if message_list is not None:
            message_list.append({"role": "assistant", "content": error_msg})
        
        return error_msg, False, message_list

def process_analysis_result(result: str, message_list: List[Dict[str, str]], pod_name: str, namespace: str, config_data: Dict[str, Any]) -> Tuple[str, bool, str, List[Dict[str, str]]]:
    """
    Process the analysis result to check for SKIP_PHASE2 marker and send Kubernetes event
    
    Args:
        result: Analysis result
        message_list: Message list for chat mode
        pod_name: Name of the pod with the error
        namespace: Namespace of the pod
        config_data: Configuration data
        
    Returns:
        Tuple[str, bool, List[Dict[str, str]]]: (Processed result, Skip Phase2 flag, Message list)
    """
    # Check if the result contains the SKIP_PHASE2 marker
    skip_phase2 = "SKIP_PHASE2: YES" in result
    
    # Remove the SKIP_PHASE2 marker from the output if present
    if skip_phase2:
        result = result.replace("SKIP_PHASE2: YES", "").strip()
        logging.info("Phase 1 indicated Phase 2 should be skipped")
    
    # Initialize LLM for summarization
    try:
        # Create LLM using the factory
        llm_factory = LLMFactory(config_data)
        llm = llm_factory.create_llm()

        # Generate summary of investigation results
        summary = summarize_investigation_results(result, llm)
        if not summary:
            summary = "No summary generated from investigation results."
        logging.info(f"Investigation summary generated: {summary}")
    
        # Send Kubernetes event
        event_sent = send_k8s_event(namespace, pod_name, "Pod", summary)
        
        if event_sent:
            logging.info(f"Kubernetes event sent with investigation summary for pod {namespace}/{pod_name}")
        else:
            logging.warning(f"Failed to send Kubernetes event for pod {namespace}/{pod_name}")
    except Exception as e:
        logging.error(f"Error during event creation: {e}")
    
    return result, skip_phase2, summary, message_list
</file>

<file path="phases/llm_plan_generator.py">
#!/usr/bin/env python3
"""
LLM-based Plan Generator for Investigation Planning

This module contains utilities for generating investigation plans using LLMs.
"""

import logging
import json
import inspect
import asyncio
from typing import Dict, List, Any, Optional, Tuple
from phases.llm_factory import LLMFactory
from phases.utils import handle_exception, format_json_safely
from langchain_core.messages import BaseMessage, ToolMessage, HumanMessage, SystemMessage, AIMessage
from langchain_core.language_models.chat_models import BaseChatModel
from tools.core.mcp_adapter import get_mcp_adapter

logger = logging.getLogger(__name__)

class LLMPlanGenerator:
    """
    Refines Investigation Plans using Large Language Models
    
    Uses LLMs to refine draft investigation plans by analyzing Knowledge Graph data,
    historical experience, and available Phase1 tools, ensuring a comprehensive
    and actionable final plan without directly invoking the tools.
    """
    
    def __init__(self, config_data: Dict[str, Any] = None):
        """
        Initialize the LLM Plan Generator
        
        Args:
            config_data: Configuration data for the LLM
        """
        self.config_data = config_data or {}
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
        self.llm = self._initialize_llm()
        
        # Get MCP adapter and tools
        self.mcp_adapter = get_mcp_adapter()
        self.mcp_tools = []
        
        # Get MCP tools for plan phase if available
        if self.mcp_adapter:
            self.mcp_tools = self.mcp_adapter.get_tools_for_phase('plan_phase')
            if self.mcp_tools:
                self.logger.info(f"Loaded {len(self.mcp_tools)} MCP tools for Plan Phase")
                
        # No need to initialize ReAct graph here anymore
        # PlanLLMGraph will be created when needed in _call_llm_and_process_response
    
    def _initialize_llm(self) -> Optional[BaseChatModel]:
        """
        Initialize the LLM for plan generation using the LLMFactory
        
        Returns:
            BaseChatModel: Initialized LLM instance or None if initialization fails
        """
        try:
            # Create LLM using the factory
            llm_factory = LLMFactory(self.config_data)
            
            # Check if streaming is enabled in config
            streaming_enabled = self.config_data.get('llm', {}).get('streaming', False)
            
            # Check if React mode is enabled
            use_react = self.config_data.get('plan_phase', {}).get('use_react', False)
            
            # Create LLM with streaming if enabled
            llm = llm_factory.create_llm(
                streaming=streaming_enabled,
                phase_name="plan_phase"
            )
            
            return llm
        except Exception as e:
            error_msg = handle_exception("_initialize_llm", e, self.logger)
            return None
    
    async def refine_plan(self, draft_plan: List[Dict[str, Any]], pod_name: str, namespace: str, 
                   volume_path: str, kg_context: Dict[str, Any], phase1_tools: List[Dict[str, Any]],
                   message_list: List[Dict[str, str]] = None, use_react: bool = True) -> Tuple[str, List[Dict[str, str]]]:
        """
        Refine a draft investigation plan using LLM
        
        Args:
            draft_plan: Draft plan from rule-based generator and static steps
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod
            volume_path: Path of the volume with I/O error
            kg_context: Knowledge Graph context with historical experience
            phase1_tools: Complete Phase1 tool registry with names, descriptions, parameters, and invocation methods
            message_list: Optional message list for chat mode
            use_react: Whether to use React mode (default: True)
            
        Returns:
            Tuple[str, List[Dict[str, str]]]: (Refined Investigation Plan, Updated message list)
        """
        try:
            # Prepare data for LLM
            system_prompt = self._generate_refinement_system_prompt(use_react)
            user_message = self._prepare_user_message(
                draft_plan, pod_name, namespace, volume_path, kg_context, phase1_tools, use_react
            )
            
            # Prepare message list for LLM
            messages = self._prepare_messages(system_prompt, user_message, message_list)
            
            # Call LLM and process response
            return await self._call_llm_and_process_response(
                messages, pod_name, namespace, volume_path, message_list, user_message, use_react
            )
            
        except Exception as e:
            error_msg = handle_exception("refine_plan", e, self.logger)
            return self._handle_plan_generation_error(
                error_msg, pod_name, namespace, volume_path, message_list, user_message
            )
    
    def _prepare_user_message(self, draft_plan: List[Dict[str, Any]], pod_name: str, 
                            namespace: str, volume_path: str, kg_context: Dict[str, Any], 
                            phase1_tools: List[Dict[str, Any]], use_react: bool = True) -> str:
        """
        Prepare user message for LLM with formatted data
        
        Args:
            draft_plan: Draft plan from rule-based generator and static steps
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod
            volume_path: Path of the volume with I/O error
            kg_context: Knowledge Graph context with historical experience
            phase1_tools: Complete Phase1 tool registry
            use_react: Whether to use React mode (default: True)
            
        Returns:
            str: Formatted user message
        """
        # Format input data for LLM context
        kg_context_str = format_json_safely(kg_context, fallback_message="Knowledge Graph context (simplified format)")
        draft_plan_str = format_json_safely(draft_plan, fallback_message="Draft plan (simplified format)")
        phase1_tools_str = format_json_safely(phase1_tools, fallback_message="Phase1 tools (simplified format)")
        
        # Format MCP tools if available
        mcp_tools_str = ""
        if self.mcp_tools:
            mcp_tools_str = format_json_safely(self.mcp_tools, fallback_message="MCP tools (simplified format)")
        
        # Extract and format historical experience data from kg_context
        historical_experiences_formatted = self._format_historical_experiences(kg_context)
        
        # Extract tools already used in draft plan
        used_tools = set()
        for step in draft_plan:
            tool = step.get('tool', '')
            if '(' in tool:
                tool = tool.split('(')[0]
            used_tools.add(tool)
        
        used_tools_str = ", ".join(used_tools)
        
        # Base user message content for both modes
        base_message = f"""# INVESTIGATION PLAN GENERATION
## TARGET: Volume read/write errors in pod {pod_name} (namespace: {namespace}, volume path: {volume_path})

This plan will guide troubleshooting in subsequent phases. Each step will execute specific tools according to this plan.

## BACKGROUND INFORMATION

### 1. KNOWLEDGE GRAPH CONTEXT
Current base knowledge and hardware information. Issues identified in the Knowledge Graph are critical:
{kg_context_str}

### 2. HISTORICAL EXPERIENCE
Learn from previous similar cases to improve your plan:
{historical_experiences_formatted}

### 3. DRAFT PLAN
Static plan steps and preliminary steps from rule-based generator:
{draft_plan_str}

### 4. TOOLS ALREADY USED IN DRAFT PLAN
{used_tools_str}

### 5. AVAILABLE TOOLS FOR PHASE1
These tools will be used in next phases (reference only, do not invoke):
{phase1_tools_str}

### 6. AVAILABLE MCP TOOLS
These MCP tools can be used for cloud-specific diagnostics:
{mcp_tools_str}

When you identify a knowledge gap, use the appropriate MCP tool to gather the information you need. Don't guess or make assumptions when you can use a tool to get accurate information.

## PLANNING INSTRUCTIONS

### PRIMARY OBJECTIVE
Create a comprehensive investigation plan that identifies potential problems and provides specific steps to diagnose and resolve volume read/write errors.

### SPECIFIC TASKS
1. **Task 1:** Analyze the Knowledge Graph context and historical experience to infer and list all possible problems
2. **Task 2:** For each possible problem, create detailed investigation steps using appropriate tools

### PLANNING GUIDELINES
1. Respect existing steps from the draft plan (both rule-based and static steps)
2. Infer potential volume read/write error phenomena and root causes based on Knowledge Graph and historical experience
3. Formulate detailed investigation steps, prioritizing verification steps most likely to identify the issue
4. Add additional steps as needed using available Phase1 tools
5. Reorder steps if necessary for logical flow
6. Ensure all steps reference only tools from the Phase1 tool registry

## IMPORTANT CONSTRAINTS
1. Search related information by MCP tools as referenced in the AVAILABLE TOOLS section at first
2. Include static steps from the draft plan without modification
3. Use historical experience data to inform additional steps and refinements
4. Ensure all tool references follow the format shown in the AVAILABLE TOOLS
5. IMPORTANT: Do not add steps that use tools already present in the draft plan. Each tool should be used at most once in the entire plan

## REQUIRED OUTPUT FORMAT

Investigation Plan:
PossibleProblem 1: [Problem description, e.g., PVC configuration errors, access mode is incorrect]
Step 1: [Description and Reason] | Tool: [tool_name(parameters)] | Expected: [expected]
Step 2: [Description and Reason] | Tool: [tool_name(parameters)] | Expected: [expected]
...
PossibleProblem 2: [Problem description, e.g., Drive status is OFFLINE]
Step 1: [Description and Reason] | Tool: [tool_name(parameters)] | Expected: [expected]
Step 2: [Description and Reason] | Tool: [tool_name(parameters)] | Expected: [expected]
...
"""
        
        # Add mode-specific instructions
        if use_react:
            # React mode instructions
            react_additions = """
## TASK
1. Analyze the available information to understand the context
2. Identify any knowledge gaps that need to be filled
3. Use MCP tools to gather additional information as needed
4. Create a comprehensive Investigation Plan with specific steps to diagnose and resolve the volume I/O error

Please start by analyzing the available information and identifying any knowledge gaps.
"""
            return base_message + react_additions
        else:
            return base_message
    
    def _prepare_messages(self, system_prompt: str, user_message: str, 
                        message_list: List[Dict[str, str]] = None) -> List[Dict[str, str]]:
        """
        Prepare message list for LLM
        
        Args:
            system_prompt: System prompt for LLM
            user_message: User message for LLM
            message_list: Optional existing message list
            
        Returns:
            List[Dict[str, str]]: Prepared message list
        """
        if message_list is None:
            # Create new message list
            return [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_message)
            ]
        
        # Use existing message list
        # If the last message is from the user, we need to regenerate the plan
        if isinstance(message_list[-1], HumanMessage):
            # Keep the system prompt and add the new user message
            return message_list
        else:
            # This is the first call, initialize with system prompt and user message
            return [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_message)
            ]
    
    async def _call_llm_and_process_response(self, messages: List[Dict[str, str]], 
                                     pod_name: str, namespace: str, volume_path: str,
                                     message_list: List[Dict[str, str]] = None,
                                     user_message: str = "", 
                                     use_react: bool = True) -> Tuple[str, List[Dict[str, str]]]:
        """
        Call LLM and process the response
        
        Args:
            messages: Messages for LLM
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod
            volume_path: Path of the volume with I/O error
            message_list: Optional message list for chat mode
            user_message: User message for fallback
            use_react: Whether to use React mode (default: True)
            
        Returns:
            Tuple[str, List[Dict[str, str]]]: (Refined Investigation Plan, Updated message list)
        """
        self.logger.info(f"Calling LLM to generate investigation plan using {'React' if use_react else 'Legacy'} mode")
        self.logger.info("This may take a few moments...")
        
        # Call model with appropriate approach based on mode
        if use_react and self.mcp_tools:
            # React mode with PlanLLMGraph
            self.logger.info(f"Using PlanLLMGraph with {len(self.mcp_tools)} MCP tools")
            
            try:
                # Initialize PlanLLMGraph
                from llm_graph.graphs.plan_llm_graph import PlanLLMGraph
                plan_graph = PlanLLMGraph(self.config_data, messages)
                
                # Prepare initial state
                initial_state = {
                    "messages": messages,
                    "pod_name": pod_name,
                    "namespace": namespace,
                    "volume_path": volume_path,
                    "knowledge_graph": None  # Knowledge graph information is now in messages
                }
                
                # Execute the graph
                final_state = await plan_graph.execute(initial_state)
                
                # Extract the investigation plan
                plan_text = final_state.get("investigation_plan", "")
                
                # Convert messages to message list format
                updated_messages = self._convert_messages_to_message_list(final_state.get("messages", []))
                
                # Update message list with the result
                updated_message_list = self._update_message_list_from_react(message_list, updated_messages)
                
                # Ensure the plan has the required format
                if "Investigation Plan:" not in plan_text:
                    plan_text = self._format_raw_plan(plan_text, pod_name, namespace, volume_path)
                
                self.logger.info("Successfully generated LLM-based investigation plan using PlanLLMGraph")
                return plan_text, updated_message_list
                
            except Exception as e:
                error_msg = handle_exception("_call_llm_and_process_response (PlanLLMGraph)", e, self.logger)
                self.logger.warning(f"Failed to use PlanLLMGraph: {error_msg}, falling back to legacy mode")
                # Fall back to legacy mode if React graph fails
                use_react = False
        
        # Legacy mode (either by choice or as fallback)
        if use_react and self.mcp_tools:
            # Simple React mode with bound tools
            self.logger.info(f"Using simple React mode with {len(self.mcp_tools)} MCP tools")
            response = self.llm.bind_tools(self.mcp_tools).invoke(messages)
        else:
            # Legacy mode or React mode without MCP tools
            response = self.llm.invoke(messages)
        
        # Extract and format the plan
        plan_text = response.content
        
        # Ensure the plan has the required format
        if "Investigation Plan:" not in plan_text:
            plan_text = self._format_raw_plan(plan_text, pod_name, namespace, volume_path)
        
        # Update message list
        updated_message_list = self._update_message_list(messages, message_list, plan_text)
        
        self.logger.info("Successfully generated LLM-based investigation plan")
        return plan_text, updated_message_list
    
    def _update_message_list(self, messages: List[Dict[str, str]], 
                           message_list: List[Dict[str, str]], 
                           plan_text: str) -> List[Dict[str, str]]:
        """
        Update message list with LLM response
        
        Args:
            messages: Messages used for LLM
            message_list: Optional existing message list
            plan_text: Generated plan text
            
        Returns:
            List[Dict[str, str]]: Updated message list
        """
        if message_list is None:
            return messages + [{"role": "assistant", "content": plan_text}]
        
        # If the last message is from the user, append the assistant response
        if message_list[-1]["role"] == "user":
            message_list.append({"role": "assistant", "content": plan_text})
        else:
            # Replace the last message if it's from the assistant
            message_list[-1] = {"role": "assistant", "content": plan_text}
        
        return message_list
    
    def _handle_plan_generation_error(self, error_msg: str, pod_name: str, namespace: str, 
                                    volume_path: str, message_list: List[Dict[str, str]] = None,
                                    user_message: str = "") -> Tuple[str, List[Dict[str, str]]]:
        """
        Handle errors during plan generation
        
        Args:
            error_msg: Error message
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod
            volume_path: Path of the volume with I/O error
            message_list: Optional message list for chat mode
            user_message: User message for fallback
            
        Returns:
            Tuple[str, List[Dict[str, str]]]: (Fallback plan, Updated message list)
        """
        # Generate fallback plan
        fallback_plan = self._generate_basic_fallback_plan(pod_name, namespace, volume_path)
        
        # Add fallback plan to message list
        if message_list is None:
            message_list = [
                {"role": "system", "content": self._generate_refinement_system_prompt()},
                {"role": "user", "content": user_message},
                {"role": "assistant", "content": fallback_plan}
            ]
        else:
            # If the last message is from the user, append the assistant response
            if message_list[-1]["role"] == "user":
                message_list.append({"role": "assistant", "content": fallback_plan})
            else:
                # Replace the last message if it's from the assistant
                message_list[-1] = {"role": "assistant", "content": fallback_plan}
        
        return fallback_plan, message_list
    
    def _generate_refinement_system_prompt(self, use_react: bool = False) -> str:
        """
        Generate system prompt for LLM focused on plan refinement with static guiding principles
        
        Args:
            use_react: Whether to use React mode (default: False)
            
        Returns:
            str: System prompt for LLM
        """
        # Base system prompt for both modes
        base_prompt = """You are an expert Kubernetes storage troubleshooter. Your task is to refine a draft Investigation Plan for troubleshooting volume read/write errors in Kubernetes.

TASK:
1. Review the draft plan containing preliminary steps from rule-based analysis and mandatory static steps
2. Analyze the Knowledge Graph and historical experience data
3. Refine the plan by:
   - Respecting existing steps (do not remove or modify static steps as much as possible)
   - Adding necessary additional steps using only the provided Phase1 tools
   - Reordering steps if needed for logical flow
   - Adding fallback steps for error handling

CONSTRAINTS:
- When you identify a knowledge gap, use the appropriate MCP tool to gather the information you need. Don't guess or make assumptions when you can use a tool to get accurate information.
- You must only reference tools available in the Phase1 tool registry
- All tool references must match the exact name and parameter format shown in the tools registry
- Include at least one disk-related check step and one volume-related check step.
- Max Steps: 15
- IMPORTANT: Each tool should be used at most once in the entire plan. Do not include duplicate tool calls. If a tool is already used in a step, do not use it again in another step.

OUTPUT FORMAT:
Your response must be a refined Investigation Plan with steps in this format:
Step X: [Description and Reason] | Tool: [tool_name(parameters)] | Expected: [expected]

You may include fallback steps for error handling in this format:
Fallback Steps (if main steps fail):
Step FX: [Description and Reason] | Tool: [tool_name(parameters)] | Expected: [expected] | Trigger: [failure_condition]

The plan must be comprehensive, logically structured, and include all necessary steps to investigate the volume I/O errors.
"""

        # Add React-specific additions if in React mode
        if use_react:
            # Get available MCP tools information
            mcp_tools_info = ""
            if hasattr(self, 'mcp_tools') and self.mcp_tools:
                mcp_tools_info = "\n".join([
                    f"- {tool.name}: {tool.description}" for tool in self.mcp_tools
                ])


            react_additions = f"""
You are operating in a ReAct (Reasoning and Acting) framework where you can:
1. REASON about the problem and identify knowledge gaps
2. ACT by calling external tools to gather information
3. OBSERVE the results and update your understanding
4. Continue this loop until you have enough information to create a comprehensive plan

Available MCP tools:
{mcp_tools_info}

When you identify a knowledge gap, use the appropriate MCP tool to gather the information you need. Don't guess or make assumptions when you can use a tool to get accurate information.

When you've completed the Investigation Plan, include the marker [END_GRAPH] at the end of your message.
"""
            return base_prompt + react_additions
        
        # Return base prompt for Legacy mode
        return base_prompt
    
    def _format_raw_plan(self, raw_plan: str, pod_name: str, namespace: str, volume_path: str) -> str:
        """
        Format raw LLM output into the required Investigation Plan format
        
        Args:
            raw_plan: Raw LLM output
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod
            volume_path: Path of the volume with I/O error
            
        Returns:
            str: Formatted Investigation Plan
        """
        lines = []
        lines.append(f"Investigation Plan:")
        lines.append(f"Target: Pod {namespace}/{pod_name}, Volume Path: {volume_path}")
        
        # Count steps
        main_steps = 0
        fallback_steps = 0
        
        # Extract steps from raw plan
        for line in raw_plan.split('\n'):
            if line.strip().startswith("Step ") and " | Tool: " in line:
                main_steps += 1
            elif line.strip().startswith("Step F") and " | Tool: " in line:
                fallback_steps += 1
        
        lines.append(f"Generated Steps: {main_steps} main steps, {fallback_steps} fallback steps")
        lines.append("")
        
        # Add the raw plan content
        lines.append(raw_plan)
        
        return "\n".join(lines)
    
    def _format_historical_experiences(self, kg_context: Dict[str, Any]) -> str:
        """
        Format historical experience data from Knowledge Graph context
        
        Args:
            kg_context: Knowledge Graph context containing historical experience data
            
        Returns:
            str: Formatted historical experience data for LLM consumption
        """
        try:
            # Extract historical experiences from kg_context
            historical_experiences = kg_context.get('historical_experiences', [])
            
            if not historical_experiences:
                return "No historical experience data available."
            
            # Format historical experiences in a clear, structured way
            return self._format_historical_experience_entries(historical_experiences)
            
        except Exception as e:
            error_msg = handle_exception("_format_historical_experiences", e, self.logger)
            return "Error formatting historical experience data."
    
    def _format_historical_experience_entries(self, experiences: List[Dict[str, Any]]) -> str:
        """
        Format historical experience entries using Chain of Thought (CoT) structure
        
        Args:
            experiences: List of historical experience entries
            
        Returns:
            str: Formatted entries with CoT structure
        """
        formatted_entries = []
        
        for idx, exp in enumerate(experiences, 1):
            # Get attributes from the experience
            attributes = exp.get('attributes', {})
            
            # Check for new CoT format fields first
            observation = attributes.get('observation', attributes.get('phenomenon', 'Unknown observation'))
            thinking = attributes.get('thinking', [])
            investigation = attributes.get('investigation', [])
            diagnosis = attributes.get('diagnosis', attributes.get('root_cause', 'Unknown diagnosis'))
            resolution = attributes.get('resolution', attributes.get('resolution_method', 'No resolution method provided'))
            
            # Format thinking points
            thinking_formatted = ""
            if thinking:
                thinking_formatted = "Thinking:\n"
                for i, point in enumerate(thinking, 1):
                    thinking_formatted += f"{i}. {point}\n"
            
            # Format investigation steps
            investigation_formatted = ""
            if investigation:
                investigation_formatted = "Investigation:\n"
                for i, step in enumerate(investigation, 1):
                    if isinstance(step, dict):
                        step_text = step.get('step', '')
                        reasoning = step.get('reasoning', '')
                        investigation_formatted += f"{i}. {step_text}\n   - {reasoning}\n"
                    else:
                        investigation_formatted += f"{i}. {step}\n"
            else:
                # Fall back to legacy format
                localization_method = attributes.get('localization_method', '')
                if localization_method:
                    investigation_formatted = f"Investigation:\n{localization_method}\n"
            
            # Format resolution steps
            resolution_formatted = ""
            if isinstance(resolution, list):
                resolution_formatted = "Resolution:\n"
                for i, step in enumerate(resolution, 1):
                    resolution_formatted += f"{i}. {step}\n"
            else:
                resolution_formatted = f"Resolution:\n{resolution}\n"
            
            # Format the entry using CoT structure
            entry = f"""## HISTORICAL EXPERIENCE #{idx}: {observation}

**OBSERVATION**: {observation}

**THINKING**:
{thinking_formatted}

**INVESTIGATION**:
{investigation_formatted}

**DIAGNOSIS**: {diagnosis}

**RESOLUTION**:
{resolution_formatted}
"""
            formatted_entries.append(entry)
        
        return "\n".join(formatted_entries)
    
    def _format_step_line(self, step: Dict[str, Any]) -> str:
        """
        Format a step into a string line
        
        Args:
            step: Step data
            
        Returns:
            str: Formatted step line
        """
        # Format arguments
        args_str = ', '.join(f'{k}={repr(v)}' for k, v in step.get('arguments', {}).items())
        
        # Format the step line
        return (
            f"Step {step['step']}: {step['description']} | "
            f"Tool: {step['tool']}({args_str}) | "
            f"Expected: {step['expected']}"
        )
    
    def _generate_basic_fallback_plan(self, pod_name: str, namespace: str, volume_path: str) -> str:
        """
        Generate a basic fallback plan when all else fails
        
        Args:
            pod_name: Name of the pod with the error
            namespace: Namespace of the pod
            volume_path: Path of the volume with I/O error
            
        Returns:
            str: Basic fallback Investigation Plan
        """
        from phases.utils import generate_basic_fallback_plan
        return generate_basic_fallback_plan(pod_name, namespace, volume_path)
    
    def _extract_kg_context_from_message(self, message_content: str) -> Dict[str, Any]:
        """
        Extract knowledge graph context from a message
        
        Args:
            message_content: Content of a message containing knowledge graph context
            
        Returns:
            Dict[str, Any]: Extracted knowledge graph context
        """
        try:
            # Find the knowledge graph context section
            start_marker = "KNOWLEDGE GRAPH CONTEXT"
            end_markers = ["HISTORICAL EXPERIENCE", "DRAFT PLAN", "TOOLS ALREADY USED"]
            
            if start_marker not in message_content:
                self.logger.warning("Knowledge Graph Context section not found in message")
                return {}
            
            # Get start position
            start_pos = message_content.find(start_marker)
            start_pos = message_content.find('\n', start_pos) + 1  # Move to next line
            
            # Find the end position using end markers
            end_pos = len(message_content)
            for marker in end_markers:
                marker_pos = message_content.find(marker, start_pos)
                if marker_pos != -1 and marker_pos < end_pos:
                    end_pos = marker_pos
            
            # Extract and parse the knowledge graph context
            kg_context_str = message_content[start_pos:end_pos].strip()
            
            # Try to parse as JSON
            try:
                # Look for JSON-like content
                json_start = kg_context_str.find('{')
                json_end = kg_context_str.rfind('}') + 1
                
                if json_start != -1 and json_end > json_start:
                    kg_json_str = kg_context_str[json_start:json_end]
                    return json.loads(kg_json_str)
                
                # Fall back to treating whole content as knowledge graph context
                return {"knowledge_graph_content": kg_context_str}
                
            except json.JSONDecodeError:
                # Not valid JSON, return as raw text
                return {"knowledge_graph_content": kg_context_str}
                
        except Exception as e:
            error_msg = handle_exception("_extract_kg_context_from_message", e, self.logger)
            self.logger.warning(f"Failed to extract Knowledge Graph context: {error_msg}")
            return {}
    
    def _update_message_list_from_react(self, original_message_list: List[Dict[str, str]], 
                                      react_messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        Update message list from React graph results
        
        Args:
            original_message_list: Original message list
            react_messages: Messages from React graph
            
        Returns:
            List[Dict[str, str]]: Updated message list
        """
        if not original_message_list:
            # If no original message list, return react messages directly
            return react_messages
        
        # Get the first message from original list (system prompt)
        system_message = original_message_list[0]
        
        # Get the last message from react messages (final plan)
        final_message = react_messages[-1] if react_messages else {"role": "assistant", "content": "No plan generated"}
        
        # Create new message list with system prompt and final plan
        new_message_list = [system_message]
        
        # If there are at least 2 messages in the original list, add the user message
        if len(original_message_list) > 1:
            user_message = original_message_list[1]
            new_message_list.append(user_message)
        
        # Add the final message
        new_message_list.append(final_message)
        
        return new_message_list
    
    def _convert_messages_to_message_list(self, messages: List[BaseMessage]) -> List[Dict[str, str]]:
        """
        Convert BaseMessage list to message list format
        
        Args:
            messages: List of BaseMessage objects
            
        Returns:
            List[Dict[str, str]]: Message list format
        """
        message_list = []
        
        for message in messages:
            role = "system"
            if isinstance(message, HumanMessage):
                role = "user"
            elif isinstance(message, AIMessage):
                role = "assistant"
            elif isinstance(message, ToolMessage):
                role = "tool"
            
            message_list.append({
                "role": role,
                "content": message.content
            })
        
        return message_list
</file>

<file path="troubleshooting/graph.py">
#!/usr/bin/env python3
"""
LangGraph Graph Building Components for Kubernetes Volume I/O Error Troubleshooting

This module contains functions for creating and configuring LangGraph state graphs
used in the analysis and remediation phases of Kubernetes volume troubleshooting.
Enhanced with specific end conditions for better control over graph termination.
Refactored to support parallel and serial tool execution for improved performance.
"""

import json
import logging
import os
import yaml
from typing import Dict, Any, List, TypedDict, Optional, Union, Set, Tuple
from tools.core.mcp_adapter import get_mcp_adapter

# Configure logging (file only, no console output)
logger = logging.getLogger('langgraph')
logger.setLevel(logging.INFO)
# Don't propagate to root logger to avoid console output
logger.propagate = False

from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import tools_condition
from langchain_core.messages import BaseMessage, ToolMessage, HumanMessage, SystemMessage
from phases.llm_factory import LLMFactory
from troubleshooting.execute_tool_node import ExecuteToolNode
from troubleshooting.hook_manager import HookManager
from troubleshooting.end_conditions import EndConditionFactory
from rich.console import Console
from rich.panel import Panel

# Enhanced state class to track additional information
class EnhancedMessagesState(TypedDict):
    """Enhanced state class that extends MessagesState with additional tracking"""
    messages: List[BaseMessage]
    iteration_count: int
    tool_call_count: int
    goals_achieved: List[str]
    root_cause_identified: bool
    fix_plan_provided: bool


# Create console for rich output
console = Console()
file_console = Console(file=open('troubleshoot.log', 'a'))

def load_tool_config() -> Tuple[Set[str], Set[str]]:
    """
    Load tool configuration from config.yaml to determine which tools
    should be executed in parallel and which should be executed serially.
    
    Returns:
        Tuple[Set[str], Set[str]]: Sets of parallel and serial tool names
    """
    try:
        with open('config.yaml', 'r') as f:
            config_data = yaml.safe_load(f)
            
        tool_config = config_data.get("tools", {})
        parallel_tools = set(tool_config.get("parallel", []))
        serial_tools = set(tool_config.get("serial", []))
        
        # Log the configuration
        logger.info(f"Loaded tool configuration: {len(parallel_tools)} parallel tools, {len(serial_tools)} serial tools")
        
        return parallel_tools, serial_tools
    except Exception as e:
        logger.error(f"Error loading tool configuration: {e}")
        # Return empty sets as fallback (all tools will be treated as serial)
        return set(), set()

# Define hook functions for SerialToolNode
def before_call_tools_hook(tool_name: str, args: Dict[str, Any], call_type: str = "Serial") -> None:
    """Hook function called before a tool is executed.
    
    Args:
        tool_name: Name of the tool being called
        args: Arguments passed to the tool
        call_type: Type of call execution ("Parallel" or "Serial")
    """
    try:
        # Format arguments for better readability
        formatted_args = json.dumps(args, indent=2) if args else "None"
        
        # Format the tool usage in a nice way
        if formatted_args != "None":
            # Print to console and log file
            tool_panel = Panel(
                f"[bold yellow]Tool:[/bold yellow] [green]{tool_name}[/green] [bold cyan]({call_type})[/bold cyan]\n\n"
                f"[bold yellow]Arguments:[/bold yellow]\n[blue]{formatted_args}[/blue]",
                title="[bold magenta]Start to Call Tools",
                border_style="magenta",
                safe_box=True
            )
            console.print(tool_panel)
        else:
            # Simple version for tools without arguments
            tool_panel = Panel(
                f"[bold yellow]Tool:[/bold yellow] [green]{tool_name}[/green] [bold cyan]({call_type})[/bold cyan]\n\n"
                f"[bold yellow]Arguments:[/bold yellow] None",
                title="[bold magenta]Start to Call Tools",
                border_style="magenta",
                safe_box=True
            )
            console.print(tool_panel)

        # Also log to file console
        file_console.print(f"Executing tool: {tool_name} ({call_type})")
        file_console.print(f"Parameters: {formatted_args}")
        
        # Log to standard logger
        logger.info(f"Executing tool: {tool_name} ({call_type})")
        logger.info(f"Parameters: {formatted_args}")
    except Exception as e:
        logger.error(f"Error in before_call_tools_hook: {e}")

def after_call_tools_hook(tool_name: str, args: Dict[str, Any], result: Any, call_type: str = "Serial") -> None:
    """Hook function called after a tool is executed.
    
    Args:
        tool_name: Name of the tool that was called
        args: Arguments that were passed to the tool
        result: Result returned by the tool
        call_type: Type of call execution ("Parallel" or "Serial")
    """
    try:
        # Format result for better readability
        if isinstance(result, ToolMessage):
            result_content = result.content
            result_status = result.status if hasattr(result, 'status') else 'success'
            formatted_result = f"Status: {result_status}\nContent: {result_content[:1000]}"
        else:
            formatted_result = str(result)[:1000]
        
        # Print tool result to console
        tool_panel = Panel(
            f"[bold cyan]Tool completed:[/bold cyan] [green]{tool_name}[/green] [bold cyan]({call_type})[/bold cyan]\n"
            f"[bold cyan]Result:[/bold cyan]\n[yellow]{formatted_result}[/yellow]",
            title="[bold magenta]Call tools Result",
            border_style="magenta",
            safe_box=True
        )
        console.print(tool_panel)

        # Also log to file console
        file_console.print(f"Tool completed: {tool_name} ({call_type})")
        file_console.print(f"Result: {formatted_result}")
        
        # Log to standard logger
        logger.info(f"Tool completed: {tool_name} ({call_type})")
        logger.info(f"Result: {formatted_result}")
    except Exception as e:
        logger.error(f"Error in after_call_tools_hook: {e}")

def create_troubleshooting_graph_with_context(collected_info: Dict[str, Any], phase: str = "phase1", 
                                            config_data: Dict[str, Any] = None, streaming: bool = False):
    """
    Create a LangGraph ReAct graph for troubleshooting with pre-collected context
    and enhanced end conditions
    
    Args:
        collected_info: Pre-collected diagnostic information from Phase 0
        phase: Current troubleshooting phase ("phase1" for investigation, "phase2" for action)
        config_data: Configuration data
        streaming: Whether to enable streaming for the LLM
        
    Returns:
        StateGraph: LangGraph StateGraph
    """
    if config_data is None:
        raise ValueError("Configuration data is required")
    
    # Initialize components
    model = _initialize_llm(config_data, streaming, phase)
    
    # Define function to call the model with pre-collected context
    def call_model(state: MessagesState):
        logging.info(f"Processing state with {len(state['messages'])} messages")
        
        # Prepare messages for the model
        state = _prepare_messages(state, collected_info, phase, model)
        
        # Get tools for the current phase
        tools = _get_tools_for_phase(phase)
        
        # Call the model with tools
        response = model.bind_tools(tools).invoke(state["messages"])
        
        logging.info(f"Model response: {response.content}...")
        
        # Create console for rich output
        console = Console()
        console.print(f"[bold cyan]LangGraph thinking process:[/bold cyan]")

        if response.content:
            console.print(Panel(
                f"[bold green]{response.content}[/bold green]",
                title="[bold magenta]Thinking step",
                border_style="magenta",
                safe_box=True
            ))

        return {"messages": state["messages"] + [response]}
    
    # Create an end condition checker using the factory
    end_condition_checker = EndConditionFactory.create_checker(
        "llm" if config_data.get("llm_end_condition_check", True) else "simple",
        model=model,
        phase=phase,
        max_iterations=config_data.get("max_iterations", 30)
    )
    
    # Define the end condition check function that delegates to the checker
    def check_end_conditions(state: MessagesState) -> Dict[str, str]:
        """
        Delegate end condition checking to the appropriate strategy
        Returns {"result": "end"} if the graph should end, {"result": "continue"} if it should continue
        """
        return end_condition_checker.check_conditions(state)

    # Load tool configuration
    parallel_tools, serial_tools = load_tool_config()
    
    # Get tools for the current phase
    tools = _get_tools_for_phase(phase)
    
    # Create ExecuteToolNode with the configured tools
    execute_tool_node = _create_execute_tool_node(tools, parallel_tools, serial_tools)
    
    # Build the graph
    graph = _build_graph(call_model, check_end_conditions, execute_tool_node)
    
    logging.info("Graph compilation complete")    
    return graph

def _initialize_llm(config_data: Dict[str, Any], streaming: bool, phase: str):
    """
    Initialize the language model using LLMFactory
    
    Args:
        config_data: Configuration data
        streaming: Whether to enable streaming for the LLM
        phase: Current troubleshooting phase
        
    Returns:
        BaseChatModel: Initialized language model
    """
    # Initialize language model using LLMFactory
    llm_factory = LLMFactory(config_data)
    model = llm_factory.create_llm(streaming=streaming, phase_name=phase)
    return model

def _prepare_messages(state: MessagesState, collected_info: Dict[str, Any], phase: str, model) -> MessagesState:
    """
    Prepare messages for the model with pre-collected context
    
    Args:
        state: Current state with messages
        collected_info: Pre-collected diagnostic information
        phase: Current troubleshooting phase
        model: Language model
        
    Returns:
        MessagesState: Updated state with prepared messages
    """
    from troubleshooting.prompt_manager import PromptManager
    
    # Create prompt manager
    prompt_manager = PromptManager(config_data=None)
    
    # Get system prompt and context summary
    system_prompt = prompt_manager.get_system_prompt(phase)
    context_summary = prompt_manager.get_context_summary(collected_info)
    
    # Create system and context messages
    system_message = SystemMessage(content=system_prompt)
    context_message = SystemMessage(content=context_summary)
    
    # Extract existing user messages (skip system message if present)
    user_messages = []
    if state["messages"]:
        if isinstance(state["messages"], list):
            for msg in state["messages"]:
                if not isinstance(msg, SystemMessage):
                    user_messages.append(msg)
            
            # Create new message list with system message, context message, and existing user messages
            state["messages"] = [system_message, context_message] + user_messages
        else:
            state["messages"] = [system_message, context_message, state["messages"]]
    else:
        state["messages"] = [system_message, context_message]
    
    return state

def _get_tools_for_phase(phase: str) -> List[Any]:
    """
    Get tools for the current phase
    
    Args:
        phase: Current troubleshooting phase
        
    Returns:
        List[Any]: List of tools for the current phase
    """
    # Get MCP adapter and tools
    mcp_adapter = get_mcp_adapter()
    mcp_tools = []
    
    # Select tools based on phase
    if phase == "phase1":
        from tools import get_phase1_tools
        tools = get_phase1_tools()
        # Add MCP tools for phase1 if available
        if mcp_adapter:
            mcp_tools = mcp_adapter.get_tools_for_phase('phase1')
            if mcp_tools:
                tools.extend(mcp_tools)
                logging.info(f"Using Phase 1 tools: {len(tools)} investigation tools (including {len(mcp_tools)} MCP tools)")
            else:
                logging.info(f"Using Phase 1 tools: {len(tools)} investigation tools")
        else:
            logging.info(f"Using Phase 1 tools: {len(tools)} investigation tools")
    elif phase == "phase2":
        from tools import get_phase2_tools
        tools = get_phase2_tools()
        
        # Add MCP tools for phase2 if available
        if mcp_adapter:
            mcp_tools = mcp_adapter.get_tools_for_phase('phase2')
            if mcp_tools:
                tools.extend(mcp_tools)
                logging.info(f"Using Phase 2 tools: {len(tools)} investigation + action tools (including {len(mcp_tools)} MCP tools)")
            else:
                logging.info(f"Using Phase 2 tools: {len(tools)} investigation + action tools")
        else:
            logging.info(f"Using Phase 2 tools: {len(tools)} investigation + action tools")
    else:
        # Fallback to all tools for backward compatibility
        from tools import define_remediation_tools
        tools = define_remediation_tools()
        logging.info(f"Using all tools (fallback): {len(tools)} tools")
    
    return tools

def _create_execute_tool_node(tools: List[Any], parallel_tools: Set[str], serial_tools: Set[str]) -> ExecuteToolNode:
    """
    Create and configure the ExecuteToolNode
    
    Args:
        tools: List of tools
        parallel_tools: Set of tool names to execute in parallel
        serial_tools: Set of tool names to execute serially
        
    Returns:
        ExecuteToolNode: Configured ExecuteToolNode
    """
    # If a tool is not explicitly categorized, default to serial
    all_tool_names = {tool.name for tool in tools}
    uncategorized_tools = all_tool_names - parallel_tools - serial_tools
    if uncategorized_tools:
        logging.info(f"Found {len(uncategorized_tools)} uncategorized tools, defaulting to serial")
        serial_tools.update(uncategorized_tools)
    
    # Create a hook manager for console output
    hook_manager = HookManager(console=console, file_console=file_console)
    
    # Register hook functions with the hook manager
    hook_manager.register_before_call_hook(before_call_tools_hook)
    hook_manager.register_after_call_hook(after_call_tools_hook)
    
    # Create ExecuteToolNode with the configured tools
    logging.info(f"Creating ExecuteToolNode for execution of {len(parallel_tools)} parallel and {len(serial_tools)} serial tools")
    execute_tool_node = ExecuteToolNode(tools, parallel_tools, serial_tools, name="execute_tools")
    
    # Register hook manager with the ExecuteToolNode
    execute_tool_node.register_before_call_hook(hook_manager.run_before_hook)
    execute_tool_node.register_after_call_hook(hook_manager.run_after_hook)
    
    return execute_tool_node

def _build_graph(call_model, check_end_conditions: callable, execute_tool_node: ExecuteToolNode) -> StateGraph:
    """
    Build the LangGraph state graph
    
    Args:
        call_model: Function to call the model
        check_end_conditions: Function to check end conditions
        execute_tool_node: ExecuteToolNode for tool execution
        
    Returns:
        StateGraph: Compiled LangGraph StateGraph
    """
    # Build state graph
    logging.info("Building state graph with enhanced end conditions")
    builder = StateGraph(MessagesState)
    
    logging.info("Adding node: call_model")
    builder.add_node("call_model", call_model)
    
    logging.info("Adding node: execute_tools")
    builder.add_node("execute_tools", execute_tool_node)
    
    logging.info("Adding node: check_end")
    builder.add_node("check_end", check_end_conditions)
    
    logging.info("Adding conditional edges for tools")
    builder.add_conditional_edges(
        "call_model",
        tools_condition,
        {
            "tools": "execute_tools",   # Route to execute_tools node
            "none": "check_end",        # If no tools, go to check_end
            "end": "check_end",
            "__end__": "check_end"
        }
    )
    
    logging.info("Adding conditional edges from check_end node")
    builder.add_conditional_edges(
        "check_end",
        lambda state: check_end_conditions(state)["result"],
        {
            "end": END,
            "__end__": END,
            "continue": "call_model"  # Loop back if conditions not met
        }
    )
    
    # Add edge from execute_tools to call_model
    logging.info("Adding edge: execute_tools -> call_model")
    builder.add_edge("execute_tools", "call_model")
    
    logging.info("Adding edge: START -> call_model")
    builder.add_edge(START, "call_model")
    
    logging.info("Compiling graph")
    return builder.compile()
</file>

</files>
